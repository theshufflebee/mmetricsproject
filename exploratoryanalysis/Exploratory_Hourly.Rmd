---
title: "Exploratory_Hourly"
output:
  pdf_document: default
  html_document: default
---

```{r}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(tidyr)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

data("stop_words")
stop_words_list <- stop_words$word

SPY <- read.csv(here("data/mothership", "SPY.csv"))

```



```{r}
posts_raw <- read.csv(here("data/mothership", "social.csv"))
```


```{r}
posts <- posts_raw %>% select(timestamp, tweet_text)
```


```{r}
posts <- posts %>%
    mutate(
      tweet_clean = str_replace_all(tweet_text, "(http[s]?://|www\\.)\\S+", ""),  # Remove URLs
      
      post_lower = str_to_lower(tweet_clean),  # New column with post converted to lowercase
      post_clean = str_replace_all(post_lower, "[^a-z\\s]", " ")
    )
```





```{r}
posts <- posts %>%
  select(-post_lower, -tweet_clean, -tweet_text)

posts$timestamp <- as.POSIXct(posts$timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "EST")



```

```{r}
posts <- posts %>%
  mutate(hour_timestamp = floor_date(timestamp, unit = "hour")) %>%   # Round to hour
  group_by(hour_timestamp) %>%
  summarise(
    combined_text = str_c(post_clean, collapse = " "),   # Concatenate texts
    .groups = "drop"
  )

posts <- posts %>%
  rename(timestamp = hour_timestamp)
```

```{r}
posts2 <- posts %>%
  mutate(
    # --- Step 1: Adjust based on time ---
    hour = hour(timestamp),
    date = as.Date(timestamp),
    
    move_before_9 = hour < 9,
    move_after_16 = hour >= 16,
    
    adjusted_timestamp = case_when(
      move_before_9 ~ as.POSIXct(paste(date, "09:00:00"), format = "%Y-%m-%d %H:%M:%S", tz = tz(timestamp)),
      move_after_16 ~ as.POSIXct(paste(date + 1, "09:00:00"), format = "%Y-%m-%d %H:%M:%S", tz = tz(timestamp)),
      TRUE ~ timestamp
    ),
    
    # --- Step 2: Adjust for weekends on adjusted_timestamp ---
    weekday = wday(adjusted_timestamp),  # <- use adjusted_timestamp here
    final_timestamp = case_when(
      weekday == 7 ~ as.POSIXct(paste(as.Date(adjusted_timestamp) + 2, "09:00:00"), format = "%Y-%m-%d %H:%M:%S", tz = tz(timestamp)),
      weekday == 1 ~ as.POSIXct(paste(as.Date(adjusted_timestamp) + 1, "09:00:00"), format = "%Y-%m-%d %H:%M:%S", tz = tz(timestamp)),
      TRUE ~ adjusted_timestamp
    )
  ) %>%
  select(final_timestamp, combined_text) %>%
  rename(timestamp = final_timestamp)

posts2 <- posts2 %>%
  group_by(timestamp) %>%
  summarise(
    text = str_c(combined_text, collapse = " "),   # Concatenate texts
    .groups = "drop"
  )

```


```{r}
start_date <- '2025-01-01'
end_date <- '2025-04-10'
```


# Market Data

```{r}
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S", tz="EST")

#find hourly volatility
#NOTE: this ignores tweets made outside trading hours!!
SPY_volatility_alltime = dplyr::select(SPY,timestamp,r_vol_h)

#aggregating per hour
SPY_volatility_alltime = SPY_volatility_alltime %>%
          mutate(timestamp = floor_date(timestamp, unit = "hour")) %>%
          distinct(timestamp, .keep_all = TRUE) 
   
#select time period
SPY_volatility = filter(SPY_volatility_alltime,
                  between(timestamp, 
                          as.Date(start_date), 
                          as.Date(end_date)))



```


```{r}
posts_selected <- filter(posts2,
                  between(timestamp, 
                          as.Date(start_date), 
                          as.Date(end_date)))


data_2 <- full_join(posts_selected, SPY_volatility, by = "timestamp")

data_3 <- data_2 %>%
  # Sort data from latest to oldest
  arrange(desc(timestamp)) %>%
  
  mutate(
    adjusted_timestamp = timestamp
  ) %>%
  
  mutate(
    adjusted_timestamp = ifelse(
      is.na(r_vol_h),
      # Use the previous adjusted timestamp (lag function) for rows with NA volatility
      lag(adjusted_timestamp),
      # Keep the original timestamp when volatility is not NA
      timestamp
    )
  ) %>%
  
  # Restore the original order
  arrange(timestamp)

```











```{r}
posts_token <- posts2 %>%
  mutate(
    tokens = text %>%
        str_replace_all("[^a-z\\s]", " ") %>%
        str_split("\\s+") %>%
        lapply(function(words) {
          words <- words[words != ""]  # Remove empty strings
          words <- setdiff(words, stop_words_list)  # Remove stopwords
          words <- lemmatize_words(words) # Reduces words to their base form -> is becomes be
        }
    
  )
  )
```

sum(wday(posts2$timestamp) %in% c(1,7))


```{r}
# Define the tokens you want to check
specific_tokens <- c("tariff")

# Count occurrences
token_count <- posts_token %>%
  pull(tokens) %>%  # Extract the 'tokens' column as a list of token vectors
  unlist() %>%      # Flatten the list into a single vector of tokens
  .[ . %in% specific_tokens ] %>%  # Filter only the tokens that match the specific ones
  length()  # Get the total count

# Print the result
print(token_count)
```

```{r}
afinn <- tidytext::get_sentiments("afinn")


posts_sentiment <- posts_token %>%
  unnest_tokens(word, text) %>%  # Tokenize the text
  inner_join(afinn, by = "word") %>%  # Join with the AFINN lexicon to get sentiment scores
  group_by(timestamp) %>%  # Group by timestamp to aggregate by post
  summarise(sentiment_score = sum(value, na.rm = TRUE))  # Sum up the sentiment scores for each post

# Merge the sentiment scores back into the original posts_token dataframe
posts_token_with_sentiment <- posts_token %>%
  left_join(posts_sentiment, by = "timestamp")
```

#Join analysis data

```{r}
posts_armax = filter(posts_token_with_sentiment,
                  between(timestamp, 
                          as.Date(start_date), 
                          as.Date(end_date)))

```



```{r}
analysis_data <- full_join(posts_armax, SPY_volatility, by = "timestamp")
analysis_data <- analysis_data %>% arrange(timestamp)

analysis_data <- analysis_data %>% mutate(dummy_post = if_else(!is.na(text), 1, 0))

analysis_data <- analysis_data %>%
  mutate(
    sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score),
    tariff = ifelse(str_count(text,pattern = "tariff") >= 1, 1,0),
    tariff = if_else(is.na(tariff), 0, tariff) 
  )

analysis_data_fin <- analysis_data %>% filter(r_vol_h != 0)



```

```{r}
lag_selector(y=analysis_data_fin$r_vol_h, xreg=analysis_data_fin$tariff,
             nb.lags=8, type="text")
```


```{r}
armax(analysis_data_fin$r_vol_h,xreg=analysis_data_fin$tariff,nb.lags=4,latex=F)

```

```{r}
#auto.armax.r selects the lowest AIC checking all 3 p,q,r values
res3 = auto.armax.r(analysis_data_fin$r_vol_h, x=analysis_data_fin$tariff, 
                max_p = 7, max_q = 7, max_r = 3, criterion = "AIC", latex=T)
```



