---
title: "Testing"
output:
  pdf_document: 
    toc: true
    fig_caption: yes
header-includes:
  - \usepackage{amsmath}
---
\newpage

```{r library_setup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(RColorBrewer)
require(wordcloud)
require(tidyr)
require(randomForest)
require(tidytext)
require(textstem)
require(tidyverse)
require(tm)
require(SnowballC)
require(quanteda.textplots)
require(quantmod)
require(quanteda)
require(rvest)
require(httr)
require(xml2)
require(textdata)
require(sentimentr)
require(syuzhet)
require(text)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax.R"))
```


# Data 

## Raw Political Data
```{r political rawdata_setup, results=FALSE, warning=FALSE, message=FALSE}

#political shocks
raw_truths <- read.csv(here("data/political_data", "truths_new.csv"))
raw_tweets <- read.csv(here("data/political_data", "tweets.csv"))

```

## Raw Data
```{r financial rawdata_setup, results=FALSE, warning=FALSE, message=FALSE}
#market prices (loads and names them automatically)
#raw_ONEQ <- read.csv(here("data/market_data", "ONEQ.csv")) #USA
#raw_SMI <- read.csv(here("data/market_data", "SMI.csv")) #CH
#raw_VTHR <- read.csv(here("data/market_data", "VTHR.csv")) #USA
#raw_VTI <- read.csv(here("data/market_data", "VTI.csv")) #USA
#raw_DAX <- read.csv(here("data/market_data", "DAX.csv")) #DE
#raw_ASHR <- read.csv(here("data/market_data", "ASHR.csv")) #CHINA
raw_SPYy <- read.csv(here("data/market_data", "Spyqyahoo.csv")) #yahoo

#S&P500
data_loader(symbol="SPY")

#STOXX50
data_loader(symbol="VGK")




#CSI 300 (China)
data_loader(symbol="ASHR")

```


## Daily Data
```{r dailydata, results=FALSE, warning=FALSE, message=FALSE}

#political shocks


#market prices
day_SPY_0409 = filter(raw_SPY, str_detect(timestamp, "^2025-04-09")) #9th of april
day_SPY_0409$timestamp = as.POSIXct(day_SPY_0409$timestamp, 
                                    format = "%Y-%m-%d %H:%M:%S", tz = "EST")

yahoo_ds0409 = filter(raw_SPYy, str_detect(Date, "^2025-04-09"))
yahoo_ds0409$Date = as.POSIXct(yahoo_ds0409$Date, 
                                    format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
yahoo_ds0409$Date = with_tz(yahoo_ds0409$Date, "EST")


#extract a particular month
SPY_24_09 = month_selector(raw_SPY,2024,09) #november 2024


#extract a particular year
SPY_24 = year_selector(raw_SPY,2024) #2024

```


# Plots

## Total 
```{r total market plots}

#SPY
ggplot(raw_SPY, aes(x = as.POSIXct(timestamp), y = close)) +
  geom_point(color = "blue", size = 0.01) +
  geom_line(aes(group=1), color="blue", linewidth=0.05) +
  labs(title = "SPY Price Over Time",
       x = "Time",
       y = "Close Price") +
  scale_x_datetime(date_labels = "%b %Y", date_breaks = "6 month") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
```


## Per Day
```{r daily market plots}

#SPY Source: alpha
ggplot(day_SPY_0409, aes(x = timestamp, y = close)) +
  geom_point(color = "blue", size = 0.01) +
  geom_line(aes(group=1)) +
  labs(title = "SPY alpha Price April 9th",
       x = "Time",
       y = "Close Price") +
   scale_x_datetime(date_labels = "%H:%M",  
                    date_breaks = "60 min") +
   theme_minimal() +
   theme(axis.text.x = element_text(angle = 45, hjust = 1))


#SPY Source: yahoo
ggplot(yahoo_ds0409, aes(x = Date, y = Close)) +
  geom_point(color = "blue", size = 0.01) +
  geom_line(aes(group=1)) +
  labs(title = "SPY yahoo Price April 9th",
       x = "Time",
       y = "Close Price") +
   scale_x_datetime(date_labels = "%H:%M",  
                    date_breaks = "60 min") +
   theme_minimal() +
   theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

# Time Series Analysis
```{r time series}

acf(log(day_SPY_0409$close))
pacf(log(day_SPY_0409$close))

acf(log(raw_SPY$close))
pacf(log(raw_SPY$close))

AR1 = arima(day_SPY_0409$close,c(1,0,0),method="ML")
AR2 = arima(day_SPY_0409$close,c(2,0,0),method="ML")
AR3 = arima(day_SPY_0409$close,c(3,0,0),method="ML")
table1 = export_summs(AR1,AR2,AR3, model.names = c("AR1","AR2","AR3"), digits = 4)
huxtable::caption(table1) <- "AR Estimations"
huxtable::set_width(table1, 0.8)

AR1res = as.numeric(AR1$residuals)
AR1res_lagged <- lag(AR1res, 1)
iidcheck1 = lm(AR1res ~ AR1res_lagged)
AR2res = as.numeric(AR2$residuals)
AR2res_lagged <- lag(AR2res, 1)
iidcheck2 = lm(AR2res ~ AR2res_lagged)
AR3res = as.numeric(AR3$residuals)
AR3res_lagged <- lag(AR3res, 1)
iidcheck3 = lm(AR3res ~ AR3res_lagged)
table2 = export_summs(iidcheck1,iidcheck2,iidcheck3, 
             model.names = c("AR1 Residuals","AR2 Residuals","AR3 Residuals"), 
             digits = 4)
huxtable::caption(table2) <- "Checking Residuals"
huxtable::set_width(table2, 0.8)
```

# Volatility 


## JPR Formula
$$
\begin{aligned}
  v_t &= \frac{1}{N}\sum_{i=1}^N(\Delta p_{t,i})^2 \\
  &\text{where } \Delta p_t \text{ is the difference in price (open - close)} \\
  &\text{and i represents every minute}
\end{aligned}
$$

```{r JPR}
#extract a particular day
SPY_25_04_02 = day_selector(raw_SPY,2025,04,02) #april 2nd 2025

#let's plot it
price_plotter_day(SPY_25_04_02,"SPY Price on April 2nd 2025") 

#quick
quickplot = function(x){price_plotter_day(day_selector(raw_SPY,2025,04,x),"SPY Price 2025")}
quickplot(9)

#realized volatility 
delta_price = SPY_25_04_02$close - SPY_25_04_02$open
delta_price_sqr = delta_price^2
SPY_25_04_02 = cbind(day_selector(raw_SPY,2025,04,02),delta_price_sqr)
v_t = sum(delta_price_sqr) / length(delta_price)

#or is it like this??
#realized volatility method2
p_t = SPY_25_04_02$close
p_t_1 <- lag(p_t, 1)
delta_price2 = p_t_1 - p_t
v_t2 = sum((na.omit(delta_price2))^2) / length(na.omit(delta_price2))

```

```{r JPR volatility compute}
#average per day (outputs scalar)
r.vol_day(SPY_25_04_02) 

#average per day for each day in a month (outputs vector of each day's realised volatility)
r.vol_month(SPY_24_09) 

#for each hour in a day (outputs a vector of each hour's realised volatility)
r.vol_day_hour(SPY_25_04_02) 

#for each hour in a day for each day in a month (outputs a matrix)
month_hour = r.vol_month_hour(SPY_24_09)
huxtable(head(data.frame(month_hour)))


#avg per day for each month of any dataset
#works for datasets with more than 1 year!
vol_SPY_daily = r.vol_daily(raw_SPY,merge=F)
head(vol_SPY_daily)

#can then filter out years, months, or days
vol_24d = year_selector(vol_SPY_daily,2024)
vol_24_08d = month_selector(vol_SPY_daily,2024,08)
vol_24_11_04d = day_selector(vol_SPY_daily,2024,11,04) #scalar


#avg per hour for each day of each month of any dataset
#works for datasets with more than 1 year!
vol_SPY_hourly = r.vol_hourly(raw_SPY,merge=F)
head(vol_SPY_hourly)

#can then filter out years, months, or days
vol_24h = year_selector(vol_SPY_hourly,2024)
vol_24_08h = month_selector(vol_SPY_hourly,2024,08)
vol_24_11_04h = day_selector(vol_SPY_hourly,2024,11,04) #vector

```


```{r SPY volatility plots}

#avg per day volatility all time
dvol_plotter(vol_SPY_daily,breaks="yearly",
             title="SPY Volatility Since 2019")

#avg per day volatility in a year
dvol_plotter(vol_24d,breaks="monthly",
             title="Realised Volatility - SPY 2024")

#avg per day volatility in a month
dvol_plotter(vol_24_08d,breaks="daily",
             title="Realised Volatility - SPY August 2024")


#hourly volatility all time
hvol_plotter(vol_SPY_hourly,breaks="yearly",
             title="SPY Volatility Since 2019")

#hourly volatility in a year
hvol_plotter(vol_24h,breaks="monthly",
             title="Realised Volatility - SPY 2024")

#hourly volatility in a month
hvol_plotter(vol_24_08h,breaks="daily",
             title="Realised Volatility - SPY August 2024")

#hourly volatility in a day
hvol_plotter(vol_24_11_04h,breaks="hourly",
             title="Realised Volatility - SPY 4th of November 2024")

```


## Garman and Klass (1980) Formula
Note that this formula uses open-high-low-close information. \\
This model is based on the assumption that price returns follow a Wiener process with zero drift and constant infinitesimal variance. Itâ€™s constructed by minimizing the variance of a quadratic estimator subject to the constraints of price and time symmetry and scale invariance of volatility. Source: https://assets.bbhub.io/professional/sites/10/intraday_volatility-3.pdf
$$
\begin{aligned}
  V_{ohlc} = 0.5[log(H)-log(L)]^2 - [2log(2)-1][log(C)-log(O)]^2
\end{aligned}
$$
```{r GnK}
#extract a particular day
day_SPY_0402 = day_selector(raw_SPY,2025,04,02) #april 2nd 2025

#variables
C = day_SPY_0402$close 
O = day_SPY_0402$open
H = day_SPY_0402$high 
L = day_SPY_0402$low

#realized volatility 
V_ohlc = 0.5*(log(H)-log(L))^2 - (2*log(2)-1)*(log(C)-log(O))^2
v_ohlc = sqrt(V_ohlc)
day_SPY_0402 = cbind(day_selector(raw_SPY,2025,04,02),V_ohlc)
avg = sum(V_ohlc) / length(V_ohlc)

```



# Tweets \& Truths


## Tweets
```{r tweets data and counting}
tweets = raw_tweets

#only keep original Tweets
tweets <- tweets %>% filter(isRetweet != "t")
tokens <- tokens(tweets$text)
dfm <- dfm(tokens)

#cleanup
tweets = as.data.table(tweets)
names(tweets)[names(tweets) == 'date'] <- 'timestamp'
tweets <- tweets[order(tweets$timestamp, decreasing=T), ]

#count by hour
tweet_count = tweets[, .N, by=.(year(timestamp), month(timestamp), 
                                day(timestamp), hour(timestamp))] 

#fix timestamp
tweet_count$timestamp = as.POSIXct(sprintf("%04d-%02d-%02d %02d:00:00", 
                         tweet_count$year, tweet_count$month, tweet_count$day, 
                         tweet_count$hour), format = "%Y-%m-%d %H:00:00")

#remove useless columns and reorder by oldest first
tweet_count = select(tweet_count, timestamp, N)
tweet_count = tweet_count[ order(tweet_count$timestamp , decreasing = F ),]
head(tweet_count)


```


## Truths
```{r truths data and counting}
truthsbackup <- truths_processer(raw_truths) 

#cleanup
truths = as.data.table(truthsbackup)
names(truths)[names(truths) == 'date_time_parsed'] <- 'timestamp'
truths <- truths[order(truths$timestamp, decreasing=T), ]

#count by hour
truth_count = truths[, .N, by=.(year(timestamp), month(timestamp), 
                                day(timestamp), hour(timestamp))] 

#fix timestamp
truth_count$timestamp = as.POSIXct(sprintf("%04d-%02d-%02d %02d:00:00", 
                         truth_count$year, truth_count$month, truth_count$day, 
                         truth_count$hour), format = "%Y-%m-%d %H:00:00")

#remove useless columns and reorder by oldest first
truth_count = select(truth_count, timestamp, N)
truth_count = truth_count[ order(truth_count$timestamp , decreasing = F ),]
head(truth_count)

```

## Merge
```{r merging tweets & truths count}
tt_count = rbind(tweet_count,truth_count) #tweets & truths 

ggplot(tt_count, aes(x = timestamp, y = N)) +
    geom_point(color = "#253494", size = 1) +
    scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
    labs(title = "Trump Social Media Count",
         x = NULL,
         y = "number of tweets/truths") +
    theme_minimal(base_size = 14) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          plot.title = element_text(face = "bold", hjust = 0.5))
```



# ARMA-X 

## Hourly
```{r h first fix the data}
#merge by matching the hours
#NOTE: this ignores tweets made outside trading hours!!
SPY_volatility_alltime = r.vol_hourly(raw_SPY,merge=F)

#select time period
SPY_volatility = filter(SPY_volatility_alltime,
                  between(timestamp, as.Date('2024-09-20'), as.Date('2025-04-10')))
colnames(SPY_volatility)[1] <- "timestamp_hour"
tt_count = filter(tt_count,
                  between(timestamp, as.Date('2024-09-20'), as.Date('2025-04-10')))

#take all relevant for armax
armax_vol = merge(SPY_volatility, tt_count, by.x = "timestamp_hour", 
                   by.y = "timestamp", all.x = T)
#NA tweets means no tweets
armax_vol$N[is.na(armax_vol$N)] = 0

#common names
armax_vol$r_vol = armax_vol$r_vol_h

```

## Daily
```{r d first fix the data, eval=FALSE, include=FALSE}
#merge by matching the days

SPY_volatility = r.vol_daily(raw_SPY,merge=F)
#since tweet database not full yet (combine tweets & truths)
SPY_volatility = filter(SPY_volatility, 
                        year(SPY_volatility$timestamp) >= 2022 & 
                        month(SPY_volatility$timestamp) >= 03)
colnames(SPY_volatility)[1] <- "timestamp_day"

#take all relevant for armax
armax_vol = merge(SPY_volatility, truth_count, by.x = "timestamp_day", 
                   by.y = "timestamp", all.x = T)
#NA tweets means no tweets
armax_vol$N[is.na(armax_vol$N)] = 0

#common names
armax_vol$r_vol = armax_vol$r_vol_d

```



```{r ARMA-X}

T <- length(armax_vol$r_vol)

# Create lagged exogenous variables (3 lags of N)
nb.lags <- 2
count_lags <- embed(armax_vol$N, nb.lags + 1)
colnames(count_lags) <- paste0("Lag_", 0:nb.lags)

# Align volatility to match count rows
vol_aligned <- tail(armax_vol$r_vol, nrow(count_lags))
# Fit ARMAX(0,0) with 3 exogenous lags
fit_armax <- Arima(vol_aligned, order = c(2,0,0), xreg = count_lags)
fit_armax2 <- Arima(armax_vol$r_vol, order = c(1,0,2), xreg = armax_vol$N)
fit_armax3 <- Arima(armax_vol$r_vol, order = c(1,0,2))

summary(fit_armax)
checkresiduals(fit_armax)
summary(fit_armax2)
checkresiduals(fit_armax2)
summary(fit_armax3)
checkresiduals(fit_armax3)


# Refit using lm for robust SEs
eq <- lm(vol_aligned ~ count_lags)

# Compute Newey-West HAC standard errors
var.cov.mat <- NeweyWest(eq, lag = 7, prewhite = FALSE)
robust_se <- sqrt(diag(var.cov.mat))

# Stargazer output
stargazer(eq, eq, type = "text",
          column.labels = c("(no HAC)", "(HAC)"), keep.stat = "n",
          se = list(NULL, robust_se), no.space = TRUE)

# Choosing the best model
best_model <- auto.arima(vol_aligned, xreg = count_lags, seasonal = FALSE,
                         max.p = 5, max.q = 5, max.d = 0,
                         stepwise = T, approximation = FALSE, trace = TRUE)

summary(best_model)
        
```





```{r}
result <- select_armax(armax_vol$r_vol, armax_vol$N, 
                       max_p = 6, max_q = 6, max_r = 5, criterion = "AIC")

summary(result$model) 
result$AICplot
```








```{r prof but with forecast library, eval=FALSE, include=FALSE}
data("FrozenJuice")
FJ <- as.data.frame(FrozenJuice)
date <- time(FrozenJuice)

# Real price = nominal / PPI
price <- FJ$price / FJ$ppi
T <- length(price)

# Compute percent change
dprice <- 100 * diff(log(price))

# Create lagged exogenous variables (3 lags of fdd)
nb.lags <- 3
FDD <- embed(FJ$fdd, nb.lags + 1)
colnames(FDD) <- paste0("Lag_", 0:nb.lags)

# Align dprice to match FDD rows
dprice_aligned <- tail(dprice, nrow(FDD))
# Fit ARMAX(0,0) with 3 exogenous lags
fit_armax <- Arima(dprice_aligned, order = c(0,0,0), xreg = FDD)

summary(fit_armax)
# Refit using lm for robust SEs
eq <- lm(dprice_aligned ~ FDD)

# Compute Newey-West HAC standard errors
var.cov.mat <- NeweyWest(eq, lag = 7, prewhite = FALSE)
robust_se <- sqrt(diag(var.cov.mat))

# Stargazer output
stargazer(eq, eq, type = "text",
          column.labels = c("(no HAC)", "(HAC)"), keep.stat = "n",
          se = list(NULL, robust_se), no.space = TRUE)

```




























