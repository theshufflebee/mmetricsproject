---
title: "thruthswordcloud"
output:
  html_document:
    keep_md: true
---

# Load the packages, data and functions
```{r}
require(here)
require(stringr)
require(dplyr)
require(ggplot2)
require(lubridate)
require(RColorBrewer)
require(wordcloud)
library(tidyr)
library(randomForest)
library(tidytext)
library(textstem)
library(tidyverse)

truths_raw <- read.csv(here("data/political_data", "trump_all_truths.csv"))
snp_raw <- read.csv(here("data/market_data", "SPY.csv"))

source(here("helperfunctions/truths_cleaning_function.R"))
```

# Process the data with the helperfunction
```{r}
truths <-truths_processer(truths_raw) # can take one or two minutes
head(truths)
```
This does multiple things. Mainly it turns the scrapped data that is a string per post, cleans it and tokenizes the post. It separates the date and prepares the date to make the plots below by creating many colums.

The second part tokenizes the tweets and lemmatizes them. This means each word becomes an "obersvation and all observations are collected on the post level. Then the words are also lemmatized, meaning reduced to their basic form. For example is becomes be, runnin or ran become run ect.


#Wordcloud
We remove the Tweets that only contain links or nothing at all from the dataset
```{r}
truths_main <- truths %>%
  filter(link != 1, media != 1)
```




```{r}
all_tokens <- unlist(truths_main$tokens)

word_freqs <- table(all_tokens)
```

```{r}
count_token_occurrences <- function(data, word) {
  sum(sapply(data$tokens, function(token_list) {
    sum(token_list == word)
  }))
}
```

```{r}
count_token_occurrences(truths_main, "https")

```


```{r}
wordcloud(
  words = names(word_freqs),
  freq = as.numeric(word_freqs),
  min.freq = 300,  # adjust depending on how important it is
  max.words = 400,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

```

From this wordcloud we can gather some of the most used words by trump. Now the question is: how much does he post during open markets? For that we can plot each post on a time plot:

```{r}
Sys.setlocale("LC_TIME", "C") # avoids the weekdays being in french or german -> forces english

truths_main <- truths_main %>%
  mutate(
    is_market_hour = case_when(
      weekdays(day) %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") & ### Important if you use a different locale you need to 
        #change language of this vector
        time_shifted >= (-2.5) & time_shifted <= 4 ~ TRUE,  # 9:30 AM to 4:00 PM
      TRUE ~ FALSE  # Non-market hours
    )
  )
```

```{r}
ggplot(truths_main, aes(x = day, y = time_shifted)) +
  geom_point(aes(color = is_market_hour), alpha = 0.5) +
  
  # Custom colors for market vs non-market hours
  scale_color_manual(values = c("FALSE" = "blue", "TRUE" = "green")) +
  
  scale_y_continuous(
    breaks = seq(-12, 12, by = 3),
    labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")
  ) +
  labs(title = "Terminally Online: Trump's Truth Social Posts (EDT)",
       x = "Day",
       y = "Time of Day",
       color = "Market Hours") +
  
  theme_minimal() +
  
  scale_x_date(
    date_labels = "%b %Y",
    date_breaks = "3 months"
  ) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(linewidth = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```

Now lets get only those tweets
```{r}
truths_market <- truths_main %>%
  filter(is_market_hour != FALSE)
```

now get the open and close

```{r}
snp_data <- snp_raw %>%
  mutate(timestamp = ymd_hms(timestamp))  # Or adapt if already correct

truths_market <- truths_market %>%
  mutate(post_time = ymd_hms(date_time_parsed))  # Rename or reuse as needed

# Join on exact datetime
truths_market <- truths_market %>%
  left_join(snp_data, by = c("post_time" = "timestamp"))
```


```{r}
truths_market <- truths_market %>%
  mutate(minute_return = open - close)

truths_market <- truths_market %>%
  filter(!is.na(open))
```

```{r}

# First, ensure tokens are characters and unnest them
truths_unnested <- truths_market %>%
  select(minute_return, tokens) %>%
  mutate(tokens = lapply(tokens, as.character)) %>%
  unnest(tokens) %>%
  mutate(token_present = 1)

# Now, we widen the data, making sure only tokens are pivoted
token_wide_df <- truths_unnested %>%
  pivot_wider(
    names_from = tokens,
    values_fn = max,
    values_from = token_present,
  )

# Check the result
print(token_wide_df)
```


```{r}
library(randomForest)
library(dplyr)

#Replace NA with 0
token_wide_df[ , -1] <- token_wide_df[ , -1] %>% replace(is.na(.), 0) #due to problems to do it during change we only now convert the NA to 0s


```


```{r}
# Random Forest Model
train_index <- sample(1:nrow(token_wide_df), size = 0.8 * nrow(token_wide_df))  # 80% training data
train_data <- token_wide_df[train_index, ]
test_data <- token_wide_df[-train_index, ]

rf_model <- randomForest(minute_return ~ ., data = train_data, importance = TRUE, ntree = 100)
importance(rf_model)
varImpPlot(rf_model)
```

```{r}
colnames(token_wide_df) <- make.names(colnames(token_wide_df))


```




