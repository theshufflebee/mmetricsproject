---
title: "The Art of the Tweet: Do Trump's Posts Affect Market Volatility?"
author: "Marcos Constantinou, Ryan Fellarhi & Jonas Bruno"
date: "25.05.2025"
output:
  bookdown::pdf_document2: 
    toc: true
    fig_caption: true
    latex_engine: lualatex
header-includes:
  - \usepackage{amsmath}
  - \usepackage{geometry}
  - \usepackage{placeins}
  - \usepackage{float}
geometry: "left=2cm,right=2cm,top=1cm,bottom=2cm"
bibliography: [packages.bib, citations.bib] #name bib files
link-citations: true           # optional but helpful for clickable citations
csl: apa.csl
abstract: "In this short paper, we aim to asses to what extent financial markets may react to Donald Trump’s social media posts, and more specifically, the effect on average realised volatility. We do so using both ARMA-X and SVAR models, with data spanning the 1st of January 2014, to the 7th of May 2025, over various time horizons and variables. We include the number of posts, a dummy for whether there was a post, and counts for mentions of words like tariffs, trade and China. While we do find significant and large
impacts of mentions of tariffs in the ARMA-X model, with well-behaved residuals when restricting our sample to the second term, all other models and specifications suffer from persistent auto-correlation
in the residuals and often high standard errors. Overall, we find limited evidence that there is a statistically significant positive effect, and provide some explanations as to why this might be the case."
---
\newpage

While we do find significant impacts of mentions of tariff and and tweets with our ARMA-X model, especially when we restrict our sample to his second term, limitations such as autocorrelation...



```{r setup, warning=FALSE, echo=FALSE, message=FALSE}
require("here")
require("stringr")
require("dplyr")
require("ggplot2")
require("lubridate")
require("tinytex")

truths_raw <- read.csv(here("data/mothership", "social.csv"))

truths <- truths_raw %>%
  mutate(
    # Use POSIX 'timestamp' directly
    date_time_parsed = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S"),
    
    # Extract date only for plot
    day = as.Date(date_time_parsed),
    
    # Extract time only for plot
    time = format(date_time_parsed, "%H:%M"),
    
    # Convert time to numeric hours & minutes as fractions
    time_numeric = hour(date_time_parsed) + minute(date_time_parsed) / 60,
    
    # Shift time such that y = 0 corresponds to 12 PM
    time_shifted = time_numeric - 12
  )

```


# Introduction

## Motivation

Over the past 15 years, social media has become an important
communication tool for politicians. One of the pioneers of this novel
approach has been Donald Trump, the 45th and 47th President of the United
States. Since his ban on Twitter after the January 6th riots, his quantity of
social media posts has drastically increased to absurd levels as clearly visible
on Figure \@ref(fig:fig1).

The content of his posts can sometimes have announcements or teases of future
political decisions. Note the recent infamous "THIS IS A GREAT TIME TO BUY!!! DJT"
post sent just an hour before lifting his reciprocal tariffs. It is then not 
improbable that agents in financial markets might take this information into 
account in their decision making. This question has been asked before in the 
literature, primarily focusing on his first term. 

This brings us to our research question:  Do Donald Trump's posts impact market volatility?

```{r fig1, echo=FALSE, fig.cap="Number of Twitter & Truth Social Posts (EDT, red lines represent open market hours)", warning=FALSE, fig.width=6, fig.height=4}
 #Create the scatter plot
ggplot(truths, aes(x = day, y = time_shifted)) +
  geom_point(alpha = 0.5, color = "blue", size = 0.2) +  # Transparancy to create "heatmap"
  scale_y_continuous(
    breaks = seq(-12, 12, by = 3),  # Custom Y scale
    labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
  ) +
  labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts",
       x = "",
       y = "Time of Day") +
  theme_minimal() +
  
  
  # Customize X Axis
  scale_x_date(
    date_labels = "%b %Y",  # Format labels to show month and year
    date_breaks = "9 months"
  ) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Add vertical lines at 9:30 AM and 4:00 PM for stock market
  geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") + 
  geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +   
  
  # theme adjustments
  theme(
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10))

```


## Literature Review

Information is one of the most valuable assets in the financial market.
Its importance lies at the core of the "Efficient Market Hypothesis", 
which states that the prices of assets fully reflect all
available information, adjusting immediately to any new data
(@famaAdjustmentStockPrices2003), and thereby creating a strong demand
for information flow. In addition, the “Mixture of Distribution
Hypothesis” states that the release of new information is closely linked
to movements in both realized and implied volatility
(@andersenReturnVolatilityTrading1996, @frenchStockReturnVariances1986,
@vlastakisInformationDemandStock2012).

Consequently, a large part of the literature has focused on the relationship
between announcements, news and market activity. For example,
@schumakerTextualAnalysisStock2009 use various linguistic and textual
representations derived from financial news to predict stock market
prices. Similarly, @ederingtonHowMarketsProcess1993 analyze the impact
of macroeconomic news announcements on interest rate and foreign
exchange futures markets, particularly in terms of price changes and
volatility. Both studies, among others, find that prices, such as stock
prices, react primarily within minutes after the release of new
information.

Recently, the world has witnessed the rise of the Internet
which revolutionized the dissemination and accessibility of information.
Social media enables investors, analysts or politicians to instantly
share their information, news or opinions. This led some studies to
focus on the communication dynamics of social platforms to predict
changes in the returns of financial assets (@dechoudhuryCanBlogCommunication2008 and 
@bartovCanTwitterHelp2018). In this context, the
impact of Trump’s tweets on various financial and macroeconomic
variables has been analysed by several studies, especially during his
first mandate. Using high-frequency financial data,
@gjerstadPresidentTrumpsTweets2021 found consistent increases in uncertainty and
trading volume, along with a decline in the U.S. stock market, regardless
of tweet content. It is relevant to note however, that the effect was stronger when 
Trump used confrontational words such as "tariff" or "trade war." Some of his
announcements also influenced the U.S. dollar exchange rate (@vlastakisInformationDemandStock2012)
and certain market indices within minutes of the tweet being posted
(@colonescuEffectsDonaldTrumps2018 and @kinyuaAnalysisImpactPresident2021). Furthermore, scholars have shown that negative Trump tweets about specific
companies tended to reduce demand for their stocks (@bransHisThumbEffect2020 and
@mendelsStanfordResearchSeries2019), whereas some
others have shown that they also impact market volatility indices such as
the VIX (@fendelPoliticalNewsStock2019) or the Volfele (@klausMeasuringTrumpVolfefe2021).
The effects of his tweets also extended beyond the U.S.. For example,
@nishimuraImpactsDonaldTrumps2025 show a
positive relationship between volatility in European stock markets and
Twitter activity of Trump, and this effect tends to intensify as public
interest for his tweet grows.






# Data

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)

getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

## Financial Data

For our financial data, we decided to use minute-by-minute prices for 
broad market indices. Since the actual indices do not update their prices that often,
we had to take proxies under the form of ETFs that track them. These ETFs exist to provide investors with direct exposure to whole markets and have very small to no deviations to the corresponding index. Our 3 markets of analysis are: SPY to track the S&P500 (note that this ETF has a very large trading volume relative to the others), VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage[^1]. Our timeframe starts on the first 
of January 2014 and goes to the 7th of May 2025.

[^1]: https://www.alphavantage.co/ 

We had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is \textit{realised} market volatility. We did so 
using the following formula:
$$
\begin{aligned}
  AHV_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close), $i$ represents
every minute, $t$ represents an hour, and $N$ is the total number of minutes in each hour.

Ultimately, we compute the AHV for each open market hour since 2014. Note 
that the first hour is from 9:30 am to 10:00 am since the market opens on a half-hour 
but closes at 4:00 pm. Plotting this data, we observe that the last few months (corresponding
to Donald Trump's first 100 days in office) display 
unprecedented levels of volatility which have reached, and even surpassed, levels seen
during the COVID-19 pandemic.

```{r fin plots, message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE}

hvol_plotter(SPY, breaks = "yearly", title = "SPY Realised Volatility")

```




## Political Data

We have two types of data for Trump's posts, Tweets \& "Truths" (from Truth Social). 
The Tweets are sourced from Kaggle (@shantanuDonaldTrumpTweets) and stop in January 2021,
seen as Trump was then banned. Due to this, we have a gap in this data going until February 2022, 
when he first posted on his own new platform. All Truth Social posts were pulled from
"trumpstruth.org", a webpage that aims to conserve all his posts. Note that we have 
had to use web-scrapping methods in order to download all these posts in a dataset.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove such a large amount of observations, that it severly limited 
our analysis. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as a critical assumption[^2]. 

[^2]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 

Since our financial data is hourly, we aggregate the social data by hour as well and construct multiple variables from this. These variables include
a dummy for whether there was a post in a particular hour ($TweetDummy$), the number of posts in an hour 
($TweetCount$), and counts
for mentions of certain words ($Tariff$, $Trade$, \& $China$). 

Furthermore, we applied simple sentiment analysis algorithms in order to extract emotions and proportions for the amount of "positive" and "negative" words from the posts. We however ended up not 
including these in our final analyses as results were not particularly interesting. Details on all our data management procedures as well as the final dataset can be found in the GitHub repository.
```{r social plots, message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE}

#find count
tweetcount_alltime = dplyr::select(data,timestamp,N)
#select time period
tweetcount = filter(tweetcount_alltime,
between(timestamp,
as.Date('2014-01-01'),
as.Date('2025-04-10')))
#plot
ggplot(tweetcount_alltime, aes(x = timestamp, y = N)) +
geom_point(color = "#253494", size = 1) +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
labs(title = "Trump Social Media Count",
x = NULL,
y = "number of tweets/truths") +
theme_minimal(base_size = 14) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(face = "bold", hjust = 0.5))

```






# ARMA-X

## Methodology

We first thought of a simple ARMA-X type specification, taking the AHV as our
"y variable" and taking any of the social media variables as the exogenous
regressors. The assumption here is that, while the market reacts to Trump posts,
Trump's posts are chaotic, nonsensical, and random enough to be considered 
exogenous. 

We of course first start by checking stationarity of our variables (using ADF tests), where we find
p-values of 0.01 suggesting that the processes are not explosive. Then, we use 
a custom function in order to choose the number of lags based on the AIC criterion.
This would often choose a very high number of lags, which could be 
explained by our data being hourly. As such we decided to put a limit of 3 lags,
which sees minimal AIC loss, similar results, and allows considerable simplification of the models. 
Our specifications follow the standard formula:
$$
AHV_t = \phi_1 AHV_{t-1} + \phi_2 AHV_{t-2} + \cdots + \phi_p AHV_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q} + \beta_0 x_t + \beta_1 x_{t-1} + \cdots + \beta_r x_{t-r}
$$

## Results


### Full Timeframe
We run models with the following exogenous regressors: $TweetDummy$, $TweetCount$,
and the mentions of words $Tariff$, $Trade$, and $China$. We first note in Table 
\@ref(tab:armax) that all the x-regressors are significant,
apart from $Trade$. Notice also that all the coefficients (apart from $Tariff_{t-3}$)
are positive, in line with our main hypothesis. The effect of $Tariff_{t-1}$ and 
$Tariff_{t-2}$ are especially large, given the average size of the volatility being 
about 0.023 over the whole sample. We in fact predict that an 
extra mention of tariffs one hour ago leads to a whopping extra 0.02 in volatility
which means it would just about double the AHV if at the average. We can see the
impulse response function (IRF) for this shock, in Figure \@ref(fig:armaxirf1). Notice that
there is a large positive response in the first periods, and then a graduate
decline over time. Something to note is that in our various specifications, when including
MA terms, the decline shows up gradual while being much sharper when only including
AR terms. 
We also ran all these models on the VGK and ASHR ETFs, though no
significant results appear apart from a small but statistically significant effect
of the tariff variable for VGK. It is worthy to note that the average volatility in those
markets are much lower than for SPY, as the trading volume is much lower.

### Split Samples
We then split our sample for the first and second term of the Trump presidency to 
explore whether there has been a shift in how markets respond from the first presidency.
We only run models using $Tariff$, $Trade$ and $China$. As seen on Table
\@ref(tab:armaxterm), the first interesting result
is in the coefficients of $Tariff$ being significant and very large in the second
term, while being small and not statistically significant in the first. A similar
story goes for the $China$ variable. This may lend some evidence to support the 
claim that investors are much more reactive to Trump's social media presence
now than before. We've found similar IRFs as for the full timeframe. 
Finally, we check the residuals of all these models with Ljung-Box tests. 
We find that p-values are zero for the full timeframe \& first term models, which 
suggests that there is significant auto-correlation and that these 
estimations are then problematic. However, for the second term, the p-values are quite high 
(~ 0.8 for $Tariff$), giving our split sample models more validity relative to the full time frame models.

These results show that perhaps ARMA-X may not be the right choice in this context 
as it is not unreasonable to think that Trump does in fact react to market movements, 
violating the exogeneity assumption that is critical for this type of model. With this information, we decide to run SVAR specifications to account for possible endogeneity.




# SVAR

## Methodology
We develop an SVAR model in order to assess the impact of
short-run shocks from Trump's posts on AHV, and to evaluate whether
market volatility can, in turn, influence Trump's posting behaviour. In
this framework, we systematically pair AHV with one explanatory variable
at a time (our x-regressor). The SVAR approach offers the advantage of accounting for
structural endogeneity. Our main assumption is that the volatility
does not contemporaneously affect Trump's posting activity - neither
quantitatively nor qualitatively - while Trump's posts do affect markets instantly. 
In essence, we impose a short-run restriction on the shock of volatility for all the
social media variables.

Based on the information criteria, we found similar results across all
specifications, with a recommended lag length of around 70. However,
including more than 6 lags (corresponding to a full trading day)
introduces strong seasonal patterns. Moreover, the higher the number of lags,
the greater the persistence of a shock up to unrealistic levels such as 150 days 
for the number of Tweets, which seems implausible. Therefore, we chose to fix the
number of lags at a maximum of 6. Finally, given the presence of heteroscedasticity
and serial correlation in the residuals, we use the Newey-West estimator
to compute robust standard errors. Our specification is built as follows:
$$
\begin{aligned}
y_t = c + \Phi_1 y_{t-1} + \cdots +
\Phi_6 y_{t-6} + B \eta_t, \hspace{0.2cm} \text{where:} \hspace{0.2cm}
y_t =
\begin{bmatrix}
X_t \\
AHV_t
\end{bmatrix},
 \quad B =
\begin{bmatrix}
b_{11} & 0 \\
b_{21} & b_{22}
\end{bmatrix}, \quad
\eta_t =
\begin{bmatrix}
\eta^{X}_t \\
\eta^{AHV}_t
\end{bmatrix}
\end{aligned}
$$

## Results

### Full Timeframe

As in the ARMA-X framework, we initially estimate a model for each of our five
main variables across the full dataset. Table \@ref(tab:VAR) shows all estimations using the SPY ETF, where
we notice that the positive coefficients (of the social media variables) are large 
but not statistically significant. Oddly, the only significant coefficients are
consistently negative.

For the $Tariff$, $Trade$ and $China$ variables, the first, second and sometimes fourth 
lags are positive and relatively large (especially in the case of $Tariff$), while 
the remaining ones are not. In contrast, for $TweetCount$ and $TweetDummy$, we 
observe fewer and smaller positive coefficients. At the same time, we find 
that the contemporaneous effects of the shocks are all positive and relatively 
strong. This leads to two types of scenarios : either 
the IRFs experience a positive shock and remain elevated ($Tariff$, $Trade$ and $China$), 
or a highly positive shock occurs, but the cumulative effect turns negative after a few 
hours ($TweetDummy$ and $TweetCount$). You can find the IRFs for $Tariff$ on Figures 
\@ref(fig:VARirf1) and \@ref(fig:VARirf2).

Finally, apart for $Tariff$, all Granger causality tests indicate that Trump's 
posts Granger-cause volatility. However, due to serial correlation in the 
residuals, these results should be interpreted with extreme caution. Overall, this model 
suggests that Trump's posts tend to have a positive instantaneous effect on 
volatility, but with very low persistence.
When analyzing the VGK \& ASHR ETFs, we observe similar patterns 
though with lower magnitude, except for the impact of $Tariff$ and $China$ on 
ASHR, where the cumulative effects show no positive impact. Additionally, 
the VGK ETF appears to react more strongly than ASHR to 
Trump's posts, especially those mentioning $Trade$ and $Tariff$.

Regarding the impact of AHV on Trump's posts, we find some evidence of a 
negative effect. For all variables, we observe one or two significantly 
negative coefficients, typically on the first and fourth lag, alongside many 
insignificant ones. However, only $TweetCount$ and $China$ pass the Granger test in 
the SPY ETF. Surprisingly, a large number of Granger tests in the VGK and 
ASHR ETFs indicate strong Granger causality, which may point to a limitation 
of the test itself, as such results appear unrealistic. 


### Split Sample

Tables \@ref(tab:VARf) and \@ref(tab:VARs) show the models for the split terms,
where we notice the results are strikingly similar. While we
observe relatively small shock effects and almost entirely negative coefficients 
during the first term, (which explain why the 
cumulative IRFs indicate a negative impact of posts), the shock effects in the
second term are substantially larger, ranging from 5 times (for $TweetCount$ and 
$TweetDummy$) to as much as 25 times greater (for $Tariff$). The only exception is 
$Trade$ in the second term, which shows the only negative impact from a shock.
Once again we find positive lagged coefficients in the second term, mostly on 
the first, second and fourth lags. However, none of these coefficients are
statistically significant though the cumulative IRFs clearly show a high positive 
impact on everything except for $TweetDummy$ and $TweetCount$, whose
coefficients and cumulative IRFs display similar patterns to those observed 
in the first term. 

Moreover, the Granger tests generally failed in both terms, with the sole
exception being $China$ in the second term. 
Regarding the ASHR ETF, we found results similar to those for SPY. Surprisingly, 
in the case of VGK, we observe a positive
impact of Trump’s posts on AHV during the first term. Nevertheless, the results 
still indicate a stronger impact of posts during the second term.

# Conclusion

We started this project with the intention of understanding whether the impact of Trump’s social media posts affect financial markets, and to see if there is perhaps a difference from his first presidential mandate. After various headaches with our data, we first ran ARMA-X models where we found significant and positive results albeit with strong auto-correlation in the errors, with only the second term analysis offering more convincing results. We then tried SVAR models for a possibly more accurate picture, though with little to no success. We once again saw strong auto-correlation in the errors, which we accounted for by using Newey-West standard errors. We found that the only significant coefficients are actually negative, suggesting Trump’s social media presence would actually \textit{reduce} volatility. There is, however, a consistent pattern in the signs and magnitude of the SVAR coefficients (particularly for $Tariff$ \& $China$)
and the fact that the standard errors are large may reflect a lack of precision in 
the selection of the shocks. It might well be that there are two types of social media posts:
information and noise. That is to say, certain posts may be completely disregarded by investors
as, for instance, emotional outbursts, personal attacks, or other financially irrelevant
remarks, while others would be treated as official policy statements with concrete
consequences to the economy. If this were to be the case, our coefficients would be 
biased downwards, underestimating the impact of the \textit{relevant} posts and would explain 
the high standard errors. A way to counteract this could be to find a way to filter 
the social media posts dataset for only the financially relevant shocks, possibly 
by using more sophisticated sentiment analysis.

Altogether, we would strongly suggest against trying to interpret these results given that the models seem to not fit particularly well. This may be due to seasonality in our data (a common trend seen in our daily AVH being high volatility in the first open hours, and a gradual slowdown for the rest of the day), or to our handling of non-market hours. Further work could look at exploring said issues in greater depth, further complicate the models by adding more variables and interactions between them, and/or additionally use more sophisticated models with very large lag counts. 

\clearpage


# References

<div id="refs"></div>

\newpage

# Appendix



```{r library_setup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(kableExtra)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```


## ARMAX

```{r datasetup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#backup
backup = data

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))
Vdata = data

#for interpretation
mean1 = mean(data$SPY_vol)

```


```{r fitting models, results=F, warning=F, message=F, echo=FALSE}
#All SPY models for the full Dataframe

models <- list()

# ARMA-X(3,3,1) with Tweet Dummy as Exogenous
models[["Model 1"]] <- armax(data$SPY_vol, xreg = data$dummy, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,3,1) with Tweet Count as Exogenous
models[["Model 2"]] <- armax(data$SPY_vol, xreg = data$N, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Model 3"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 3, p = 3, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Model 4"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 1, p = 3, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Model 5"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 2) 

```

### SPY ARMA-X Models (Jan 2014 - May 2025) 

\FloatBarrier

```{r armax1, results="asis", warning=F, message=F, echo=FALSE}

names = list( "ar1" = "AR(1)",
              "ar2" = "AR(2)",
              "ar3" = "AR(3)",
              "ma1" = "MA(1)",
              "ma2" = "MA(2)",
              "ma3" = "MA(3)",
              "(Intercept)" = "Constant",
              "dummy_lag_0" = "$TweetDummy_{t}$",
              "dummy_lag_1" = "$TweetDummy_{t-1}$",
              "N_lag_0" = "$TweetCount_{t}$",
              "N_lag_1" = "$TweetCount_{t-1}$",
              "tariff_lag_0" = "$Tariff_{t}$",
              "tariff_lag_1" = "$Tariff_{t-1}$",
              "tariff_lag_2" = "$Tariff_{t-2}$",
              "tariff_lag_3" = "$Tariff_{t-3}$",
              "trade_lag_0" = "$Trade_{t}$",
              "trade_lag_1" = "$Trade_{t-1}$",
              "china_lag_0" = "$China_{t}$")

table1 = texreg(models,
          custom.model.names = names(models), 
          custom.coef.map = names,
          caption = "ARMA-X Models of Average Hourly Volatility",
          caption.above = TRUE,
          label = "tab:armax",
          digits = 4,
          fontsize = "small",
          float.pos = 'H')
table1

```

\FloatBarrier

\newpage

### SPY ARMA-X IRF 

```{r armaxirf1, results="asis", warning=F, message=F, echo=FALSE, fig.cap="ARMA-X IRF", fig.width=6, fig.height=2.5}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

#irf.plot(models[["Model 1"]],nb.periods,title="Tweet Dummy Shock")
#irf.plot(models[["Model 2"]],nb.periods,title="Tweet Count Shock")
plot1 = irf.plot(models[["Model 3"]],nb.periods,
                 title="Tariff Mention Shock: Jan 2014 - May 2025")
plot1
#irf.plot(models[["Model 4"]],nb.periods,title="Trade Mention Shock")
#irf.plot(models[["Model 5"]],nb.periods,title="China Mention Shock")

#ggsave("armax_plot1.png",plot=plot1,bg="white")

```


```{r calc res, warning=F, message=F, echo=FALSE, results=F}
#Calculate residuals

res1 = checkresiduals(models[["Model 1"]], plot = FALSE)
res2 = checkresiduals(models[["Model 2"]], plot = FALSE)
res3 = checkresiduals(models[["Model 3"]], plot = FALSE)
res4 = checkresiduals(models[["Model 4"]], plot = FALSE)
res5 = checkresiduals(models[["Model 5"]], plot = FALSE)

```

```{r SPYres, warning=F, message=F, echo=FALSE}

resnames = c("Twitter Dummy", "Twitter Count", "Tariff", "Trade", "China")

#extract p-values directly from checkresiduals results
pvals <- data.frame("X-Regressor" = resnames,
                    `Full Timeframe` = c(
                      res1$p.value,
                      res2$p.value,
                      res3$p.value,
                      res4$p.value,
                      res5$p.value))


```




```{r datasetup first, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
#First term Calculations

#load final dataset
data = backup

#first term
data = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#for interpretation
mean2 = mean(data$SPY_vol)

```


```{r 1st term models, results=F, warning=F, message=F, echo=FALSE}
#SPY Models first Term

models <- list()

# ARMA-X(3,3,0) with Tariff Mentions as Exogenous
models[["First Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 3, q = 3)

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

```


```{r 1st SPYresiduals, warning=F, message=F, results=F, echo=FALSE}
# Run first term residuals
res6 = checkresiduals(models[["First Term (1)"]], plot = FALSE)
res7 = checkresiduals(models[["First Term (2)"]], plot = FALSE)
res8 = checkresiduals(models[["First Term (3)"]], plot = FALSE)

pvals_new1 <- data.frame(
  "First-Term" = c(
    NA,
    NA,
    res6$p.value,
    res7$p.value,
    res8$p.value))

```




```{r datasetup second, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
#Second Term Calculations

#load final dataset
data = backup

#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

#for interpretation
mean3 = mean(data$SPY_vol)

```


```{r 2nd term models, results=F, warning=F, message=F, echo=FALSE}
#Run Second Term Models

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 1, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

```



```{r 2nd SPYresiduals, results=F, warning=F, message=F, echo=FALSE}
# Calculate SPY residuals for the second term

res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)

pvals_new2 <- data.frame(
  "Second-Term" = c(
    NA,
    NA,
    res9$p.value,
    res10$p.value,
    res11$p.value))

#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)

```

### SPY ARMA-X Split Models

\FloatBarrier

```{r armax2, results="asis", warning=F, message=F, echo=FALSE}

xnames = list("ar1" = "AR(1)",
              "ar2" = "AR(2)",
              "ar3" = "AR(3)",
              "ma1" = "MA(1)",
              "ma2" = "MA(2)",
              "ma3" = "MA(3)",
              "(Intercept)" = "Constant",
              "tariff_lag_0" = "$Tariff_{t}$",
              "tariff_lag_1" = "$Tariff_{t-1}$",
              "tariff_lag_2" = "$Tariff_{t-2}$",
              "trade_lag_0" = "$Trade_{t}$",
              "china_lag_0" = "$China_{t}$",
              "china_lag_1" = "$China_{t-1}$",
              "china_lag_2" = "$China_{t-2}$")

table2 = texreg(models,
       custom.model.names = names(models), 
       custom.coef.map = xnames,
       caption = "Split-Term ARMA-X Models of Average Hourly Volatility",
       caption.above = TRUE,
       label = "tab:armaxterm",
       digits = 4,
       fontsize = "small",
       float.pos = "H")

table2

```

```{r armaxirf2, warning=F, message=F, echo=FALSE, eval=FALSE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

plot2 = irf.plot(models[["Second Term (1)"]],nb.periods,
                 title="Tariff Mention Shock - Second Term")
plot2

#ggsave("armax_plot2.png",plot=plot2,bg="white")

plot3 = irf.plot(models[["Second Term (3)"]],nb.periods,
                 title="China Mention Shock - Second Term")
plot3

```


```{r armax3, message=FALSE, warning=FALSE, results="asis", echo=FALSE, eval=FALSE}

table3 = knitr::kable(pvals_combined, digits = 100, booktabs=T,
             caption = "Ljung-Box Test p-values for Residuals")

table3

```

 
```{r means, message=F, warning=F, results="asis", echo=FALSE, eval=FALSE}

means <- data.frame(
  Timeframe = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
  `SPY Volatility Mean` = c(
    mean1,
    mean2,
    mean3))

table4 = knitr::kable(means, digits = 6, booktabs=T,
             caption = "Summary Statistics of SPY Volatility")

table4

```


## SVAR

```{r load packages for var, message=FALSE, warning=FALSE, echo = FALSE}
library("quantmod")
library("TSA")
library("aTSA")
library("rugarch")
library("rmgarch")
library("ggplot2")
library("tibble")
library("dplyr")
library("lubridate")
library(tibble)
library(tidyverse)
library(knitr)
library("fGarch")
library(FinTS)
library(kableExtra)
library(writexl)
library("purrr")
library("forecast")
library("texreg")
require(vars)
```

### SPY SVAR Models (Jan 2014 - May 2025) 

```{r Estime dummy, message=FALSE, warning=FALSE, echo=FALSE}
y = cbind(Vdata$dummy, Vdata$SPY_vol)
colnames(y)[1:2] <- c("dummy", "vol")
est.VAR <- VAR(y,p=6)

#extract results
mod_vol = est.VAR$varresult$vol
f = formula(mod_vol)
d = model.frame(mod_vol)
lm_clean = lm(f, data= d)

#apply Newey-West
nw_vcov = NeweyWest(lm_clean, lag=6)
nw_se = sqrt(diag(nw_vcov))

#t-stats
coef = coef(lm_clean)
t_stat = coef/nw_se

#recalculate p-values
robust = 2*(1-pt(abs(t_stat), df = df.residual(lm_clean)))




```

```{r B mat, message=FALSE, warning=FALSE, echo=FALSE}

#Recreate a Robust Omega Matrix
U = residuals(est.VAR)
T = nrow(U)
L = 6 #number of lag
Omega = matrix(0, ncol(U), ncol(U))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l_ = t(U[(l+1):T, , drop=FALSE]) %*% U[1:(T-l), , drop=FALSE] /T
  if (l == 0){
    Omega = Omega + Gamma_l_
  } else {
    Omega = Omega + weight*(Gamma_l_ + t(Gamma_l_))
  }
}


#make the B matrix
loss <- function(param){
  #Define the restriction
  B <- matrix(c(param[1], param[2], 0, param[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X <- Omega - B %*% t(B)
  
  #loss function
  loss <- sum(X^2)
  return(loss)
}

res.opt <- optim(c(1, 0, 1), loss, method = "BFGS")
B.hat <- matrix(c(res.opt$par[1], res.opt$par[2], 0, res.opt$par[3]), ncol = 2)

```

```{r estimate with N, message=FALSE, warning=FALSE, echo=FALSE}
y2 = cbind(Vdata$N, Vdata$SPY_vol)
colnames(y2)[1:2] <- c("N", "vol")
est.VAR2 <- VAR(y2,p=6)

#extract results
mod_vol2 = est.VAR2$varresult$vol
f2 = formula(mod_vol2)
d2 = model.frame(mod_vol2)
lm_clean2 = lm(f2, data= d2)

#apply Newey-West
nw_vcov2 = NeweyWest(lm_clean2, lag=6)
nw_se2 = sqrt(diag(nw_vcov2))

#t-stats
coef2 = coef(lm_clean2)
t_stat2 = coef2/nw_se2

#recalculate p-values
robust2 = 2*(1-pt(abs(t_stat2), df = df.residual(lm_clean2)))

```

```{r estime tariff, message=FALSE, warning=FALSE, echo=FALSE}
y3 = cbind(Vdata$tariff, Vdata$SPY_vol)
colnames(y3)[1:2] <- c("tariff", "vol")
est.VAR3 <- VAR(y3,p=6)

#extract results
mod_vol3 = est.VAR3$varresult$vol
f3 = formula(mod_vol3)
d3 = model.frame(mod_vol3)
lm_clean3 = lm(f3, data= d3)

#apply Newey-West
nw_vcov3 = NeweyWest(lm_clean3, lag=6)
nw_se3 = sqrt(diag(nw_vcov3))

#t-stats
coef3 = coef(lm_clean3)
t_stat3 = coef3/nw_se3

#recalculate p-values
robust3 = 2*(1-pt(abs(t_stat3), df = df.residual(lm_clean3)))

```

```{r B mat3, message=FALSE, warning=FALSE, echo=FALSE}
#Recreate a Robust Omega Matrix
U3 = residuals(est.VAR3)
T3 = nrow(U3)
Omega3 = matrix(0, ncol(U3), ncol(U3))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l_3 = t(U3[(l+1):T3, , drop=FALSE]) %*% U3[1:(T3-l), , drop=FALSE] /T3
  if (l == 0){
    Omega3 = Omega3 + Gamma_l_3
  } else {
    Omega3 = Omega3 + weight*(Gamma_l_3 + t(Gamma_l_3))
  }
}


#make the B matrix
loss3 <- function(param3){
  #Define the restriction
  B3 <- matrix(c(param3[1], param3[2], 0, param3[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X3 <- Omega3 - B3 %*% t(B3)
  
  #loss function
  loss3 <- sum(X3^2)
  return(loss3)
}

res.opt3 <- optim(c(1, 0, 1), loss3, method = "BFGS")
B.hat3 <- matrix(c(res.opt3$par[1], res.opt3$par[2], 0, res.opt3$par[3]), ncol = 2)

```

```{r estimate trade, message=FALSE, warning=FALSE, echo=FALSE}
y4 = cbind(Vdata$trade, Vdata$SPY_vol)
colnames(y4)[1:2] <- c("trade", "vol")
est.VAR4 <- VAR(y4,p=6)

#extract results
mod_vol4 = est.VAR4$varresult$vol
f4 = formula(mod_vol4)
d4 = model.frame(mod_vol4)
lm_clean4 = lm(f4, data= d4)

#apply Newey-West
nw_vcov4 = NeweyWest(lm_clean4, lag=6)
nw_se4 = sqrt(diag(nw_vcov4))

#t-stats
coef4 = coef(lm_clean4)
t_stat4 = coef4/nw_se4

#recalculate p-values
robust4 = 2*(1-pt(abs(t_stat4), df = df.residual(lm_clean4)))


```

```{r estime China, message=FALSE, warning=FALSE, echo=FALSE, results="asis"}

ychina = cbind(Vdata$china, Vdata$SPY_vol)
colnames(ychina)[1:2] <- c("china", "vol")
est.VARchina <- VAR(ychina,p=6)

#extract results
mod_volchina = est.VARchina$varresult$vol
fchina = formula(mod_volchina)
dchina = model.frame(mod_volchina)
lm_cleanchina = lm(fchina, data= dchina)

#apply Newey-West
nw_vcovchina = NeweyWest(lm_cleanchina, lag=6)
nw_sechina = sqrt(diag(nw_vcovchina))

#t-stats
coefchina = coef(lm_cleanchina)
t_statchina = coefchina/nw_sechina

#recalculate p-values
robustchina = 2*(1-pt(abs(t_statchina), df = df.residual(lm_cleanchina)))

```


```{r vartable1, message=FALSE, warning=FALSE, results='asis', echo=FALSE}

dt_t = d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)


f_t <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model <- lm(f_t, data = dt_t)

dt_t2 = d2 %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)



f_t2 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model2 <- lm(f_t2, data = dt_t2)


dt_t3 = d3 %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)



f_t3 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model3 <- lm(f_t3, data = dt_t3)


dt_t4 = d4 %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)
  


f_t4 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model4 <- lm(f_t4, data = dt_t4)

dt_tchina = dchina %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)



f_tchina <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
modelchina <- lm(f_tchina, data = dt_tchina)


nw_se_t <- sqrt(diag(sandwich::NeweyWest(model, lag = 6)))
nw_se2_t <- sqrt(diag(sandwich::NeweyWest(model2, lag = 6)))
nw_se3_t <- sqrt(diag(sandwich::NeweyWest(model3, lag = 6)))
nw_se4_t <- sqrt(diag(sandwich::NeweyWest(model4, lag = 6)))
nw_sechina_t <- sqrt(diag(sandwich::NeweyWest(modelchina, lag = 6)))


robust_t <- 2 * (1-pt(abs(coef(model) / nw_se_t), df = df.residual(model)))
robust2_t <- 2 * (1-pt(abs(coef(model2) / nw_se2_t), df = df.residual(model2)))
robust3_t <- 2 * (1-pt(abs(coef(model3) / nw_se3_t), df = df.residual(model3)))
robust4_t <- 2 * (1-pt(abs(coef(model4) / nw_se4_t), df = df.residual(model4)))
robustchina_t <- 2 * (1-pt(abs(coef(modelchina) / nw_sechina_t), df = df.residual(modelchina)))



nw_se_t       <- nw_se_t[names(coef(model))]
robust_t      <- robust_t[names(coef(model))]
nw_se2_t      <- nw_se2_t[names(coef(model2))]
robust2_t     <- robust2_t[names(coef(model2))]
nw_se3_t      <- nw_se3_t[names(coef(model3))]
robust3_t     <- robust3_t[names(coef(model3))]
nw_se4_t      <- nw_se4_t[names(coef(model4))]
robust4_t     <- robust4_t[names(coef(model4))]
nw_sechina_t  <- nw_sechina_t[names(coef(modelchina))]
robustchina_t <- robustchina_t[names(coef(modelchina))]




# Create list for models
models_list <- list(model, model2, model3, model4, modelchina)

# Create list for robust SEs
robust_ses <- list(nw_se_t, nw_se2_t, nw_se3_t, nw_se4_t, nw_sechina_t)

# Create list for p-value
robust_pvals <- list(robust_t, robust2_t, robust3_t, robust4_t, robustchina_t)

# Name of Variables
custom_names <- list(
  "vol.l1" = "$AHV_{t-1}$",
  "vol.l2" = "$AHV_{t-2}$",
  "vol.l3" = "$AHV_{t-3}$",
  "vol.l4" = "$AHV_{t-4}$",
  "vol.l5" = "$AHV_{t-5}$",
  "vol.l6" = "$AHV_{t-6}$",
  "X.l1" = "$X_{t-1}$",
  "X.l2" = "$X_{t-2}$",
  "X.l3" = "$X_{t-3}$",
  "X.l4" = "$X_{t-4}$",
  "X.l5" = "$X_{t-5}$",
  "X.l6" = "$X_{t-6}$",
  "const" = "Constant"
)

# Generate table
table_texreg <- texreg(
  l = models_list,
  override.se = robust_ses,
  custom.coef.map = custom_names,
  override.pvalues = robust_pvals,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  caption = "SVAR Models of Average Hourly Volatility",
  label = "tab:VAR",
  caption.above = TRUE,
  digits = 4,
  fontsize = "small",
  float.pos = "H",
  custom.gof.rows = list("Shock (IRF)" = c(0.0041713, 0.003061, 0.001189, 0.000215, 0.001937)),
  custom.note = "\\parbox{.4\\linewidth}
{\\vspace{2pt}Each SVAR regression has only two variables: AHV and X. The column names represent the X variable for the selected model. \\\\ %stars.}")

# print
table_texreg

```

### SPY SVAR IRFs 

```{r VARirf1, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="SVAR IRF 1", fig.width=6, fig.height=2.3}
nb.sim = 7*7
#get back the coefficient of est.VAR
phi3 <- Acoef(est.VAR3)
PHI3 = make.PHI(phi3)

#take the constant
constant3 <- sapply(est.VAR3$varresult, function(eq) coef(eq)["const"])
c3=as.matrix(constant3)

#Simulate the IRF
p3 <- length(phi3)
n3 <- dim(phi3[[1]])[1]

Y3 <- simul.VAR(c=c3, Phi = phi3, B = B.hat3, nb.sim ,y0.star=rep(0, n3*p3),
                  indic.IRF = 1, u.shock = c(1,0))


#Plot the IRF
Yd3 = data.frame(
  period = 1:nrow(Y3),
  response = Y3[,2])

ggplot(Yd3,aes(x=period, y=response)) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("Tariff Mention Shock: Jan 2014 - May 2025") +
  ylab("")+
  xlab("Hours after shock") +
  theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r VARirf2, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="SVAR IRF 2", fig.width=6, fig.height=2.5, results='asis', fig.pos="H"}
ggplot(Yd3,aes(x=period, y=cumsum(response))) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("Tariff Mention Shock (Cumulative): Jan 2014 - May 2025") +
  ylab("")+
  xlab("Hours after shock") +
  theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### SPY SVAR First-Term Models

```{r datasetup2, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

# First and Second Mandate

#first term
Vdata_f = filter(Vdata,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#second term
Vdata_s = filter(Vdata,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

```

```{r first mandate dum, warning=FALSE, message=FALSE, results="asis", echo=FALSE}
y_f_d = cbind(Vdata_f$dummy, Vdata_f$SPY_vol)
colnames(y_f_d)[1:2] <- c("dummy", "vol")
est.VAR_f_d <- VAR(y_f_d,p=6)

#extract results
mod_vol_f_d = est.VAR_f_d$varresult$vol
f_f_d = formula(mod_vol_f_d)
d_f_d = model.frame(mod_vol_f_d)
lm_clean_f_d = lm(f_f_d, data= d_f_d)

#apply Newey-West
nw_vcov_f_d = NeweyWest(lm_clean_f_d, lag=6)
nw_se_f_d = sqrt(diag(nw_vcov_f_d))

#t-stats
coef_f_d = coef(lm_clean_f_d)
t_stat_f_d = coef_f_d/nw_se_f_d

#recalculate p-values
robust_f_d = 2*(1-pt(abs(t_stat_f_d), df = df.residual(lm_clean_f_d)))

```

```{r first mandate N, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_f_n = cbind(Vdata_f$N, Vdata_f$SPY_vol)
colnames(y_f_n)[1:2] <- c("N", "vol")
est.VAR_f_n <- VAR(y_f_n,p=6)

#extract results
mod_vol_f_n = est.VAR_f_n$varresult$vol
f_f_n = formula(mod_vol_f_n)
d_f_n = model.frame(mod_vol_f_n)
lm_clean_f_n = lm(f_f_n, data= d_f_n)

#apply Newey-West
nw_vcov_f_n = NeweyWest(lm_clean_f_n, lag=6)
nw_se_f_n = sqrt(diag(nw_vcov_f_n))

#t-stats
coef_f_n = coef(lm_clean_f_n)
t_stat_f_n = coef_f_n/nw_se_f_n

#recalculate p-values
robust_f_n = 2*(1-pt(abs(t_stat_f_n), df = df.residual(lm_clean_f_n)))
```

```{r first mandate tariff, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_f_ta = cbind(Vdata_f$tariff, Vdata_f$SPY_vol)
colnames(y_f_ta)[1:2] <- c("tariff", "vol")
est.VAR_f_ta <- VAR(y_f_ta,p=6)

#extract results
mod_vol_f_ta = est.VAR_f_ta$varresult$vol
f_f_ta = formula(mod_vol_f_ta)
d_f_ta = model.frame(mod_vol_f_ta)
lm_clean_f_ta = lm(f_f_ta, data= d_f_ta)

#apply Newey-West
nw_vcov_f_ta = NeweyWest(lm_clean_f_ta, lag=6)
nw_se_f_ta = sqrt(diag(nw_vcov_f_ta))

#t-stats
coef_f_ta = coef(lm_clean_f_ta)
t_stat_f_ta = coef_f_ta/nw_se_f_ta

#recalculate p-values
robust_f_ta = 2*(1-pt(abs(t_stat_f_ta), df = df.residual(lm_clean_f_ta)))

```

```{r first mandate trade, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_f_tr = cbind(Vdata_f$trade, Vdata_f$SPY_vol)
colnames(y_f_tr)[1:2] <- c("trade", "vol")
est.VAR_f_tr <- VAR(y_f_tr,p=6)

#extract results
mod_vol_f_tr = est.VAR_f_tr$varresult$vol
f_f_tr = formula(mod_vol_f_tr)
d_f_tr = model.frame(mod_vol_f_tr)
lm_clean_f_tr = lm(f_f_tr, data= d_f_tr)

#apply Newey-West
nw_vcov_f_tr = NeweyWest(lm_clean_f_tr, lag=6)
nw_se_f_tr = sqrt(diag(nw_vcov_f_tr))

#t-stats
coef_f_tr = coef(lm_clean_f_tr)
t_stat_f_tr = coef_f_tr/nw_se_f_tr

#recalculate p-values
robust_f_tr = 2*(1-pt(abs(t_stat_f_tr), df = df.residual(lm_clean_f_tr)))
```

```{r first mandate china, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_f_ch = cbind(Vdata_f$china, Vdata_f$SPY_vol)
colnames(y_f_ch)[1:2] <- c("china", "vol")
est.VAR_f_ch <- VAR(y_f_ch,p=6)

#extract results
mod_vol_f_ch = est.VAR_f_ch$varresult$vol
f_f_ch = formula(mod_vol_f_ch)
d_f_ch = model.frame(mod_vol_f_ch)
lm_clean_f_ch = lm(f_f_ch, data= d_f_ch)

#apply Newey-West
nw_vcov_f_ch = NeweyWest(lm_clean_f_ch, lag=6)
nw_se_f_ch = sqrt(diag(nw_vcov_f_ch))

#t-stats
coef_f_ch = coef(lm_clean_f_ch)
t_stat_f_ch = coef_f_ch/nw_se_f_ch

#recalculate p-values
robust_f_ch = 2*(1-pt(abs(t_stat_f_ch), df = df.residual(lm_clean_f_ch)))

```

```{r table first term, message=FALSE, warning=FALSE, results='asis', echo=FALSE, eval=TRUE}


#first

d_f_d_t = d_f_d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)

f_t_f_d <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_d <- lm(f_t_f_d, data = d_f_d_t)

d_f_n_t = d_f_n %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)

f_t_f_n <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_n <- lm(f_t_f_n, data = d_f_n_t)

d_f_ta_t = d_f_ta %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)

f_t_f_ta <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_ta <- lm(f_t_f_ta, data = d_f_ta_t)


d_f_tr_t = d_f_tr %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)

f_t_f_tr <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_tr <- lm(f_t_f_tr, data = d_f_tr_t)


d_f_ch_t = d_f_ch %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)

f_t_f_ch <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_ch <- lm(f_t_f_ch, data = d_f_ch_t)




nw_se_f_d_t <- sqrt(diag(sandwich::NeweyWest(model_f_d, lag = 6)))
nw_se_f_n_t <- sqrt(diag(sandwich::NeweyWest(model_f_n, lag = 6)))
nw_se_f_ta_t <- sqrt(diag(sandwich::NeweyWest(model_f_ta, lag = 6)))
nw_se_f_tr_t <- sqrt(diag(sandwich::NeweyWest(model_f_tr, lag = 6)))
nw_se_f_china_t <- sqrt(diag(sandwich::NeweyWest(model_f_ch, lag = 6)))


robust_f_d_t <- 2 * (1-pt(abs(coef(model_f_d) / nw_se_f_d_t), df = df.residual(model_f_d)))
robust_f_n_t <- 2 * (1-pt(abs(coef(model_f_n) / nw_se_f_n_t), df = df.residual(model_f_n)))
robust_f_ta_t <- 2 * (1-pt(abs(coef(model_f_ta) / nw_se_f_ta_t), df = df.residual(model_f_ta)))
robust_f_tr_t <- 2 * (1-pt(abs(coef(model_f_tr) / nw_se_f_tr_t), df = df.residual(model_f_tr)))
robust_f_ch_t <- 2 * (1-pt(abs(coef(model_f_ch) / nw_se_f_china_t), df = df.residual(model_f_ch)))


nw_se_f_d_t       <- nw_se_f_d_t[names(coef(model_f_d))]
robust_f_d_t      <- robust_f_d_t[names(coef(model_f_d))]
nw_se_f_n_t      <- nw_se_f_n_t[names(coef(model_f_n))]
robust_f_n_t     <- robust_f_n_t[names(coef(model_f_n))]
nw_se_f_ta_t      <- nw_se_f_ta_t[names(coef(model_f_ta))]
robust_f_ta_t     <- robust_f_ta_t[names(coef(model_f_ta))]
nw_se_f_tr_t     <- nw_se_f_tr_t[names(coef(model_f_tr))]
robust_f_tr_t     <- robust_f_tr_t[names(coef(model_f_tr))]
nw_se_f_china_t  <- nw_se_f_china_t[names(coef(model_f_ch))]
robust_f_ch_t <- robust_f_ch_t[names(coef(model_f_ch))]


# list of models, SE and p-value
models_list_f <- list(model_f_d, model_f_n, model_f_ta, model_f_tr, model_f_ch)
robust_ses_f <- list(nw_se_f_d_t, nw_se_f_n_t, nw_se_f_ta_t, nw_se_f_tr_t, nw_se_f_china_t)
robust_pvals_f <- list(robust_f_d_t, robust_f_n_t, robust_f_ta_t, robust_f_tr_t, robust_f_ch_t)

# Name of coefficient
custom_names <- list(
  "vol.l1" = "$AHV_{t-1}$",
  "vol.l2" = "$AHV_{t-2}$",
  "vol.l3" = "$AHV_{t-3}$",
  "vol.l4" = "$AHV_{t-4}$",
  "vol.l5" = "$AHV_{t-5}$",
  "vol.l6" = "$AHV_{t-6}$",
  "X.l1" = "$X_{t-1}$",
  "X.l2" = "$X_{t-2}$",
  "X.l3" = "$X_{t-3}$",
  "X.l4" = "$X_{t-4}$",
  "X.l5" = "$X_{t-5}$",
  "X.l6" = "$X_{t-6}$",
  "const" = "Constant"
)

# Generate table
table_texreg_f <- texreg(
  l = models_list_f,
  override.se = robust_ses_f,
  override.pvalues = robust_pvals_f,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  custom.coef.map = custom_names,
  caption = "First-Term SVAR Models of Average Hourly Volatility",
  label = "tab:VARf",
  caption.above = TRUE,
  digits = 4,
  fontsize = "small",
  float.pos = "H",
  custom.gof.rows = list("Shock (IRF)" = c(0.002919, 0.002236, 0.000484, 0.000702, 0.000904)),
  star.cutoffs = c(0.001, 0.01, 0.05),
  custom.note = "\\parbox{.4\\linewidth}
{\\vspace{2pt}Each SVAR regression has only two variables: AHV and X. The column names represent the X variable for the selected model. \\\\ %stars.}")

# Print table
table_texreg_f
```

### SPY SVAR Second-Term Models

```{r second mandate dum, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_s_d = cbind(Vdata_s$dummy, Vdata_s$SPY_vol)
colnames(y_s_d)[1:2] <- c("dummy", "vol")
est.VAR_s_d <- VAR(y_s_d,p=6)

#extract results
mod_vol_s_d = est.VAR_s_d$varresult$vol
f_s_d = formula(mod_vol_s_d)
d_s_d = model.frame(mod_vol_s_d)
lm_clean_s_d = lm(f_s_d, data= d_s_d)

#apply Newey-West
nw_vcov_s_d = NeweyWest(lm_clean_s_d, lag=6)
nw_se_s_d = sqrt(diag(nw_vcov_s_d))

#t-stats
coef_s_d = coef(lm_clean_s_d)
t_stat_s_d = coef_s_d/nw_se_s_d

#recalculate p-values
robust_s_d = 2*(1-pt(abs(t_stat_s_d), df = df.residual(lm_clean_s_d)))
```

```{r second mandate N, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_s_n = cbind(Vdata_s$N, Vdata_s$SPY_vol)
colnames(y_s_n)[1:2] <- c("N", "vol")
est.VAR_s_n <- VAR(y_s_n,p=6)

#extract results
mod_vol_s_n = est.VAR_s_n$varresult$vol
f_s_n = formula(mod_vol_s_n)
d_s_n = model.frame(mod_vol_s_n)
lm_clean_s_n = lm(f_s_n, data= d_s_n)

#apply Newey-West
nw_vcov_s_n = NeweyWest(lm_clean_s_n, lag=6)
nw_se_s_n = sqrt(diag(nw_vcov_s_n))

#t-stats
coef_s_n = coef(lm_clean_s_n)
t_stat_s_n = coef_s_n/nw_se_s_n

#recalculate p-values
robust_s_n = 2*(1-pt(abs(t_stat_s_n), df = df.residual(lm_clean_s_n)))


```

```{r second mandate tariff, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_s_ta = cbind(Vdata_s$tariff, Vdata_s$SPY_vol)
colnames(y_s_ta)[1:2] <- c("tariff", "vol")
est.VAR_s_ta <- VAR(y_s_ta,p=6)

#extract results
mod_vol_s_ta = est.VAR_s_ta$varresult$vol
f_s_ta = formula(mod_vol_s_ta)
d_s_ta = model.frame(mod_vol_s_ta)
lm_clean_s_ta = lm(f_s_ta, data= d_s_ta)

#apply Newey-West
nw_vcov_s_ta = NeweyWest(lm_clean_s_ta, lag=6)
nw_se_s_ta = sqrt(diag(nw_vcov_s_ta))

#t-stats
coef_s_ta = coef(lm_clean_s_ta)
t_stat_s_ta = coef_s_ta/nw_se_s_ta

#recalculate p-values
robust_s_ta = 2*(1-pt(abs(t_stat_s_ta), df = df.residual(lm_clean_s_ta)))

```

```{r second mandate trade, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_s_tr = cbind(Vdata_s$trade, Vdata_s$SPY_vol)
colnames(y_s_tr)[1:2] <- c("trade", "vol")
est.VAR_s_tr <- VAR(y_s_tr,p=6)

#extract results
mod_vol_s_tr = est.VAR_s_tr$varresult$vol
f_s_tr = formula(mod_vol_s_tr)
d_s_tr = model.frame(mod_vol_s_tr)
lm_clean_s_tr = lm(f_s_tr, data= d_s_tr)

#apply Newey-West
nw_vcov_s_tr = NeweyWest(lm_clean_s_tr, lag=6)
nw_se_s_tr = sqrt(diag(nw_vcov_s_tr))

#t-stats
coef_s_tr = coef(lm_clean_s_tr)
t_stat_s_tr = coef_s_tr/nw_se_s_tr

#recalculate p-values
robust_s_tr = 2*(1-pt(abs(t_stat_s_tr), df = df.residual(lm_clean_s_tr)))

```

```{r second mandate china, warning=FALSE, message=FALSE, results="asis", echo=FALSE}

y_s_ch = cbind(Vdata_s$china, Vdata_s$SPY_vol)
colnames(y_s_ch)[1:2] <- c("china", "vol")
est.VAR_s_ch <- VAR(y_s_ch,p=6)

#extract results
mod_vol_s_ch = est.VAR_s_ch$varresult$vol
f_s_ch = formula(mod_vol_s_ch)
d_s_ch = model.frame(mod_vol_s_ch)
lm_clean_s_ch = lm(f_s_ch, data= d_s_ch)

#apply Newey-West
nw_vcov_s_ch = NeweyWest(lm_clean_s_ch, lag=6)
nw_se_s_ch = sqrt(diag(nw_vcov_s_ch))

#t-stats
coef_s_ch = coef(lm_clean_s_ch)
t_stat_s_ch = coef_s_ch/nw_se_s_ch

#recalculate p-values
robust_s_ch = 2*(1-pt(abs(t_stat_s_ch), df = df.residual(lm_clean_s_ch)))

```

```{r B mat second mandate china, warning=FALSE, message=FALSE, echo=FALSE}
#Construct the Robust Omega Matrix
U_s_ch = residuals(est.VAR_s_ch)
T_s_ch = nrow(U_s_ch)
Omega_s_ch = matrix(0, ncol(U_s_ch), ncol(U_s_ch))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l__s_ch = t(U_s_ch[(l+1):T_s_ch, , drop=FALSE]) %*% U_s_ch[1:(T_s_ch-l), , drop=FALSE] /T_s_ch
  if (l == 0){
    Omega_s_ch = Omega_s_ch + Gamma_l__s_ch
  } else {
    Omega_s_ch = Omega_s_ch + weight*(Gamma_l__s_ch + t(Gamma_l__s_ch))
  }
}


#make the B matrix
loss_s_ch <- function(param_s_ch){
  #Define the restriction
  B_s_ch <- matrix(c(param_s_ch[1], param_s_ch[2], 0, param_s_ch[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X_s_ch <- Omega_s_ch - B_s_ch %*% t(B_s_ch)
  
  #loss function
  loss_s_ch <- sum(X_s_ch^2)
  return(loss_s_ch)
}

res.opt_s_ch <- optim(c(1, 0, 1), loss_s_ch, method = "BFGS")
B.hat_s_ch <- matrix(c(res.opt_s_ch$par[1], res.opt_s_ch$par[2], 0, res.opt_s_ch$par[3]), ncol = 2)

```

```{r Table second Term, message=FALSE, warning=FALSE, results='asis', echo=FALSE}

#Second

d_s_d_t = d_s_d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)

f_t_s_d <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_d <- lm(f_t_s_d, data = d_s_d_t)


d_s_n_t = d_s_n %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)

f_t_s_n <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_n <- lm(f_t_s_n, data = d_s_n_t)


d_s_ta_t = d_s_ta %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)

f_t_s_ta <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_ta <- lm(f_t_s_ta, data = d_s_ta_t)


d_s_tr_t = d_s_tr %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)


f_t_s_tr <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_tr <- lm(f_t_s_tr, data = d_s_tr_t)


d_s_ch_t = d_s_ch %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)

f_t_s_ch <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_ch <- lm(f_t_s_ch, data = d_s_ch_t)


nw_se_s_d_t <- sqrt(diag(sandwich::NeweyWest(model_s_d, lag = 6)))
nw_se_s_n_t <- sqrt(diag(sandwich::NeweyWest(model_s_n, lag = 6)))
nw_se_s_ta_t <- sqrt(diag(sandwich::NeweyWest(model_s_ta, lag = 6)))
nw_se_s_tr_t <- sqrt(diag(sandwich::NeweyWest(model_s_tr, lag = 6)))
nw_se_s_china_t <- sqrt(diag(sandwich::NeweyWest(model_s_ch, lag = 6)))


robust_s_d_t <- 2 * (1-pt(abs(coef(model_s_d) / nw_se_s_d_t), df = df.residual(model_s_d)))
robust_s_n_t <- 2 * (1-pt(abs(coef(model_s_n) / nw_se_s_n_t), df = df.residual(model_s_n)))
robust_s_ta_t <- 2 * (1-pt(abs(coef(model_s_ta) / nw_se_s_ta_t), df = df.residual(model_s_ta)))
robust_s_tr_t <- 2 * (1-pt(abs(coef(model_s_tr) / nw_se_s_tr_t), df = df.residual(model_s_tr)))
robust_s_ch_t <- 2 * (1-pt(abs(coef(model_s_ch) / nw_se_s_china_t), df = df.residual(model_s_ch)))

nw_se_s_d_t       <- nw_se_s_d_t[names(coef(model_s_d))]
robust_s_d_t      <- robust_s_d_t[names(coef(model_s_d))]
nw_se_s_n_t      <- nw_se_s_n_t[names(coef(model_s_n))]
robust_s_n_t     <- robust_s_n_t[names(coef(model_s_n))]
nw_se_s_ta_t      <- nw_se_s_ta_t[names(coef(model_s_ta))]
robust_s_ta_t     <- robust_s_ta_t[names(coef(model_s_ta))]
nw_se_s_tr_t     <- nw_se_s_tr_t[names(coef(model_s_tr))]
robust_s_tr_t     <- robust_s_tr_t[names(coef(model_s_tr))]
nw_se_s_china_t  <- nw_se_s_china_t[names(coef(model_s_ch))]
robust_s_ch_t <- robust_s_ch_t[names(coef(model_s_ch))]

# lists of model, robust SE  and p-values 
models_list_s <- list(model_s_d, model_s_n, model_s_ta, model_s_tr, model_s_ch)
robust_ses_s <- list(nw_se_s_d_t, nw_se_s_n_t, nw_se_s_ta_t, nw_se_s_tr_t, nw_se_s_china_t)
robust_pvals_s <- list(robust_s_d_t, robust_s_n_t, robust_s_ta_t, robust_s_tr_t, robust_s_ch_t)

# Generate table
table_texreg_s <- texreg(
  l = models_list_s,
  override.se = robust_ses_s,
  override.pvalues = robust_pvals_s,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  custom.coef.map = custom_names,
  caption = "Second-Term SVAR Models of Average Hourly Volatility",
  label = "tab:VARs",
  caption.above = TRUE,
  digits = 4,
  fontsize = "small",
  float.pos = "H",
  custom.gof.rows = list("Shock (IRF)" = c(0.014659, 0.013315, 0.010752, -0.005665, 0.013937)),
  star.cutoffs = c(0.05, 0.01, 0.001),
  custom.note = "\\parbox{.4\\linewidth}
{\\vspace{2pt}Each SVAR regression has only two variables: AHV and X. The column names represent the X variable for the selected model. \\\\ %stars.}")

# Print
table_texreg_s

```

