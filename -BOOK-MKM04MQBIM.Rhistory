pvals_new1 <- data.frame(
"First-Term" = c(
NA,
NA,
res6$p.value,
res7$p.value,
res8$p.value))
#Second Term Calculations
#load final dataset
data = backup
#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))
#for interpretation
mean3 = mean(data$SPY_vol)
#Run Second Term Models
# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
nb.lags = 2, p = 1, q = 2)
# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
nb.lags = 0, p = 1, q = 2)
# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
nb.lags = 2, p = 1, q = 2)
# Calculate SPY residuals for the second term
res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)
pvals_new2 <- data.frame(
"Second-Term" = c(
NA,
NA,
res9$p.value,
res10$p.value,
res11$p.value))
#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)
library(texreg)
# Change model names (not sure if needed anymore)
model_names <- c(
"First Term (1)", "First Term (2)", "First Term (3)",
"Second Term (1)", "Second Term (2)", "Second Term (3)"
)
names(models) <- model_names
# HTML coefficients names adjusted (is this correct?)
xnames_ordered <- c(
"AR(1)", "AR(2)", "AR(3)",
"MA(1)", "MA(2)", "MA(3)",
"Constant",
"Tariff<sub>t</sub>", "Tariff<sub>t-1</sub>", "Tariff<sub>t-2</sub>",
"Trade<sub>t</sub>",
"China<sub>t</sub>", "China<sub>t-1</sub>", "China<sub>t-2</sub>"
)
# Render as html -> need to render file to have it nice
htmlreg(
models,
custom.coef.names = xnames_ordered,
custom.model.names = model_names,
caption = "Split-Term ARMAX Models of Average Hourly Volatility",
stars = c(0.001, 0.01, 0.05),
doctype = FALSE
)
kable(pvals_combined, digits = 6, format="html", caption = "Ljung-Box Test p-values for Residuals") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")
means <- data.frame(
Model = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
`SPY Volatility Mean` = c(
mean1,
mean2,
mean3))
kable(means, digits = 6, format="html", caption = "Summary Statistics of SPY Volatility") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(syuzhet) #sentiment analysis
require(purrr) #map functions
require(tidyr) #more data stuff
require(alphavantager)
data("stop_words")
stop_words_list <- stop_words$word
av_api_key(Sys.getenv("ALPHAVANTAGE_API_KEY"))
getwd()
#setwd("...") -> set wd at base repo folder
#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))
# 1. Political
#truthsocial
raw_truths <- read.csv(here("data/political_data", "truths_new.csv"))
raw_truths <- read.csv(here("data/political_data", "truths250510.csv"))
#twitter
raw_tweets <- read.csv(here("data/political_data", "tweets.csv"))
# 2. Financial
#S&P500
data_loader(symbol="SPY")
#STOXX50
data_loader(symbol="VGK")
#CSI 300 (China)
data_loader(symbol="ASHR")
# 1. Tweets
tweets = raw_tweets
#only keep original Tweets
tweets <- tweets %>% filter(isRetweet != "t")
tokens <- tokens(tweets$text)
dfm <- dfm(tokens)
#cleanup
tweets = data.frame(tweets$date,tweets$text)
colnames(tweets) = c("timestamp","tweet_text")
tweets$timestamp = as.POSIXct(tweets$timestamp,format = "%Y-%m-%d %H:%M:%S")
second(tweets$timestamp) = 0
# 2. Truths
truthsbackup <- truths_processer(raw_truths)
truths = truthsbackup
#cleanup
truths = data.frame(truths$date_time_parsed,truths$post)
colnames(truths) = c("timestamp","truths_text")
truths$timestamp = as.POSIXct(truths$timestamp,format = "%Y-%m-%d %H:%M:%S")
second(truths$timestamp) = 0
#note: we are keeping tweets of just an image/video/link in order to have
#them in the tweet count and tweet dummy
# Merging social media data since it is not overlapping
names(truths)[names(truths) == 'truths_text'] <- 'tweet_text'
social = rbind(tweets,truths)
social <- social[order(social$timestamp, decreasing=F), ]
# Round 2 cleanup
social <- social %>%
mutate(
tweet_clean = str_replace_all(tweet_text, "(http[s]?://|www\\.)\\S+", ""),  # Remove URLs
post_lower = str_to_lower(tweet_clean),  # New column with post converted to lowercase
post_clean = str_replace_all(post_lower, "[^a-z\\s]", " "))
social <- social %>%
select(-post_lower, -tweet_clean, -tweet_text)
names(social)[names(social) == 'post_clean'] <- 'tweet_text'
social$timestamp <- as.POSIXct(social$timestamp, format = "%Y-%m-%d %H:%M:%S")
#select time period
social = filter(social,between(timestamp,
as.Date('2009-01-01'),
as.Date('2025-12-12')))
#for plot
truths_plot <- social %>%
mutate(
# Use POSIX 'timestamp' directly
date_time_parsed = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S"),
# Extract date only for plot
day = as.Date(date_time_parsed),
# Extract time only for plot
time = format(date_time_parsed, "%H:%M"),
# Convert time to numeric hours & minutes as fractions
time_numeric = hour(date_time_parsed) + minute(date_time_parsed) / 60,
# Shift time such that y = 0 corresponds to 12 PM
time_shifted = time_numeric - 12)
# 3. Financial
#remove index
SPY = raw_SPY[-1]
VGK = raw_VGK[-1]
ASHR = raw_ASHR[-1]
#Create the scatter plot
ggplot(truths_plot, aes(x = day, y = time_shifted)) +
geom_point(alpha = 0.5, color = "blue", shape = ".") +  # Transparancy to create "heatmap"
scale_y_continuous(
breaks = seq(-12, 12, by = 3),  # Custom Y scale
labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
) +
labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
x = "Date",
y = "Time of Day") +
theme_minimal() +
# Customize X Axis
scale_x_date(
date_labels = "%b %Y",  # Format labels to show month and year
date_breaks = "9 months"
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
# Add vertical lines at 9:30 AM and 4:00 PM for stock market
geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") +
geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +
# theme adjustments
theme(
panel.grid.minor = element_blank(),  # Remove minor gridlines
panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
axis.title = element_text(size = 12),
axis.text = element_text(size = 10))
#Create the scatter plot
ggplot(truths_plot, aes(x = day, y = time_shifted)) +
geom_point(alpha = 0.9, color = "blue", shape = ".") +  # Transparancy to create "heatmap"
scale_y_continuous(
breaks = seq(-12, 12, by = 3),  # Custom Y scale
labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
) +
labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
x = "Date",
y = "Time of Day") +
theme_minimal() +
# Customize X Axis
scale_x_date(
date_labels = "%b %Y",  # Format labels to show month and year
date_breaks = "9 months"
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
# Add vertical lines at 9:30 AM and 4:00 PM for stock market
geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") +
geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +
# theme adjustments
theme(
panel.grid.minor = element_blank(),  # Remove minor gridlines
panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
axis.title = element_text(size = 12),
axis.text = element_text(size = 10))
require(ggplot2) #plots
#Create the scatter plot
ggplot(truths_plot, aes(x = day, y = time_shifted)) +
geom_point(alpha = 0.5, color = "blue", shape = ".") +  # Transparancy to create "heatmap"
scale_y_continuous(
breaks = seq(-12, 12, by = 3),  # Custom Y scale
labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
) +
labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
x = "Date",
y = "Time of Day") +
theme_minimal() +
# Customize X Axis
scale_x_date(
date_labels = "%b %Y",  # Format labels to show month and year
date_breaks = "9 months"
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
# Add vertical lines at 9:30 AM and 4:00 PM for stock market
geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") +
geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +
# theme adjustments
theme(
panel.grid.minor = element_blank(),  # Remove minor gridlines
panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
axis.title = element_text(size = 12),
axis.text = element_text(size = 10))
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
getwd()
#setwd("...") -> set wd at base repo folder
#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))
#load final dataset
source(here("helperfunctions/full_data.R"))
#backup
backup = data
#select timeframe
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))
#for interpretation
mean1 = mean(data$SPY_vol)
models <- list()
# ARMA-X(3,3,1) with Tweet Dummy as Exogenous
models[["Model 1"]] <- armax(data$SPY_vol, xreg = data$dummy, latex = F,
nb.lags = 1, p = 3, q = 3)
# ARMA-X(3,3,1) with Tweet Count as Exogenous
models[["Model 2"]] <- armax(data$SPY_vol, xreg = data$N, latex = F,
nb.lags = 1, p = 3, q = 3)
# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Model 3"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
nb.lags = 3, p = 3, q = 2)
# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Model 4"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
nb.lags = 1, p = 3, q = 2)
# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Model 5"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
nb.lags = 0, p = 3, q = 2)
names = list( "ar1" = "AR(1)",
"ar2" = "AR(2)",
"ar3" = "AR(3)",
"ma1" = "MA(1)",
"ma2" = "MA(2)",
"ma3" = "MA(3)",
"(Intercept)" = "Constant",
"dummy_lag_0" = "$TweetDummy_{t}$",
"dummy_lag_1" = "$TweetDummy_{t-1}$",
"N_lag_0" = "$TweetCount_{t}$",
"N_lag_1" = "$TweetCount_{t-1}$",
"tariff_lag_0" = "$Tariff_{t}$",
"tariff_lag_1" = "$Tariff_{t-1}$",
"tariff_lag_2" = "$Tariff_{t-2}$",
"tariff_lag_3" = "$Tariff_{t-3}$",
"trade_lag_0" = "$Trade_{t}$",
"trade_lag_1" = "$Trade_{t-1}$",
"china_lag_0" = "$China_{t}$")
table1 = texreg(models,
custom.model.names = names(models),
custom.coef.map = names,
caption = "ARMAX Models of Average Hourly Volatility",
caption.above = TRUE,
label = "tab:armax",
digits = 4)
table1
write(table1, file = "armax_table1.tex")
#we want to plot the IRFs of these models
nb.periods = 7 * 15
#irf.plot(models[["Model 1"]],nb.periods,title="Tweet Dummy Shock")
#irf.plot(models[["Model 2"]],nb.periods,title="Tweet Count Shock")
plot1 = irf.plot(models[["Model 3"]],nb.periods,
title="Tariff Mention Shock - Full Timeframe")
plot1
#irf.plot(models[["Model 4"]],nb.periods,title="Trade Mention Shock")
#irf.plot(models[["Model 5"]],nb.periods,title="China Mention Shock")
ggsave("armax_plot1.png",plot=plot1,bg="white")
res1 = checkresiduals(models[["Model 1"]], plot = FALSE)
res2 = checkresiduals(models[["Model 2"]], plot = FALSE)
res3 = checkresiduals(models[["Model 3"]], plot = FALSE)
res4 = checkresiduals(models[["Model 4"]], plot = FALSE)
res5 = checkresiduals(models[["Model 5"]], plot = FALSE)
resnames = c("Twitter Dummy", "Twitter Count", "Tariff", "Trade", "China")
#extract p-values directly from checkresiduals results
pvals <- data.frame("X-Regressor" = resnames,
`Full Timeframe` = c(
res1$p.value,
res2$p.value,
res3$p.value,
res4$p.value,
res5$p.value))
#load final dataset
data = backup
#first term
data = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))
#for interpretation
mean2 = mean(data$SPY_vol)
models <- list()
# ARMA-X(3,3,0) with Tariff Mentions as Exogenous
models[["First Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
nb.lags = 0, p = 3, q = 3)
# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
nb.lags = 0, p = 3, q = 3)
# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
nb.lags = 0, p = 3, q = 3)
res6 = checkresiduals(models[["First Term (1)"]], plot = FALSE)
res7 = checkresiduals(models[["First Term (2)"]], plot = FALSE)
res8 = checkresiduals(models[["First Term (3)"]], plot = FALSE)
pvals_new1 <- data.frame(
"First-Term" = c(
NA,
NA,
res6$p.value,
res7$p.value,
res8$p.value))
#load final dataset
data = backup
#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))
#for interpretation
mean3 = mean(data$SPY_vol)
# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
nb.lags = 2, p = 1, q = 2)
# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
nb.lags = 0, p = 1, q = 2)
# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
nb.lags = 2, p = 1, q = 2)
#we want to plot the IRFs of these models
nb.periods = 7 * 15
plot2 = irf.plot(models[["Second Term (1)"]],nb.periods,
title="Tariff Mention Shock - Second Term")
plot2
ggsave("armax_plot2.png",plot=plot2,bg="white")
plot3 = irf.plot(models[["Second Term (3)"]],nb.periods,
title="China Mention Shock - Second Term")
plot3
ggsave("armax_plot3.png",plot=plot3,bg="white")
res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)
pvals_new2 <- data.frame(
"Second-Term" = c(
NA,
NA,
res9$p.value,
res10$p.value,
res11$p.value))
#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)
xnames = list("ar1" = "AR(1)",
"ar2" = "AR(2)",
"ar3" = "AR(3)",
"ma1" = "MA(1)",
"ma2" = "MA(2)",
"ma3" = "MA(3)",
"(Intercept)" = "Constant",
"tariff_lag_0" = "$Tariff_{t}$",
"tariff_lag_1" = "$Tariff_{t-1}$",
"tariff_lag_2" = "$Tariff_{t-2}$",
"trade_lag_0" = "$Trade_{t}$",
"china_lag_0" = "$China_{t}$",
"china_lag_1" = "$China_{t-1}$",
"china_lag_2" = "$China_{t-2}$")
table2 = texreg(models,
custom.model.names = names(models),
custom.coef.map = xnames,
caption = "Split-Term ARMAX Models of Average Hourly Volatility",
caption.above = TRUE,
label = "tab:armax_term",
digits = 4)
table2
write(table2, file = "armax_table2.tex")
table3 = knitr::kable(pvals_combined, digits = 100, format="latex",
caption = "Ljung-Box Test p-values for Residuals")
table3
write(table3, file = "armax_table3.tex")
means <- data.frame(
Model = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
`SPY Volatility Mean` = c(
mean1,
mean2,
mean3))
table4 = knitr::kable(means, digits = 6, format="latex",
caption = "Summary Statistics of SPY Volatility")
table4
write(table4, file = "armax_table4.tex")
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)
getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final")
#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))
checkresiduals()
