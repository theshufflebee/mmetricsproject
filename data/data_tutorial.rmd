---
title: "Data Management"
output:
  pdf_document: 
    toc: true
    fig_caption: yes
    latex_engine: lualatex
header-includes:
  - \usepackage{amsmath}
---
\newpage

```{r library_setup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/select_armax.R"))
```


# Data Prep
```{r rawdata_setup, results=FALSE, warning=FALSE, message=FALSE}

# 1. Political 

#truthsocial
raw_truths <- read.csv(here("data/political_data", "truths_new.csv"))

#twitter
raw_tweets <- read.csv(here("data/political_data", "tweets.csv"))


# 2. Financial

#S&P500
data_loader(symbol="SPY")

#STOXX50
data_loader(symbol="VGK")

#CSI 300 (China)
data_loader(symbol="ASHR")

```

```{r cleanup}
#MAKE FUNCTIONS TO CLEANUP EASY

# 1. Tweets 
tweets = raw_tweets

#only keep original Tweets
tweets <- tweets %>% filter(isRetweet != "t")
tokens <- tokens(tweets$text)
dfm <- dfm(tokens)

#cleanup
tweets = as.data.table(tweets)
names(tweets)[names(tweets) == 'date'] <- 'timestamp'
tweets$timestamp = as.POSIXct(tweets$timestamp,format = "%Y-%m-%d %H:%M:%S")

#nrc_scores <- get_nrc_sentiment(complete_data$posts)


# 2. Truths 
truthsbackup <- truths_processer(raw_truths) 

#cleanup
truths = as.data.table(truthsbackup)
names(truths)[names(truths) == 'date_time_parsed'] <- 'timestamp'


# 3. Financial

#remove index
SPY = raw_SPY[-1]
VGK = raw_VGK[-1]
ASHR = raw_ASHR[-1]

#rename financial columns to add symbol
colnames(SPY)[-1] <- paste0("SPY", colnames(SPY)[-1])
colnames(VGK)[-1] <- paste0("VGK", colnames(VGK)[-1])
colnames(ASHR)[-1] <- paste0("ASHR", colnames(ASHR)[-1])

```

# Raw Data Save
```{r data save}

#financial
write.csv(SPY, here("data/mothership/SPY.csv"))
write.csv(VGK, here("data/mothership/VGK.csv"))
write.csv(ASHR, here("data/mothership/ASHR.csv"))

#social media
write.table(tweets, here("data/mothership/tweets.csv"))
truths <- apply(truths,2,as.character) #fix BROKEN
write.table(truths, here("data/mothership/truths.csv"))

```


# Merging All Data
```{r mothership merge}

#this will become a function
#first load all datasets in mothership folder

#mothership base (dataframe with each minute since 2010)
mothership <- data.frame(timestamp = seq(as.POSIXct("2019-01-01"),
                                as.POSIXct("2020-04-16"), 
                                by=(1*60)))

#make posixct
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
VGK$timestamp = as.POSIXct(VGK$timestamp,format = "%Y-%m-%d %H:%M:%S")
ASHR$timestamp = as.POSIXct(ASHR$timestamp,format = "%Y-%m-%d %H:%M:%S")
tweets$timestamp = as.POSIXct(tweets$timestamp,format = "%Y-%m-%d %H:%M:%S")
truths$timestamp = as.POSIXct(truths$timestamp,format = "%Y-%m-%d %H:%M:%S")


#merge with financial and socialmedia
mothership = left_join(mothership, SPY, by = "timestamp")
mothership = left_join(mothership, VGK, by = "timestamp")
mothership = left_join(mothership, ASHR, by = "timestamp")
mothership = left_join(mothership, tweets, by = "timestamp")
mothership = left_join(mothership, truths, by = "timestamp")

```

# Adding Dummies etc
```{r mothership add}
# 1. Add count for tweets


# 2. Sentiments

# 3. Dummy Tweet

# 4. Dummy Important Word

# 5. Dummy Emotional Word

```




# Using Alpha Vantage API
```{r eval=FALSE}
library(alphavantager)

av_api_key(Sys.getenv("ALPHAVANTAGE_API_KEY"))

#for past month
data=av_get("ASHR", av_fun = "TIME_SERIES_INTRADAY", interval = "1min", 
              adjusted="false", extended_hours="false", outputsize = "full")

#for a particular month
data2=av_get("SPY", av_fun = "TIME_SERIES_INTRADAY", interval = "1min", 
              adjusted="false", extended_hours="false", 
              month="2025-04", outputsize = "full") #create loop for more



write.csv(data,"~/ASHR.csv", row.names = T) #saves to documents
write.csv(data2,"~/SPY-2025-04.csv", row.names = T) 

```

```{r loop, eval=FALSE}
library(alphavantager)

av_api_key(Sys.getenv("ALPHAVANTAGE_API_KEY"))

year = "2022"
months = c("01","02","03","04","05","06","07","08","09","10","11","12")
market = "SPY"

for (t in 1:length(months)) {
    date = paste(year, months[t], sep="-")
dataloop = av_get(market, av_fun="TIME_SERIES_INTRADAY",interval="1min",
                  adjusted="false", extended_hours="false",
                  month=date, outputsize="full")
filename = paste(market,date,sep="-")
filename = paste("~/", filename, sep="")
filename = paste(filename,".csv",sep="")
write.csv(dataloop,filename)
}
```


# Tutorials

Manual smp500: https://cafim.sssup.it/~giulio/other/alpha_vantage/index.html#orgaaf54ef

AlphaVantageR Tutorial: https://github.com/business-science/alphavantager/blob/master/man/av_get.Rd

Intra-Day Analysis: https://arxiv.org/html/2406.17198v1

# Symbols Explanation
- ONEQ = NASDAQ Composite
- SPY = S&P500 
- SMI = Swiss Market Index
- VTHR = Russell 3000 (US)
- VTI = CRSP US Total Market Index
- VGK = Euro Stoxx 50
- ASHR = basically china