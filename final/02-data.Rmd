---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r library_setup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

# Data

## Financial Data

Since we do not have access to high frequency data on Stock Indices directly, we
use ETF's that approximate these indices. We downloaded one-minute invervall data
on multiple Indices, notably the S&P 500 ETF with the Ticker SPY from Alphavantage.
Then we calculated hourly volatility for these indices. Since trading starts at
09:30 the interval from 09:00 to 10:00 only includes half an hour of actual trading.

The formula for hourly volatility is... 2


For our financial data, we decided to try to find minute-by-minute prices for 
broad market indices. While the actual indices do not update their prices so often,
we had to take proxies under the form of ETF's that track them. Our 3 markets of
analysis are: SPY to track the S&P500, VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage. Our timeframe is from the first 
of January 2014 to the 7th of May 2025.


We then had to transform this data to get our main variable of interest, realised 
market volatility. We did so with the following formula:
$$
\begin{aligned}
  v_t &= \frac{1}{N}\sum_{i=1}^N(\Delta p_{t,i})^2 \\
  &\text{where } \Delta p_t \text{ is the difference in price (open - close)} \\
  &\text{and i represents every minute}
\end{aligned}
$$
We used a custom function in order to get the average hourly volatility for each 
open market hour. Note that the first hour is from 9:30 am to 10:00 am since the 
market open on a half-hour but closes at 4:00 pm. We can plot this data in
<span style="color:red"> *figX* </span>.

## Political Data

We source Trumps Posts from two places. The Tweets are from Kaggle
@DonaldTrumpTweets and go until the 8th of January 2021. Since he
switched his primary Posting platform to Truth Social we use only that
Data from 2021 onwards. All Truth Social Posts were scrapped from
trumpstruth.org, a webpage that aims to conserve all his posts. You can
find the dataset, and webscrapper, and Data cleaning process in the
Appendix.

Since we're using financial data that is constrained by trading hours,
we decided to move posts after 16:00 to the next trading day's opening
hour.[^2]

All tweets then get aggregated on the hour. We then extract a dummy if he posted
in the hour, how many times he posted and how many times he mentioned certain words.

Further we applied some simple sentiment analysis algorithms on the data to see
if there are certain sentiments in his tweets that move the markets. More
information and detailed process are in the appendix and online.

## Final Dataframe

Show here maybe??


[^2]: So a Post on friday 18:00 would be moved to Monday 09:00
