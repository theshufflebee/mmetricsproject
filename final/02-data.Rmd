---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)

getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

# Data

## Financial Data

For our financial data, we decided to try to find minute-by-minute prices for 
broad market indices. While the actual indices do not update their prices so often,
we had to take proxies under the form of ETF's that track them. Our 3 markets of
analysis are: SPY to track the S&P500, VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage. Our timeframe is from the first 
of January 2014 to the 7th of May 2025.


We then had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is realised market volatility. We did so 
with the following formula:
$$
\begin{aligned}
  v_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close) and $i$ represents
every minute.

We used a custom function in order to get the AHV for each open market hour. Note 
that the first hour is from 9:30 am to 10:00 am since the 
market open on a half-hour but closes at 4:00 pm. We can plot this data in
the following table:


```{r fin plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

price_plotter(SPY, title = "SPY Prices")

hvol_plotter(SPY, breaks = "yearly", title = "SPY Realised Volatility")

```

We can clearly see that the last few months show a new era of never seen before
levels of volatility. Shocks on volatility recently have reached, and even surpassed
(for a few data points) levels seen during the COVID-19 pandemic.

## Political Data

We have two sources for Trump's posts. The Tweets are from Kaggle
@shantanuDonaldTrumpTweets and go until the 8th of January 2021. Since he
switched his primary posting platform to Truth Social we use only that
Data from 2021 onwards. All Truth Social posts were scrapped from
trumpstruth.org, a webpage that aims to conserve all his posts. Note that we have 
had to use web-scrapping methods in order to download all his Truth Social posts
in a dataset.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove a lot of observations. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as an assumption.^[2] 

Since our financial data is hourly, we aggregate the social data by hour. We 
then construct multiple variables from the social media data. These include
a dummy for whether there was a post, the number of posts an hour and counts
for certain words ("tariffs","trade","china"). Further we applied some simple 
sentiment analysis algorithms on the data to see if there are certain sentiments 
in his tweets that move the markets. Details on all our data management procedures
can be found in the GitHub repository.

```{r social plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

#find count
tweetcount_alltime = dplyr::select(data,timestamp,N)
#select time period
tweetcount = filter(tweetcount_alltime,
between(timestamp,
as.Date('2014-01-01'),
as.Date('2025-04-10')))
#plot
ggplot(tweetcount_alltime, aes(x = timestamp, y = N)) +
geom_point(color = "#253494", size = 1) +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
labs(title = "Trump Social Media Count",
x = NULL,
y = "number of tweets/truths") +
theme_minimal(base_size = 14) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(face = "bold", hjust = 0.5))

```


## Final Dataframe

```{r data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

tail(data[1:4])

tail(data[5:9])

tail(data[10:14])

tail(data[15:19])

```





Â¨
^[2]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 
