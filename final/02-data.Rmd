---
bibliography: citations.bib
csl: apa.csl
link-citations: true
editor_options:
  markdown:
    wrap: 72
---

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)
require(scales)


getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))
truths <- read.csv(here("data/mothership", "social.csv"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```


# Data

## Financial Data

For our financial data, we decided to use minute-by-minute prices for 
broad market indices. Since the actual indices do not update their prices that often,
we had to take proxies under the form of ETFs that track them. These ETFs exist to provide investors with direct exposure to whole markets and have very small to no deviations to the corresponding index. Our 3 markets of analysis are: SPY to track the S&P500 (note that this ETF has a very large trading volume relative to the others), VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage[^2]. Our timeframe starts on the first 
of January 2014 and goes to the 7th of May 2025.


```{r snp_download, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
library(fredr)

sp500_data <- fredr(
  series_id = "SP500",
  observation_start = as.Date("2016-01-01"),
  observation_end = Sys.Date(),
  frequency = "d"  # daily data
)

sp500_select <- sp500_data%>%
  drop_na()%>%
  select(date, value)%>%
  mutate(index_sp500 = value / dplyr::first(value) * 100)

df_last_close <- SPY %>%
  mutate(day = as.Date(timestamp)) %>%               # extract the date
  group_by(day) %>%
  filter(timestamp == max(timestamp)) %>%            # last timestamp per day
  select(day, close) %>%                   
  ungroup()

df_last_close_indexed <- df_last_close %>%
  arrange(day) %>%
  filter(day >= as.Date("2016-01-01"))%>%
  mutate(index_SPY = close / dplyr::first(close) * 100)

SPY_comp <- left_join(df_last_close_indexed, sp500_select, by = c("day" = "date"))

```


```{r fig-spy-snp, fig.width=10, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
ggplot(SPY_comp,aes(x = day))+
  geom_line(aes(y = index_SPY, color = "SPY Daily Close")) +
  geom_line(aes(y = index_sp500, color = "S&P 500 Daily Close")) +
  scale_color_manual(values = c("SPY Daily Close" = "blue", "S&P 500 Daily Close" = "red")) +
  labs(title = "SPY vs. S&P 500 (Indexed: 2016 = 100) ",
       x = NULL,
       y = NULL,
       color = "Legend") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(
    panel.grid.minor = element_blank(),  
    panel.grid.major = element_line(linewidth = 0.5)
  )+
   # Add caption  
  labs(caption = str_wrap("To show that SPY tracks the S&P 500 almost perfectly, we've sourced the S&P 500 daily close from FRED and plotted it against our SPY closing Prices. The blue line that tracks the SPY ETF is barely visible", width = 150)) +
  theme(plot.caption = element_text(hjust = 0, face = "italic", size = 10))


```


[^2]: https://www.alphavantage.co/ 

We had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is \textit{realised} market volatility. We did so 
using the following formula:
$$
\begin{aligned}
  AHV_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close), $i$ represents
every minute, $t$ represents an hour, and $N$ is the total number of minutes in each hour.

Ultimately, we compute the AHV for each open market hour since 2014. Note 
that the first hour is from 9:30 am to 10:00 am since the market opens on a half-hour 
but closes at 4:00 pm. Plotting this data, we observe that the last few months (corresponding
to Donald Trump's first 100 days in office) display 
unprecedented levels of volatility which have reached, and even surpassed, levels seen
during the COVID-19 pandemic.



```{r fig-spy-price, fig.width=10, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
combined_plot <- function(data, title, breaks = "monthly") {
  
  # Index close price so first value = 100
  data <- data[order(data$timestamp), ]  # ensure data sorted by time
  data$close_indexed <- 100 * data$close / data$close[1]
  
  # take the range of both datasets (using indexed close price)
  price_range <- range(data$close_indexed, na.rm = TRUE)
  vol_range <- range(data$r_vol_h, na.rm = TRUE)
  
  # find the scale factor for volatility to fit price scale
  scale_factor <- diff(price_range) / diff(vol_range)
  
  # rescale volatility to price scale
  data$scaled_vol <- (data$r_vol_h - vol_range[1]) * scale_factor + price_range[1]

  # Pick x-axis breaks
  x_scale <- switch(breaks,
                    "yearly" = scale_x_datetime(date_labels = "%b %Y", date_breaks = "6 month"),
                    "monthly" = scale_x_datetime(date_labels = "%b %Y", date_breaks = "1 month"),
                    "daily"   = scale_x_datetime(date_labels = "%a %d", date_breaks = "1 day"),
                    "hourly"  = scale_x_datetime(date_labels = "%Hh", date_breaks = "1 hour"),
                    NULL)

  ggplot(data, aes(x = as.POSIXct(timestamp))) +
    # Price line (indexed)
    geom_line(aes(y = close_indexed), color = "darkblue", linewidth = 0.5) +
    # Volatility line (scaled)
    geom_line(aes(y = scaled_vol), color = "darkred", linewidth = 0.5, linetype = "dashed") +
    
    # Left axis: price (indexed)
    scale_y_continuous(
      name = "Indexed SPY Close Price (Blue)",
      sec.axis = sec_axis(~ (. - price_range[1]) / scale_factor + vol_range[1],
                          name = "Hourly Realised Volatility (Red)",
                          labels = scales::percent_format(accuracy = 0.1))
    ) +
    
    x_scale +
    labs(title = title, x = "") +
    theme_classic(base_size = 14) +
    
    # theme adjustments
    theme(
    panel.grid.minor = element_blank(),  
    panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines only
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )+
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.title = element_text(face = "bold", hjust = 0.5)
    )
}
combined_plot(SPY, "SPY Close Price vs Volatility", breaks = "yearly")

```


## Political Data


We have two types of data for Trump's posts, Tweets \& "Truths" (from Truth Social). 
The Tweets are sourced from Kaggle (@shantanuDonaldTrumpTweets) and stop in January 2021,
seen as Trump was then banned. Due to this, we have a gap in this data going until February 2022, 
when he first posted on his own new platform. All Truth Social posts were pulled from
"trumpstruth.org", a webpage that aims to conserve all his posts. Note that we have 
had to use web-scrapping methods in order to download all these posts in a dataset.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove such a large amount of observations that it severely limited 
our analysis. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as a critical assumption[^3]. 

[^3]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 

Since our financial data is hourly, we aggregate the social data by hour as well and construct multiple variables from this. These variables include
a dummy for whether there was a post in a particular hour ($TweetDummy$), the number of posts in an hour 
($TweetCount$), and counts
for mentions of certain words ($Tariff$, $Trade$, \& $China$). 

Furthermore, we applied simple sentiment analysis algorithms in order to extract emotions and proportions for the amount of "positive" and "negative" words from the posts. We however ended up not 
including these in our final analyses as results were not particularly interesting. Details on all our data management procedures as well as the final dataset can be found in the GitHub repository.

```{r social plots, fig.width=10, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
truths_2 <- dplyr::select(truths, timestamp, tariff, trade, china)


daily_counts <- truths_2 %>%
  mutate(day = as.Date(timestamp)) %>%
  filter(day >= as.Date("2014-01-01")) %>%
  group_by(day) %>%
  summarise(
    N = n(),
    across(where(is.numeric), sum, na.rm = TRUE), .groups = "drop")


ggplot(daily_counts, aes(x = day, y = N)) +
  # First plot points where tariff == 0, in light blue
  geom_point(data = filter(daily_counts, tariff == 0), color = "lightblue", size = 1) +
  
  # Then plot points where tariff > 0, in dark blue (on top)
  geom_point(data = filter(daily_counts, tariff > 0), color = "darkblue", size = 1) +
  
  geom_smooth(method = "loess", span = 0.2, se = FALSE, color = "darkred", size = 1) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "9 months") +
  scale_y_log10() +
  labs(
    title = "Trump Social Media Posts",
    x = NULL,
    y = "Number of posts (log scale)"
  ) +
  theme_classic(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(linewidth = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", hjust = 0.5)
  )+
  
   # Add caption  
  labs(caption = str_wrap("Every dot represents number of tweets on a given day. Darkblue dots represent days where Trump mentioned tariff in at least one of his posts.", width = 150)) +
  theme(plot.caption = element_text(hjust = 0, face = "italic", size = 10))
```




## Final Dataframe
Our final Dataframe is quite wide and contains the following Variables:

| Variable | Description                                                                                                    |
|----------|----------------------------------------------------------------------------------------------------------------|
| _vol     | The hourly volatility of given indices                                                                        |
| dummy    | A dummy variable, 1 if Trump posted in that hour                                                               |
| N        | Number of posts in that hour                                                                                   |
| tariff   | Mentions of tariff in that hour                                                                                 |
| trade    | Mentions of trade in that hour                                                                                   |
| china    | Mentions of China in that hour                                                                                   |
| prop     | Sentiments according to NRC sentiment lexicon. Calculated as share of total sentiment and add up to 1. Positive and negative are separate measures and also add up to 1. |


And here is a look at the final Dataframe. The sentiments aren't important for our final analysis are we dind't get any useful results
with them and decided to cut them from our final analysis: 
```{r data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

tail(data[1:4])

tail(data[5:9])

tail(data[10:14])

tail(data[15:19])

```

