---
title: "Terminally Online: Does Donald Trump impact Financial Markets"
author: "Marcos Constantinou, Ryan Fellarhi & Jonas Bruno"
date: "Last edited: 20.05.2025"
site: bookdown::bookdown_site
documentclass: book
bibliography: [packages.bib, citations.bib] #name bib files
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg -> Do Later
description: |
  This is our website for a Universityproject. We put this together from a basic bookdown example so there might be some stuff in here from that.
link-citations: true           # clickable citations
csl: apa.csl
github-repo: www.github.com/theshufflebee/mmetricsproject
---


```{r r library_setup:index, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE}
#rm(list=ls())
require(here)
require(stringr)
require(dplyr)
require(ggplot2)
require(lubridate)
require(RColorBrewer)
require(wordcloud)
require(tidyr)
require(tidytext)
require(textstem)
require(tidyverse)
require(tm)
require(SnowballC)
require(quanteda.textplots)
require(quantmod)
require(alphavantager)
require(quanteda)
require(rvest)
require(httr)
require(xml2)
require(textdata)
require(sentimentr)
require(syuzhet)
require(text)
library(wordcloud2)


#load datasets
tweets <- read.csv(here("data/political_data/tweets.csv"))
truths <- read.csv(here("data/political_data/truths250510.csv"))

source(here("helperfunctions/truths_cleaning_function.R"))
```



```{r data_title, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
#Only keep original Tweets
tweets_clean <- tweets %>% filter(isRetweet != "t")
tweets_clean <- tweets_clean$text
truths_clean <- truths_processer(truths)

truths_clean2 <- truths_clean$post

full_posts <- c(tweets_clean, truths_clean2)

tokens <- tokens(full_posts)
dfm <- dfm(tokens)
corpus_tweets <- corpus(tweets$text)

additional_words <- c("rt", "amp")
all_stopwords <- c(stopwords("en"), additional_words)

tokens_tweets <- tokens(corpus_tweets,  
                        remove_punct = TRUE, 
                        remove_numbers = TRUE)

tokens_tweets <- tokens_remove(tokens_tweets, all_stopwords)  # Remove stopwords

dfm_tweets <- dfm(tokens_tweets)
```



```{r wordcould, cache=FALSE, echo=FALSE}
print(textplot_wordcloud(
  dfm_tweets,
  min_count = 200,
  color = RColorBrewer::brewer.pal(8, "Reds")
))
```


# Readme {.unnumbered}

This is a Website for a class project from the 2025 Class Macroeconometrics at Université de Lausanne. It is intended as an archive and Display of our Project. You can find all data for recreation on www.github.com/theshufflebee/mmetricsproject.

# Abstract {.unnumbered}


In this short paper, we aim to asses to what extent financial markets may react
to Donald Trump's social media posts, and specifically the effect on average
realised volatility. We do so using both ARMA-X and VAR models, with data spanning
the 1st of January 2014, to the 7th of May 2025, over various time horizons and
independent variables. We find limited evidence that there is a significant positive
effect, and provide some explanations as to why this could be the case.



<!--chapter:end:index.Rmd-->

---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r, warning=FALSE, echo=FALSE, cache=TRUE}
require("here")
require("stringr")
require("dplyr")
require("ggplot2")
require("lubridate")

truths_raw <- read.csv(here("data/mothership", "social.csv"))

truths <- truths_raw %>%
  mutate(
    # Use POSIX 'timestamp' directly
    date_time_parsed = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S"),
    
    # Extract date only for plot
    day = as.Date(date_time_parsed),
    
    # Extract time only for plot
    time = format(date_time_parsed, "%H:%M"),
    
    # Convert time to numeric hours & minutes as fractions
    time_numeric = hour(date_time_parsed) + minute(date_time_parsed) / 60,
    
    # Shift time such that y = 0 corresponds to 12 PM
    time_shifted = time_numeric - 12
  )

```

# Introduction

## Motivation

Over the past 15 years social media has become an important
communication tool for politicians. One of the pioneers of this novel
approach has been Donald Trump, the 45th and 47th President of the United
States. Since his ban on Twitter after the January 6th riots, his quantity of
social media posts has drastically increased. This is shown in the following
figure. [^1]

[^1]: Includes both Posts and Reposts

The content of his posts can sometimes have announcements or teases of future
political decisions. Note the recent infamous "THIS IS A GREAT TIME TO BUY!!! DJT"
post sent just an hour before lifting his reciprocal tariffs. It is then not 
improbable that agents in financial markets might take this information into 
account in their decision making. This question has been asked before in the 
literature, focusing rather on his first term. 

This brings us to our research question:  Do Donald Trumps Posts impact market Volatility?

    
\@ref(fig:fig1)

```{r fig1, echo=FALSE, fig.cap="Terminally Online: Trump's Twitter & Truth Social Posts (EDT)", warning=FALSE, cache=TRUE}
 #Create the scatter plot
ggplot(truths, aes(x = day, y = time_shifted)) +
  geom_point(alpha = 0.5, color = "blue", size = 0.55) +  # Transparancy to create "heatmap"
  scale_y_continuous(
    breaks = seq(-12, 12, by = 3),  # Custom Y scale
    labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
  ) +
  labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
       x = "",
       y = "Time of Day") +
  theme_minimal() +
  
  
  # Customize X Axis
  scale_x_date(
    date_labels = "%b %Y",  # Format labels to show month and year
    date_breaks = "9 months"
  ) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Add vertical lines at 9:30 AM and 4:00 PM for stock market
  geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") + 
  geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +   
  
  # theme adjustments
  theme(
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```



## Literature Review

Information is one of the most valuable assets in the financial market.
Its importance lies at the core of the "Efficient Market Hypothesis", 
which states that the prices of assets fully reflect all
available information, adjusting immediately to any new data
@famaAdjustmentStockPrices2003 , and thereby creating a strong demand
for information flow. In addition, the “Mixture of Distribution
Hypothesis” states that the release of new information is closely linked
to movements in both realized and implied volatility
@andersenReturnVolatilityTrading1996, @frenchStockReturnVariances1986,
@vlastakisInformationDemandStock2012.

Consequently, a large part of the literature had focused on the relation
between announcements, news and market activity. For example,
@schumakerTextualAnalysisStock2009 use various linguistic and textual
representations derived from financial news to predict stock market
prices. Similarly, @ederingtonHowMarketsProcess1993 analyze the impact
of macroeconomic news announcements on interest rate and foreign
exchange futures markets, particularly in terms of price changes and
volatility. Both studies, among others, find that prices— such as stock
prices—react primarily within minutes after the release of new
information.

Recently, the world has witnessed the rise of the Internet
which revolutionized the dissemination and accessibility of information.
Social media enable investors, analysts or politicians to instantly
share their information, news or opinions. This led some studies to
focus on the communication dynamics of social platform to predict
changes in the returns of financial assets @dechoudhuryCanBlogCommunication2008 & 
@bartovCanTwitterHelp2018. In this context, the
impact of Trump’s tweets on various financial and macroeconomic
variables has been analysed by several studies, especially during his
first mandate.

Using high-frequency financial data,
@gjerstadPresidentTrumpsTweets2021 found an increase in uncertainty and
trading volume, along with a decline in the U.S. stock market—regardless
of the tweet's content. However, the effect was stronger when Trump used
confrontational words such as "tariff" or "trade war." Some of his
announcements also influenced the U.S. dollar exchange rate @vlastakisInformationDemandStock2012
and certain market indices within minutes of the tweet being posted
@colonescuEffectsDonaldTrumps2018 & @kinyuaAnalysisImpactPresident2021.

Other scholars have shown that negative Trump tweets about specific
companies tended to reduce demand for their stocks @bransHisThumbEffect2020 &
@mendelsStanfordResearchSeries2019, whereas some
other have shown that they also impact market volatility indices such as
the VIX @fendelPoliticalNewsStock2019 or the Volfele @klausMeasuringTrumpVolfefe2021.
The effects of his tweets also extended beyond the U.S.. For example,
@nishimuraImpactsDonaldTrumps2025 shows a
positive relationship between volatility in European stock markets and
tweeter activity of Trump, and this effect tends to intensify as public
intention for his tweet grows @nishimuraImpactsDonaldTrumps2025.

Our paper is built as follows: Section 2 describes the data, their sources, and 
the several transformations applied. Section 3 focuses on ARMA-X models, describing 
both our methodology and results. Section 4 does the same, though for our VAR models. 
Section 5 concludes.

<!--chapter:end:01-introduction.Rmd-->

---
bibliography: citations.bib
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)

getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

# Data

## Financial Data

For our financial data, we decided to try to find minute-by-minute prices for 
broad market indices. While the actual indices do not update their prices so often,
we had to take proxies under the form of ETF's that track them. Our 3 markets of
analysis are: SPY to track the S&P500, VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage. Our timeframe is from the first 
of January 2014 to the 7th of May 2025.


We then had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is realised market volatility. We did so 
with the following formula:
$$
\begin{aligned}
  v_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close) and $i$ represents
every minute.

We used a custom function in order to get the AHV for each open market hour. Note 
that the first hour is from 9:30 am to 10:00 am since the market opens on a half-hour 
but closes at 4:00 pm. We can plot this data in the following table:


```{r fin plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

price_plotter(SPY, title = "SPY Prices")

hvol_plotter(SPY, breaks = "yearly", title = "SPY Realised Volatility")

```

We can clearly see that the last few months show a new era of never seen before
levels of volatility. Shocks on volatility recently have reached, and even surpassed
(for a few data points) levels seen during the COVID-19 pandemic.

## Political Data

We have two sources for Trump's posts. The Tweets are from Kaggle
@shantanuDonaldTrumpTweets and go until the 8th of January 2021. Since he
switched his primary posting platform to Truth Social we use only that
Data from 2021 onwards. All Truth Social posts were scrapped from
trumpstruth.org, a webpage that aims to conserve all his posts. Note that we have 
had to use web-scrapping methods in order to download all his Truth Social posts
in a dataset.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove a lot of observations. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as an assumption.^[2] 

Since our financial data is hourly, we aggregate the social data by hour. We 
then construct multiple variables from the social media data. These include
a dummy for whether there was a post, the number of posts an hour and counts
for certain words ("tariffs","trade","china"). Further we applied some simple 
sentiment analysis algorithms on the data to see if there are certain sentiments 
in his tweets that move the markets. Details on all our data management procedures
can be found in the GitHub repository.

```{r social plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

#find count
tweetcount_alltime = dplyr::select(data,timestamp,N)
#select time period
tweetcount = filter(tweetcount_alltime,
between(timestamp,
as.Date('2014-01-01'),
as.Date('2025-04-10')))
#plot
ggplot(tweetcount_alltime, aes(x = timestamp, y = N)) +
geom_point(color = "#253494", size = 1) +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
labs(title = "Trump Social Media Count",
x = NULL,
y = "number of tweets/truths") +
theme_minimal(base_size = 14) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(face = "bold", hjust = 0.5))

```


## Final Dataframe
Here is a look into our final, fully cleaned, fully transformed dataset.
```{r data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

tail(data[1:4])

tail(data[5:9])

tail(data[10:14])

tail(data[15:19])

```





¨
^[2]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 

<!--chapter:end:02-data.Rmd-->

# ARMA-X

## Methodology

Once we have our final dataframe, we could then finally start on some analysis.
We first thought of a simple ARMA-X type specification, taking the AHV as our
"y variable" and taking any of the social media variables as the exogenous
regressors. The assumption here is that, while the market reacts to Trump posts,
Trump's posts are chaotic, nonsensical, and random enough to be considered 
exogenous. 

We of course first start by checking stationarity of our variables (ADF), where we find
p-values of 0.01 suggesting that the processes are not explosive. Then, we use 
a custom function in order to choose the number of lags based on the AIC criterion.
This however, while often choose a very high number of lags, which could be 
explained by our data being hourly. As such we decided to put a limit of 3 lags,
which sees minimal AIC loss and simplifying our models considerably.

## Results


### Full Timeframe
We run models with the following exogenous regressors: $TweetDummy$, $TweetCount$,
and the mentions of words $Tariff$, $Trade$, and $China$. We first note on the table
 in section \@ref(sec:spy-table) that all the x-regressors are significant,
apart from trade. Notice also that all the coefficients (apart from $Tariff_{t-3}$)
are positive, in line with our main hypothesis. The effect of $Tariff_{t-1}$ and 
$Tariff_{t-2}$ are especially large, given the usual size of the volatility as seen
in Section \@ref(sec:means-table). We in fact predict that an 
extra mention of tariffs one hour ago, leads to a whopping extra 0.02 in volatility
 which is just about the average size for the full timeframe. We can see the
impulse response function (IRF) for this shock, in \@ref(sec:SPY-IRF) Notice that
there is a large response in the first periods, and then a graduate
decline over time. Something to note is that in our analyses of IRF's, when including
MA terms, the decline shows up gradual while being much sharper when only including
AR terms. 
Note that we ran all these models on the VGK and ASHR ETF's as well, though no
significant results appear apart from a small but statistically significant effect
of the tariff variable for VGK.

### Split Samples
We then split our sample for the first and second term of the Trump presidency.
We only run models on tariff, trade and china this time. As seen on table
\@ref(sec:spy-table-terms), the first interesting result
is in the coefficients of tariff being significant and very large in the second
term, while being small and not statistically significant in the first. A similar
story goes for the China variable. This may lend some evidence to support the 
claim that investors are much more reactive to Trump's social media presence
now than before. We've found similar IRF's as for the full timeframe. Tables \@ref(sec:SPY-IRF)
show the IRF's for the second term, of the impacts of tariff and china mention
shocks on the AHV.
Finally, we can check the residuals of all these models to test them somewhat. 
In Section \@ref(sec:SPY-res-test), the pvalues being zero
for the full timeframe and first term indicate that there is autocorrelation in 
the residuals, thus suggesting that these estimations have problems. Note however,
that the p-values for the second term are quite high, lending support to our
models on the split sample. These results suggest that perhaps ARMA-X models are
not right in this context as it is not unreasonable to think that Trump does 
in fact react to market movements, which would break the exogeneity assumption
that is critical for this type of model. With this information, we decided to run
a VAR model to deepen our understanding of these variables.



<!--chapter:end:03-armax.Rmd-->

# SVAR 

## Methodology

We develop a SVAR model in order to assess the impact of
short-run shocks from Trump's posts on AHV, and to evaluate whether
market volatility can, in turn, influence Trump's posting behavior. In
this framework, we systematically pair AHV with one explanatory variable
at a time (our X-regressor). The SVAR approach offers the advantage of accounting for
structural endogeneity. Our main assumption is that the volatility
does not contemporaneously affect Trump's posting activity - neither
quantitatively nor qualitatively, while Trump's posts do affect markets instantly. 
In essence, we impose a short-run restriction on the shock of volatility for all the
social media variables.

Based on the information criteria, we found similar results across all
specifications, with a recommended lag length of around 70. However,
inducing more than 6 lags (corresponding to a full trading day)
introduces strong seasonality. Moreover, the higher the number of lags,
the greater the persistence of a shock, up to unrealistic levels such as 150 days 
for the number of Tweets, which seems implausible. Therefore, we chose to fix the
number of lags at a maximum of 6. Finally, given the presence of heteroscedasticity
and serial correlation in the residuals, we use the Newey-West estimator
to compute robust standard errors.


## Results

### Full Timeframe


As in the ARMA-X framework, we initially estimate a model for each of our five
main variables. For all estimations using the SPY ETF market, the significant 
coefficients of social media variables were consistently negative, while the 
positive coefficients were large but not significant \@ref(sec:svar-table-1).
([*desc1figVAR*]{style="color:red"})
For the $Tariff$, $Trade$ and $China$ mention variables, the first, second and sometimes forth 
lag were positive and relatively large (especially in the case of Tariff), while 
the remaining ones were not. In contrast, for $TweetCount$ and $TweetDummy$, we 
observed fewer and smaller positive coefficients. At the same time, we found 
that the contemporaneous effects of the shocks were all positive and relatively 
strong. This leads to two types of scenarios : either 
the IRF's experienced a positive shock and remain elevated ($Tariff$, $Trade$ and $China$), 
or a highly positive shock occurs, but the cumulative effect turns negative after few 
hours ($TweetDummy$ and $Count$). \@ref(sec:irf-dummy), \@ref(sec:irf-cum-dummy)
[*IRFtarifVAR*]{style="color:red"} [*IRFdummyVAR*]{style="color:red"}

Finally, except for $Tariff$, all Granger causality tests indicate that Trump's 
posts have an impact on volatility. However, due to serial correlation in the 
residuals, these results should be interpreted with caution. Overall, this model 
suggest that Trump's posts tend to have a positive instantaneous effect on 
volatility, but with very low persistence.
When analyzing the European and Chinese ETF's, we observe similar patterns 
though with lower magnitude, except for the impact of $Tariff$ and $China$ on the 
ASHR ETF (Chinese Market), where the cumulative effects show no positive impact. Additionally, 
the VGK ETF (European Market) appears to react more strongly than ASHR to 
Trump's posts, especially those mentioning $Trade$ and $Tariff$.

Regarding the impact of AHV on Trump's posts, we find some evidence of a 
negative effect. For all variables, we observe one or two significantly 
negative coefficients per variables, typically on the first and fourth lag, alongside many 
insignificant ones. However, only $TweetCount$ and China pass the Granger test in 
the US ETF Surprisingly, a large number of Granger tests in the Chinese and 
European ETFs indicate strong Granger causality, which may point to a limitation 
of the test itself, as such results appear unrealistic. 


### Split Sample

For the split framework, the results were similar and striking \@ref(sec:irf-china-2)
[*IRFchinasecondtermVAR*]{style="color:red"}. While we
observed relatively small shock effects and almost entirely negative coefficients 
during the first term \@ref(sec:tab-svar-1) ([*descFirsttermfigVAR*]{style="color:red"}), leading the 
cumulative IRF's to indicate a negative impact of posts, the shock effects in the
second term were substantially larger, ranging from 5 times (for $TweetCount$ and 
$TweetDummy$) to as much as 25 times greater (for $Tariff$). the only exception was 
$Trade$ in the second term, which showed the only negative impact from a shock.
Once again, we found positive lagged coefficients in the second term, mostly on 
the first, second and forth lags. However, none of these coefficients were
statistically significant, but the cumulative IRF's clearly show a high positive 
impact on everything except for $TweetDummy$ and $TweetCount$, whose
coefficients and cumulative IRF's displayed similar patterns to those observed 
in the first term. \@ref(sec:tab-svar-2) ([*descSecondtermfigVAR*]{style="color:red"})
Moreover, the Granger tests generally failed in both terms, with the sole
exception being $China$ in the second term. 
Regarding the ASHR ETF, we found results similar to those for the US ETF. Surprisingly, 
in the case of the European market, we observed a positive
impact of Trump’s posts on AHV during the first term. Nevertheless, the results 
still indicate a stronger impact of posts during the second term.

<!--chapter:end:04-var.Rmd-->

# Conclusion

We started this project with the intention of understanding whether the impact 
of Trump's social media posts affect financial markets, and to see if there
is perhaps a difference from his first presidential mandate. After various 
headaches with our data, we first ran ARMA-X models where we found significant
and positive results albeit with strong auto-correlation in the errors, with only
the second term analysis offering more convincing results. We then try VAR models
for a possibly more accurate picture, albeit with little to no success. We once
again find strong auto-correlation in the errors, which we here fix by using 
Newey-West standard errors. We found that the only significant coefficients are
actually negative, suggesting Trump's social media presence would reduce volatility.

However, we would suggest strong against trying to interpret these results given
that the models seem to not fit particularly well. This may be due to seasonality
in our data (a common trend seen is our daily AVH is high volatility in the first 
open hours, and a gradual slowdown for the rest of the day), or to our handling 
of non-market hours. Further work could look at exploring said issues in greater depth,
further complicate the models by adding more variables and interactions between them,
and/or additionally <span style="color:red"> *use more sophisticated models, such as models including co-integration.* </span>


<!--chapter:end:05-conclusion.Rmd-->

---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

# Bibliography



`r if (knitr::is_html_output()) '
## References {-}
'`

<div id="refs"></div>

<!--chapter:end:06-bibliography.Rmd-->

# Appendix


```{r library_setup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
# Core utilities
require(tinytex)       # LaTeX
require(here)          # Directory finder
require(stringr)       # String analysis
require(dplyr)         # Data manipulation
require(lubridate)     # Date handling
require(zoo)           # Time series lags
require(vroom)         # Fast data import
require(data.table)    # Efficient data filtering
require(tibble)        # Tidy data frames
require(tidyverse)     # Includes ggplot2, dplyr, etc.
require(knitr)         # Knitting reports
require(purrr)         # Functional programming

# Time series and econometrics
require(forecast)      # Time series models
require(expm)          # Matrix exponentials
require(AER)           # Applied Econometrics with R
require(AEC)           # JP-Renne functions
require(lmtest)        # Regression diagnostics
require(sandwich)      # Robust errors
require(sysid)         # ARMAX modeling
require(aTSA)          # ADF test
require(tseries)       # Time series tests
require(TSA)           # Time Series Analysis
require(vars)          # VAR models
require(quantmod)      # Quantitative financial modeling
require(FinTS)         # Financial time series
require(fGarch)        # GARCH models
require(rugarch)       # Univariate GARCH
require(rmgarch)       # Multivariate GARCH

# Text analysis
require(tidytext)      # Text mining
require(textstem)      # Lemmatization
require(quanteda)      # Tokenization
require(syuzhet)       # Sentiment analysis

# Table and reporting
require(jtools)        # Regression tables
require(huxtable)      # HTML/PDF tables
require(stargazer)     # Regression summary tables
require(texreg)        # LaTeX/HTML model output
require(kableExtra)    # Pretty tables in knitr

# Other
require(writexl)       # Write Excel files
require(alphavantager) # Alpha Vantage API access

#-> sorted with AI

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

## ARMAX

We choose the specification in the armax_models file. In this file, we will
just run said specifications to produce nice tables and graphs to include in 
our final paper. This is also why there are specification differences in the 
separate timeframes. We always use the best fit we found earlier.

```{r datasetup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#backup
backup = data

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#for interpretation
mean1 = mean(data$SPY_vol)

```


```{r fitting models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#All SPY models for the full Dataframe

models <- list()

# ARMA-X(3,3,1) with Tweet Dummy as Exogenous
models[["Model 1"]] <- armax(data$SPY_vol, xreg = data$dummy, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,3,1) with Tweet Count as Exogenous
models[["Model 2"]] <- armax(data$SPY_vol, xreg = data$N, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Model 3"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 3, p = 3, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Model 4"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 1, p = 3, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Model 5"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 2) 

```

### SPY ARMAX Table (Jan 2014 - May 2025) {#sec:spy-table}
```{r tab-armax-full, results="asis", warning=FALSE, echo=FALSE, message=FALSE, cache=TRUE}
library(texreg)

# Define variable display names (HTML-friendly using <sub>)
names <- list(
  "ar1" = "AR(1)",
  "ar2" = "AR(2)",
  "ar3" = "AR(3)",
  "ma1" = "MA(1)",
  "ma2" = "MA(2)",
  "ma3" = "MA(3)",
  "(Intercept)" = "Constant",
  "dummy_lag_0" = "TweetDummy<sub>t</sub>",
  "dummy_lag_1" = "TweetDummy<sub>t-1</sub>",
  "N_lag_0" = "TweetCount<sub>t</sub>",
  "N_lag_1" = "TweetCount<sub>t-1</sub>",
  "tariff_lag_0" = "Tariff<sub>t</sub>",
  "tariff_lag_1" = "Tariff<sub>t-1</sub>",
  "tariff_lag_2" = "Tariff<sub>t-2</sub>",
  "tariff_lag_3" = "Tariff<sub>t-3</sub>",
  "trade_lag_0" = "Trade<sub>t</sub>",
  "trade_lag_1" = "Trade<sub>t-1</sub>",
  "china_lag_0" = "China<sub>t</sub>"
)

# Output the HTML table directly
cat(
  htmlreg(
    models,
    custom.model.names = names(models),
    custom.coef.map = names,
    caption = "ARMAX Models of Average Hourly Volatility",
    caption.above = TRUE,
    digits = 4,
    doctype = FALSE
  ))

```


### SPY ARMAX IRFs (Jan 2014 - May 2025){#sec:SPY-IRF}

```{r fig-SPYirf, results="asis", warning=F, message=F, echo=FALSE, cache=TRUE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

#irf.plot(models[["Model 1"]],nb.periods,title="Tweet Dummy Shock")
#irf.plot(models[["Model 2"]],nb.periods,title="Tweet Count Shock")
plot1 = irf.plot(models[["Model 3"]],nb.periods,
                 title="Tariff Mention Shock - Jan 2014 - May 2025")
plot1
#irf.plot(models[["Model 4"]],nb.periods,title="Trade Mention Shock")
#irf.plot(models[["Model 5"]],nb.periods,title="China Mention Shock")

ggsave("armax_plot1.png",plot=plot1,bg="white")

```


```{r, warning=F, message=F, echo=FALSE, results=F}
#Calculate residuals

res1 = checkresiduals(models[["Model 1"]], plot = FALSE)
res2 = checkresiduals(models[["Model 2"]], plot = FALSE)
res3 = checkresiduals(models[["Model 3"]], plot = FALSE)
res4 = checkresiduals(models[["Model 4"]], plot = FALSE)
res5 = checkresiduals(models[["Model 5"]], plot = FALSE)

```

```{r SPYres, warning=F, message=F, echo=FALSE, cache=TRUE}

resnames = c("Twitter Dummy", "Twitter Count", "Tariff", "Trade", "China")

#extract p-values directly from checkresiduals results
pvals <- data.frame("X-Regressor" = resnames,
                    `Full Timeframe` = c(
                      res1$p.value,
                      res2$p.value,
                      res3$p.value,
                      res4$p.value,
                      res5$p.value))


```




```{r datasetup first, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#First term Calculations

#load final dataset
data = backup

#first term
data = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#for interpretation
mean2 = mean(data$SPY_vol)

```


```{r 1st term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#SPY Models first Term

models <- list()

# ARMA-X(3,3,0) with Tariff Mentions as Exogenous
models[["First Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 3, q = 3)

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

```


```{r 1st SPYresiduals, warning=F, message=F, results=F, echo=FALSE, cache=TRUE}
# Run first term residuals
res6 = checkresiduals(models[["First Term (1)"]], plot = FALSE)
res7 = checkresiduals(models[["First Term (2)"]], plot = FALSE)
res8 = checkresiduals(models[["First Term (3)"]], plot = FALSE)

pvals_new1 <- data.frame(
  "First-Term" = c(
    NA,
    NA,
    res6$p.value,
    res7$p.value,
    res8$p.value))

```




```{r datasetup second, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#Second Term Calculations

#load final dataset
data = backup

#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

#for interpretation
mean3 = mean(data$SPY_vol)

```


```{r 2nd term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#Run Second Term Models

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 1, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

```



```{r 2nd SPYresiduals, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
# Calculate SPY residuals for the second term

res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)

pvals_new2 <- data.frame(
  "Second-Term" = c(
    NA,
    NA,
    res9$p.value,
    res10$p.value,
    res11$p.value))

#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)

```


### SPY ARMAX Table (Split Presidential Terms){#sec:spy-table-terms}

\centering

```{r tab-armax-split, results="asis", warning=F, echo=FALSE, message=F, cache=TRUE}

library(texreg)

# Change model names (not sure if needed anymore)
model_names <- c(
 "First Term (1)", "First Term (2)", "First Term (3)",
 "Second Term (1)", "Second Term (2)", "Second Term (3)"
)
names(models) <- model_names

# HTML coefficients names adjusted (is this correct?)
xnames_ordered <- c(
  "AR(1)", "AR(2)", "AR(3)",
  "MA(1)", "MA(2)", "MA(3)",
  "Constant",
  "Tariff<sub>t</sub>", "Tariff<sub>t-1</sub>", "Tariff<sub>t-2</sub>",
  "Trade<sub>t</sub>",
  "China<sub>t</sub>", "China<sub>t-1</sub>", "China<sub>t-2</sub>"
)

# Render as html -> need to render file to have it nice
htmlreg(
  models,
  custom.coef.names = xnames_ordered,
  custom.model.names = model_names,
  caption = "Split-Term ARMAX Models of Average Hourly Volatility",
  caption.above = TRUE,
  digits = 4,
  stars = c(0.001, 0.01, 0.05),
  doctype = FALSE)


```


### SPY ARMAX IRFs (Split Terms){#sec:SPY-SPLIT-IRF}

```{r 2nd SPYirf, warning=F, message=F, echo=FALSE, cache=TRUE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

plot2 = irf.plot(models[["Second Term (1)"]],nb.periods,
                 title="Tariff Mention Shock - Second Term")
plot2

#ggsave("armax_plot2.png",plot=plot2,bg="white")

plot3 = irf.plot(models[["Second Term (3)"]],nb.periods,
                 title="China Mention Shock - Second Term")
plot3

#ggsave("armax_plot3.png",plot=plot3,bg="white")

```


### Residual Test {#sec:SPY-res-test}

```{r tab-res-test, message=F, warning=F, results="asis", echo=FALSE, cache=TRUE}

kable(pvals_combined, digits = 6, format="html", caption = "Ljung-Box Test p-values for Residuals") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```

### Descriptive Stats{#sec:means-table}
 
```{r tab-means, message=F, warning=F, results="asis", cache=TRUE, echo=FALSE}

means <- data.frame(
  Model = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
  `SPY Volatility Mean` = c(
    mean1,
    mean2,
    mean3))

kable(means, digits = 6, format="html", caption = "Summary Statistics of SPY Volatility") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```

## VAR

```{r load_data_var, message=FALSE, warning=FALSE, results=FALSE, echo=FALSE}
#select timeframe 
Vdata = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))
```

```{r Estime dummy, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# Dummy Estimates
#-------------------------

y = cbind(Vdata$dummy, Vdata$SPY_vol)
colnames(y)[1:2] <- c("dummy", "vol")
est.VAR <- VAR(y,p=6)

#extract results
mod_vol = est.VAR$varresult$vol
f = formula(mod_vol)
d = model.frame(mod_vol)
lm_clean = lm(f, data= d)

#apply Newey-West
nw_vcov = NeweyWest(lm_clean, lag=6)
nw_se = sqrt(diag(nw_vcov))

#t-stats
coef = coef(lm_clean)
t_stat = coef/nw_se

#recalculate p-values
robust = 2*(1-pt(abs(t_stat), df = df.residual(lm_clean)))
```

```{r B mat, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
#Recreate a Robust Omega Matrix
U = residuals(est.VAR)
T = nrow(U)
L = 6 #number of lag
Omega = matrix(0, ncol(U), ncol(U))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l_ = t(U[(l+1):T, , drop=FALSE]) %*% U[1:(T-l), , drop=FALSE] /T
  if (l == 0){
    Omega = Omega + Gamma_l_
  } else {
    Omega = Omega + weight*(Gamma_l_ + t(Gamma_l_))
  }
}

#make the B matrix
loss <- function(param){
  #Define the restriction
  B <- matrix(c(param[1], param[2], 0, param[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X <- Omega - B %*% t(B)
  
  #loss function
  loss <- sum(X^2)
  return(loss)
}

res.opt <- optim(c(1, 0, 1), loss, method = "BFGS")
B.hat <- matrix(c(res.opt$par[1], res.opt$par[2], 0, res.opt$par[3]), ncol = 2)
```

### SVAR IRF for Tweet Dummy{#sec:irf-dummy}

```{r IRF 1dummy, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
nb.sim = 7*7
#get back the coefficient of est.VAR
phi <- Acoef(est.VAR)
PHI = make.PHI(phi)

#take the constant
constant <- sapply(est.VAR$varresult, function(eq) coef(eq)["const"])
c=as.matrix(constant)

#Simulate the IRF
p <- length(phi)
n <- dim(phi[[1]])[1]


Y <- simul.VAR(c=c, Phi = phi, B = B.hat, nb.sim ,y0.star=rep(0, n*p),
                  indic.IRF = 1, u.shock = c(1,0))



Yd = data.frame(
  period = 1:nrow(Y),
  response = Y[,2])

ggplot(Yd,aes(x=period, y=response)) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P IRF of Dummy on Volatility")+
  ylab("")+
  xlab("") +
  theme_minimal()

```

### SVAR Cumultative IRF of Tweet Dummy{#sec:irf-cum-dummy}

```{r IRF 2Dummy, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
ggplot(Yd,aes(x=period, y=cumsum(response))) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P Cumulalitve IRF of Dummy on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```

```{r estimate with N, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
y2 = cbind(Vdata$N, Vdata$SPY_vol)
colnames(y2)[1:2] <- c("N", "vol")
est.VAR2 <- VAR(y2,p=6)

#extract results
mod_vol2 = est.VAR2$varresult$vol
f2 = formula(mod_vol2)
d2 = model.frame(mod_vol2)
lm_clean2 = lm(f2, data= d2)

#apply Newey-West
nw_vcov2 = NeweyWest(lm_clean2, lag=6)
nw_se2 = sqrt(diag(nw_vcov2))

#t-stats
coef2 = coef(lm_clean2)
t_stat2 = coef2/nw_se2

#recalculate p-values
robust2 = 2*(1-pt(abs(t_stat2), df = df.residual(lm_clean2)))

```

```{r estime tariff, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# Tariff Estimates
#---------------------
y3 = cbind(Vdata$tariff, Vdata$SPY_vol)
colnames(y3)[1:2] <- c("tariff", "vol")
est.VAR3 <- VAR(y3,p=6)

#extract results
mod_vol3 = est.VAR3$varresult$vol
f3 = formula(mod_vol3)
d3 = model.frame(mod_vol3)
lm_clean3 = lm(f3, data= d3)

#apply Newey-West
nw_vcov3 = NeweyWest(lm_clean3, lag=6)
nw_se3 = sqrt(diag(nw_vcov3))

#t-stats
coef3 = coef(lm_clean3)
t_stat3 = coef3/nw_se3

#recalculate p-values
robust3 = 2*(1-pt(abs(t_stat3), df = df.residual(lm_clean3)))

```

```{r B mat3, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
#Recreate a Robust Omega Matrix
U3 = residuals(est.VAR3)
T3 = nrow(U3)
Omega3 = matrix(0, ncol(U3), ncol(U3))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l_3 = t(U3[(l+1):T3, , drop=FALSE]) %*% U3[1:(T3-l), , drop=FALSE] /T3
  if (l == 0){
    Omega3 = Omega3 + Gamma_l_3
  } else {
    Omega3 = Omega3 + weight*(Gamma_l_3 + t(Gamma_l_3))
  }
}


#make the B matrix
loss3 <- function(param3){
  #Define the restriction
  B3 <- matrix(c(param3[1], param3[2], 0, param3[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X3 <- Omega3 - B3 %*% t(B3)
  
  #loss function
  loss3 <- sum(X3^2)
  return(loss3)
}

res.opt3 <- optim(c(1, 0, 1), loss3, method = "BFGS")
B.hat3 <- matrix(c(res.opt3$par[1], res.opt3$par[2], 0, res.opt3$par[3]), ncol = 2)



```

### SVAR IRF for Tariff{#sec:irf-tariff}

```{r IRF tariffff, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
#get back the coefficient of est.VAR
phi3 <- Acoef(est.VAR3)
PHI3 = make.PHI(phi3)

#take the constant
constant3 <- sapply(est.VAR3$varresult, function(eq) coef(eq)["const"])
c3=as.matrix(constant3)

#Simulate the IRF
p3 <- length(phi3)
n3 <- dim(phi3[[1]])[1]

Y3 <- simul.VAR(c=c3, Phi = phi3, B = B.hat3, nb.sim ,y0.star=rep(0, n3*p3),
                  indic.IRF = 1, u.shock = c(1,0))


#Plot the IRF
Yd3 = data.frame(
  period = 1:nrow(Y3),
  response = Y3[,2])

ggplot(Yd3,aes(x=period, y=response)) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P IRF of tariff on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```

### SVAR Cumultatove IRF for Tariff{#sec:irf-cum-tariff}

```{r IRF tariff, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
ggplot(Yd3,aes(x=period, y=cumsum(response))) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P Cumulalitve IRF of tariff on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```


```{r estimate trade, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# Trade Estimates
#----------------------

y4 = cbind(Vdata$trade, Vdata$SPY_vol)
colnames(y4)[1:2] <- c("trade", "vol")
est.VAR4 <- VAR(y4,p=6)

#extract results
mod_vol4 = est.VAR4$varresult$vol
f4 = formula(mod_vol4)
d4 = model.frame(mod_vol4)
lm_clean4 = lm(f4, data= d4)

#apply Newey-West
nw_vcov4 = NeweyWest(lm_clean4, lag=6)
nw_se4 = sqrt(diag(nw_vcov4))

#t-stats
coef4 = coef(lm_clean4)
t_stat4 = coef4/nw_se4

#recalculate p-values
robust4 = 2*(1-pt(abs(t_stat4), df = df.residual(lm_clean4)))


```

```{r estime China, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
# Chinae Estimates
#---------------------
ychina = cbind(Vdata$china, Vdata$SPY_vol)
colnames(ychina)[1:2] <- c("china", "vol")
est.VARchina <- VAR(ychina,p=6)

#extract results
mod_volchina = est.VARchina$varresult$vol
fchina = formula(mod_volchina)
dchina = model.frame(mod_volchina)
lm_cleanchina = lm(fchina, data= dchina)

#apply Newey-West
nw_vcovchina = NeweyWest(lm_cleanchina, lag=6)
nw_sechina = sqrt(diag(nw_vcovchina))

#t-stats
coefchina = coef(lm_cleanchina)
t_statchina = coefchina/nw_sechina

#recalculate p-values
robustchina = 2*(1-pt(abs(t_statchina), df = df.residual(lm_cleanchina)))
```

### SVAR Table Full Timeframe

```{r mana, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
dt_t = d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)


f_t <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model <- lm(f_t, data = dt_t)

dt_t2 = d2 %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)



f_t2 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model2 <- lm(f_t2, data = dt_t2)


dt_t3 = d3 %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)



f_t3 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model3 <- lm(f_t3, data = dt_t3)


dt_t4 = d4 %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)
  


f_t4 <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model4 <- lm(f_t4, data = dt_t4)

dt_tchina = dchina %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)



f_tchina <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
modelchina <- lm(f_tchina, data = dt_tchina)


nw_se_t <- sqrt(diag(sandwich::NeweyWest(model, lag = 6)))
nw_se2_t <- sqrt(diag(sandwich::NeweyWest(model2, lag = 6)))
nw_se3_t <- sqrt(diag(sandwich::NeweyWest(model3, lag = 6)))
nw_se4_t <- sqrt(diag(sandwich::NeweyWest(model4, lag = 6)))
nw_sechina_t <- sqrt(diag(sandwich::NeweyWest(modelchina, lag = 6)))


robust_t <- 2 * (1-pt(abs(coef(model) / nw_se_t), df = df.residual(model)))
robust2_t <- 2 * (1-pt(abs(coef(model2) / nw_se2_t), df = df.residual(model2)))
robust3_t <- 2 * (1-pt(abs(coef(model3) / nw_se3_t), df = df.residual(model3)))
robust4_t <- 2 * (1-pt(abs(coef(model4) / nw_se4_t), df = df.residual(model4)))
robustchina_t <- 2 * (1-pt(abs(coef(modelchina) / nw_sechina_t), df = df.residual(modelchina)))



nw_se_t       <- nw_se_t[names(coef(model))]
robust_t      <- robust_t[names(coef(model))]
nw_se2_t      <- nw_se2_t[names(coef(model2))]
robust2_t     <- robust2_t[names(coef(model2))]
nw_se3_t      <- nw_se3_t[names(coef(model3))]
robust3_t     <- robust3_t[names(coef(model3))]
nw_se4_t      <- nw_se4_t[names(coef(model4))]
robust4_t     <- robust4_t[names(coef(model4))]
nw_sechina_t  <- nw_sechina_t[names(coef(modelchina))]
robustchina_t <- robustchina_t[names(coef(modelchina))]




# Create list for models
models_list <- list(model, model2, model3, model4, modelchina)

# Create list for SE robustes
robust_ses <- list(nw_se_t, nw_se2_t, nw_se3_t, nw_se4_t, nw_sechina_t)

# Create list for p-value
robust_pvals <- list(robust_t, robust2_t, robust3_t, robust4_t, robustchina_t)

# Name of Variables
custom_names <- list(
  "vol.l1" = "$AHV_{t-1}$",
  "vol.l2" = "$AHV_{t-2}$",
  "vol.l3" = "$AHV_{t-3}$",
  "vol.l4" = "$AHV_{t-4}$",
  "vol.l5" = "$AHV_{t-5}$",
  "vol.l6" = "$AHV_{t-6}$",
  "X.l1" = "$X_{t-1}$",
  "X.l2" = "$X_{t-2}$",
  "X.l3" = "$X_{t-3}$",
  "X.l4" = "$X_{t-4}$",
  "X.l5" = "$X_{t-5}$",
  "X.l6" = "$X_{t-6}$",
  "const" = "Constant"
)
```
### SVAR Table Full Timeframe{#sec:svar-table-1}

```{r tab:first-var, warning=FALSE, results='asis', echo=FALSE}
tmpfile <- tempfile(fileext = ".html")  # create temp file

table_texreg_html <- htmlreg(
  l = models_list,
  override.se = robust_ses,
  override.pvalues = robust_pvals,
  custom.coef.map = custom_names,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  caption = "VAR Models of Average Hourly Volatility",
  digits = 6,
  custom.gof.rows = list("Shock (IRF)" = c(0.0041713, 0.003061, 0.001189, 0.000215, 0.001937)),
  custom.note = "This table displays VAR regression with only two variables: AHV and the X regressor. Column names represent the X variable for the selected model.",
  file = tmpfile
)

# Manually add a caption in HTML format
cat(readLines(tmpfile), sep = "\n")
```




```{r datasetup2, results=FALSE, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
data <- c(0)

source(here("helperfunctions/full_data.R"))



# First and Second Mandate

#first term
Vdata_f = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#second term
Vdata_s = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

```



```{r first mandate dum, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# First Term Estimates
#------------------------
# Dummy


y_f_d = cbind(Vdata_f$dummy, Vdata_f$SPY_vol)
colnames(y_f_d)[1:2] <- c("dummy", "vol")
est.VAR_f_d <- VAR(y_f_d,p=6)

#extract results
mod_vol_f_d = est.VAR_f_d$varresult$vol
f_f_d = formula(mod_vol_f_d)
d_f_d = model.frame(mod_vol_f_d)
lm_clean_f_d = lm(f_f_d, data= d_f_d)

#apply Newey-West
nw_vcov_f_d = NeweyWest(lm_clean_f_d, lag=6)
nw_se_f_d = sqrt(diag(nw_vcov_f_d))

#t-stats
coef_f_d = coef(lm_clean_f_d)
t_stat_f_d = coef_f_d/nw_se_f_d

#recalculate p-values
robust_f_d = 2*(1-pt(abs(t_stat_f_d), df = df.residual(lm_clean_f_d)))


```


```{r first mandate N, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# First Mandate Calculations
#-------------------------------
y_f_n = cbind(Vdata_f$N, Vdata_f$SPY_vol)
colnames(y_f_n)[1:2] <- c("N", "vol")
est.VAR_f_n <- VAR(y_f_n,p=6)

#extract results
mod_vol_f_n = est.VAR_f_n$varresult$vol
f_f_n = formula(mod_vol_f_n)
d_f_n = model.frame(mod_vol_f_n)
lm_clean_f_n = lm(f_f_n, data= d_f_n)

#apply Newey-West
nw_vcov_f_n = NeweyWest(lm_clean_f_n, lag=6)
nw_se_f_n = sqrt(diag(nw_vcov_f_n))

#t-stats
coef_f_n = coef(lm_clean_f_n)
t_stat_f_n = coef_f_n/nw_se_f_n

#recalculate p-values
robust_f_n = 2*(1-pt(abs(t_stat_f_n), df = df.residual(lm_clean_f_n)))
```


```{r first mandate tariff, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# Tariff first Mandate
#--------------------------

y_f_ta = cbind(Vdata_f$tariff, Vdata_f$SPY_vol)
colnames(y_f_ta)[1:2] <- c("tariff", "vol")
est.VAR_f_ta <- VAR(y_f_ta,p=6)

#extract results
mod_vol_f_ta = est.VAR_f_ta$varresult$vol
f_f_ta = formula(mod_vol_f_ta)
d_f_ta = model.frame(mod_vol_f_ta)
lm_clean_f_ta = lm(f_f_ta, data= d_f_ta)

#apply Newey-West
nw_vcov_f_ta = NeweyWest(lm_clean_f_ta, lag=6)
nw_se_f_ta = sqrt(diag(nw_vcov_f_ta))

#t-stats
coef_f_ta = coef(lm_clean_f_ta)
t_stat_f_ta = coef_f_ta/nw_se_f_ta

#recalculate p-values
robust_f_ta = 2*(1-pt(abs(t_stat_f_ta), df = df.residual(lm_clean_f_ta)))

```


```{r first mandate trade, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# Trade FIrst Mandate
#------------------------
y_f_tr = cbind(Vdata_f$trade, Vdata_f$SPY_vol)
colnames(y_f_tr)[1:2] <- c("trade", "vol")
est.VAR_f_tr <- VAR(y_f_tr,p=6)

#extract results
mod_vol_f_tr = est.VAR_f_tr$varresult$vol
f_f_tr = formula(mod_vol_f_tr)
d_f_tr = model.frame(mod_vol_f_tr)
lm_clean_f_tr = lm(f_f_tr, data= d_f_tr)

#apply Newey-West
nw_vcov_f_tr = NeweyWest(lm_clean_f_tr, lag=6)
nw_se_f_tr = sqrt(diag(nw_vcov_f_tr))

#t-stats
coef_f_tr = coef(lm_clean_f_tr)
t_stat_f_tr = coef_f_tr/nw_se_f_tr

#recalculate p-values
robust_f_tr = 2*(1-pt(abs(t_stat_f_tr), df = df.residual(lm_clean_f_tr)))
```


```{r first mandate china, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# China First Mandate
#-----------------------

y_f_ch = cbind(Vdata_f$china, Vdata_f$SPY_vol)
colnames(y_f_ch)[1:2] <- c("china", "vol")
est.VAR_f_ch <- VAR(y_f_ch,p=6)

#extract results
mod_vol_f_ch = est.VAR_f_ch$varresult$vol
f_f_ch = formula(mod_vol_f_ch)
d_f_ch = model.frame(mod_vol_f_ch)
lm_clean_f_ch = lm(f_f_ch, data= d_f_ch)

#apply Newey-West
nw_vcov_f_ch = NeweyWest(lm_clean_f_ch, lag=6)
nw_se_f_ch = sqrt(diag(nw_vcov_f_ch))

#t-stats
coef_f_ch = coef(lm_clean_f_ch)
t_stat_f_ch = coef_f_ch/nw_se_f_ch

#recalculate p-values
robust_f_ch = 2*(1-pt(abs(t_stat_f_ch), df = df.residual(lm_clean_f_ch)))


```


```{r second mandate dum, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# Second term
#---------------------
y_s_d = cbind(Vdata_s$dummy, Vdata_s$SPY_vol)
colnames(y_s_d)[1:2] <- c("dummy", "vol")
est.VAR_s_d <- VAR(y_s_d,p=6)

#extract results
mod_vol_s_d = est.VAR_s_d$varresult$vol
f_s_d = formula(mod_vol_s_d)
d_s_d = model.frame(mod_vol_s_d)
lm_clean_s_d = lm(f_s_d, data= d_s_d)

#apply Newey-West
nw_vcov_s_d = NeweyWest(lm_clean_s_d, lag=6)
nw_se_s_d = sqrt(diag(nw_vcov_s_d))

#t-stats
coef_s_d = coef(lm_clean_s_d)
t_stat_s_d = coef_s_d/nw_se_s_d

#recalculate p-values
robust_s_d = 2*(1-pt(abs(t_stat_s_d), df = df.residual(lm_clean_s_d)))
```


```{r second mandate N, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}

y_s_n = cbind(Vdata_s$N, Vdata_s$SPY_vol)
colnames(y_s_n)[1:2] <- c("N", "vol")
est.VAR_s_n <- VAR(y_s_n,p=6)

#extract results
mod_vol_s_n = est.VAR_s_n$varresult$vol
f_s_n = formula(mod_vol_s_n)
d_s_n = model.frame(mod_vol_s_n)
lm_clean_s_n = lm(f_s_n, data= d_s_n)

#apply Newey-West
nw_vcov_s_n = NeweyWest(lm_clean_s_n, lag=6)
nw_se_s_n = sqrt(diag(nw_vcov_s_n))

#t-stats
coef_s_n = coef(lm_clean_s_n)
t_stat_s_n = coef_s_n/nw_se_s_n

#recalculate p-values
robust_s_n = 2*(1-pt(abs(t_stat_s_n), df = df.residual(lm_clean_s_n)))


```


```{r second mandate tariff, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# Tariff
#-------------

y_s_ta = cbind(Vdata_s$tariff, Vdata_s$SPY_vol)
colnames(y_s_ta)[1:2] <- c("tariff", "vol")
est.VAR_s_ta <- VAR(y_s_ta,p=6)

#extract results
mod_vol_s_ta = est.VAR_s_ta$varresult$vol
f_s_ta = formula(mod_vol_s_ta)
d_s_ta = model.frame(mod_vol_s_ta)
lm_clean_s_ta = lm(f_s_ta, data= d_s_ta)

#apply Newey-West
nw_vcov_s_ta = NeweyWest(lm_clean_s_ta, lag=6)
nw_se_s_ta = sqrt(diag(nw_vcov_s_ta))

#t-stats
coef_s_ta = coef(lm_clean_s_ta)
t_stat_s_ta = coef_s_ta/nw_se_s_ta

#recalculate p-values
robust_s_ta = 2*(1-pt(abs(t_stat_s_ta), df = df.residual(lm_clean_s_ta)))

```


```{r second mandate trade, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# Trade
#----------

y_s_tr = cbind(Vdata_s$trade, Vdata_s$SPY_vol)
colnames(y_s_tr)[1:2] <- c("trade", "vol")
est.VAR_s_tr <- VAR(y_s_tr,p=6)

#extract results
mod_vol_s_tr = est.VAR_s_tr$varresult$vol
f_s_tr = formula(mod_vol_s_tr)
d_s_tr = model.frame(mod_vol_s_tr)
lm_clean_s_tr = lm(f_s_tr, data= d_s_tr)

#apply Newey-West
nw_vcov_s_tr = NeweyWest(lm_clean_s_tr, lag=6)
nw_se_s_tr = sqrt(diag(nw_vcov_s_tr))

#t-stats
coef_s_tr = coef(lm_clean_s_tr)
t_stat_s_tr = coef_s_tr/nw_se_s_tr

#recalculate p-values
robust_s_tr = 2*(1-pt(abs(t_stat_s_tr), df = df.residual(lm_clean_s_tr)))

```


```{r second mandate china, warning=FALSE, message=FALSE, results="asis", cache=TRUE, echo=FALSE}
# China
#--------------

y_s_ch = cbind(Vdata_s$china, Vdata_s$SPY_vol)
colnames(y_s_ch)[1:2] <- c("china", "vol")
est.VAR_s_ch <- VAR(y_s_ch,p=6)

#extract results
mod_vol_s_ch = est.VAR_s_ch$varresult$vol
f_s_ch = formula(mod_vol_s_ch)
d_s_ch = model.frame(mod_vol_s_ch)
lm_clean_s_ch = lm(f_s_ch, data= d_s_ch)

#apply Newey-West
nw_vcov_s_ch = NeweyWest(lm_clean_s_ch, lag=6)
nw_se_s_ch = sqrt(diag(nw_vcov_s_ch))

#t-stats
coef_s_ch = coef(lm_clean_s_ch)
t_stat_s_ch = coef_s_ch/nw_se_s_ch

#recalculate p-values
robust_s_ch = 2*(1-pt(abs(t_stat_s_ch), df = df.residual(lm_clean_s_ch)))

```


```{r B mat second mandate china, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
#Construct the Robust Omega Matrix
L <- 6
nb.sim = 7*7

U_s_ch = residuals(est.VAR_s_ch)
T_s_ch = nrow(U_s_ch)
Omega_s_ch = matrix(0, ncol(U_s_ch), ncol(U_s_ch))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l__s_ch = t(U_s_ch[(l+1):T_s_ch, , drop=FALSE]) %*% U_s_ch[1:(T_s_ch-l), , drop=FALSE] /T_s_ch
  if (l == 0){
    Omega_s_ch = Omega_s_ch + Gamma_l__s_ch
  } else {
    Omega_s_ch = Omega_s_ch + weight*(Gamma_l__s_ch + t(Gamma_l__s_ch))
  }
}


#make the B matrix
loss_s_ch <- function(param_s_ch){
  #Define the restriction
  B_s_ch <- matrix(c(param_s_ch[1], param_s_ch[2], 0, param_s_ch[3]), ncol = 2)
  
  #Make BB' approximatively equal to omega
  X_s_ch <- Omega_s_ch - B_s_ch %*% t(B_s_ch)
  
  #loss function
  loss_s_ch <- sum(X_s_ch^2)
  return(loss_s_ch)
}

res.opt_s_ch <- optim(c(1, 0, 1), loss_s_ch, method = "BFGS")
B.hat_s_ch <- matrix(c(res.opt_s_ch$par[1], res.opt_s_ch$par[2], 0, res.opt_s_ch$par[3]), ncol = 2)



```

### SVAR IRF for China - Second Term{#sec:irf-china-2}

```{r IRF second china, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
#get back the coefficient of est.VAR
phi_s_ch <- Acoef(est.VAR_s_ch)
PHI_s_ch = make.PHI(phi_s_ch)

#take the constant
constant_s_ch <- sapply(est.VAR_s_ch$varresult, function(eq) coef(eq)["const"])
c_s_ch=as.matrix(constant_s_ch)

#Simulate the IRF
p_s_ch <- length(phi_s_ch)
n_s_ch <- dim(phi_s_ch[[1]])[1]

Y_s_ch <- simul.VAR(c=c_s_ch, Phi = phi_s_ch, B = B.hat_s_ch, nb.sim ,y0.star=rep(0, n_s_ch*p_s_ch),
                  indic.IRF = 1, u.shock = c(1,0))


#Plot the IRF
Yd_s_ch = data.frame(
  period = 1:nrow(Y_s_ch),
  response = Y_s_ch[,2])

ggplot(Yd_s_ch,aes(x=period, y=response)) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P IRF of Second Term China on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```

### SVAR Cumultative IRF for China - Second Term{#sec:irf-cum-china-2}
```{r IRF _s_ch, cache=TRUE, echo=FALSE}
ggplot(Yd_s_ch,aes(x=period, y=cumsum(response))) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P Cumulalitve IRF of Second Term China on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```

### SVAR Table First Term{#sec:tab-svar-1}

```{r tab-second-var, message=FALSE, warning=FALSE, results='asis', cache=TRUE, echo=FALSE}


#first

d_f_d_t = d_f_d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)

f_t_f_d <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_d <- lm(f_t_f_d, data = d_f_d_t)

d_f_n_t = d_f_n %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)

f_t_f_n <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_n <- lm(f_t_f_n, data = d_f_n_t)

d_f_ta_t = d_f_ta %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)

f_t_f_ta <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_ta <- lm(f_t_f_ta, data = d_f_ta_t)


d_f_tr_t = d_f_tr %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)

f_t_f_tr <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_tr <- lm(f_t_f_tr, data = d_f_tr_t)


d_f_ch_t = d_f_ch %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)

f_t_f_ch <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_f_ch <- lm(f_t_f_ch, data = d_f_ch_t)




nw_se_f_d_t <- sqrt(diag(sandwich::NeweyWest(model_f_d, lag = 6)))
nw_se_f_n_t <- sqrt(diag(sandwich::NeweyWest(model_f_n, lag = 6)))
nw_se_f_ta_t <- sqrt(diag(sandwich::NeweyWest(model_f_ta, lag = 6)))
nw_se_f_tr_t <- sqrt(diag(sandwich::NeweyWest(model_f_tr, lag = 6)))
nw_se_f_china_t <- sqrt(diag(sandwich::NeweyWest(model_f_ch, lag = 6)))


robust_f_d_t <- 2 * (1-pt(abs(coef(model_f_d) / nw_se_f_d_t), df = df.residual(model_f_d)))
robust_f_n_t <- 2 * (1-pt(abs(coef(model_f_n) / nw_se_f_n_t), df = df.residual(model_f_n)))
robust_f_ta_t <- 2 * (1-pt(abs(coef(model_f_ta) / nw_se_f_ta_t), df = df.residual(model_f_ta)))
robust_f_tr_t <- 2 * (1-pt(abs(coef(model_f_tr) / nw_se_f_tr_t), df = df.residual(model_f_tr)))
robust_f_ch_t <- 2 * (1-pt(abs(coef(model_f_ch) / nw_se_f_china_t), df = df.residual(model_f_ch)))


nw_se_f_d_t       <- nw_se_f_d_t[names(coef(model_f_d))]
robust_f_d_t      <- robust_f_d_t[names(coef(model_f_d))]
nw_se_f_n_t      <- nw_se_f_n_t[names(coef(model_f_n))]
robust_f_n_t     <- robust_f_n_t[names(coef(model_f_n))]
nw_se_f_ta_t      <- nw_se_f_ta_t[names(coef(model_f_ta))]
robust_f_ta_t     <- robust_f_ta_t[names(coef(model_f_ta))]
nw_se_f_tr_t     <- nw_se_f_tr_t[names(coef(model_f_tr))]
robust_f_tr_t     <- robust_f_tr_t[names(coef(model_f_tr))]
nw_se_f_china_t  <- nw_se_f_china_t[names(coef(model_f_ch))]
robust_f_ch_t <- robust_f_ch_t[names(coef(model_f_ch))]


# list of models, SE and p-value
models_list_f <- list(model_f_d, model_f_n, model_f_ta, model_f_tr, model_f_ch)
robust_ses_f <- list(nw_se_f_d_t, nw_se_f_n_t, nw_se_f_ta_t, nw_se_f_tr_t, nw_se_f_china_t)
robust_pvals_f <- list(robust_f_d_t, robust_f_n_t, robust_f_ta_t, robust_f_tr_t, robust_f_ch_t)

# Name of coefficient
custom_names <- list(
  "vol.l1" = "$AHV_{t-1}$",
  "vol.l2" = "$AHV_{t-2}$",
  "vol.l3" = "$AHV_{t-3}$",
  "vol.l4" = "$AHV_{t-4}$",
  "vol.l5" = "$AHV_{t-5}$",
  "vol.l6" = "$AHV_{t-6}$",
  "X.l1" = "$X_{t-1}$",
  "X.l2" = "$X_{t-2}$",
  "X.l3" = "$X_{t-3}$",
  "X.l4" = "$X_{t-4}$",
  "X.l5" = "$X_{t-5}$",
  "X.l6" = "$X_{t-6}$",
  "const" = "Constant"
)

tmpfile_f <- tempfile(fileext = ".html")  # create temp file

# Generate table
table_texreg_f <- htmlreg(
  l = models_list_f,
  override.se = robust_ses_f,
  override.pvalues = robust_pvals_f,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  custom.coef.map = custom_names,
  caption = "First-Term VAR Models of Average Hourly Volatility",
  label = "tab:VAR_First_Term",
  caption.above = TRUE,
  digits = 6,
  custom.gof.rows = list("Shock (IRF)" = c(0.002919, 0.002236, 0.000484, 0.000702, 0.000904)),
  star.cutoffs = c(0.001, 0.01, 0.05),
  custom.note = "This table displays VAR regression with only two variables : AHV and X regressor. The column names represent the X variable for the selected model.",
  file = tmpfile_f
)

# Print table
cat(readLines(tmpfile_f), sep = "\n")

```

### SVAR Table Second Term{#sec:tab-svar-2}

```{r tab-third-var, message=FALSE, warning=FALSE, results='asis', cache=TRUE, echo=FALSE}

#Second

d_s_d_t = d_s_d %>%
    rename(X.l1 = dummy.l1,
    X.l2 = dummy.l2,
    X.l3 = dummy.l3,
    X.l4 = dummy.l4,
    X.l5 = dummy.l5,
    X.l6 = dummy.l6)

f_t_s_d <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_d <- lm(f_t_s_d, data = d_s_d_t)


d_s_n_t = d_s_n %>%
    rename(X.l1 = N.l1,
    X.l2 = N.l2,
    X.l3 = N.l3,
    X.l4 = N.l4,
    X.l5 = N.l5,
    X.l6 = N.l6)

f_t_s_n <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_n <- lm(f_t_s_n, data = d_s_n_t)


d_s_ta_t = d_s_ta %>%
    rename(X.l1 = tariff.l1,
    X.l2 = tariff.l2,
    X.l3 = tariff.l3,
    X.l4 = tariff.l4,
    X.l5 = tariff.l5,
    X.l6 = tariff.l6)

f_t_s_ta <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_ta <- lm(f_t_s_ta, data = d_s_ta_t)


d_s_tr_t = d_s_tr %>%
    rename(X.l1 = trade.l1,
    X.l2 = trade.l2,
    X.l3 = trade.l3,
    X.l4 = trade.l4,
    X.l5 = trade.l5,
    X.l6 = trade.l6)


f_t_s_tr <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_tr <- lm(f_t_s_tr, data = d_s_tr_t)


d_s_ch_t = d_s_ch %>%
    rename(X.l1 = china.l1,
    X.l2 = china.l2,
    X.l3 = china.l3,
    X.l4 = china.l4,
    X.l5 = china.l5,
    X.l6 = china.l6)

f_t_s_ch <- as.formula("y ~ -1 + vol.l1 + vol.l2 + vol.l3 + vol.l4 + vol.l5 + vol.l6 +
                         X.l1 + X.l2 + X.l3 + X.l4 + X.l5 + X.l6 + const")
model_s_ch <- lm(f_t_s_ch, data = d_s_ch_t)


nw_se_s_d_t <- sqrt(diag(sandwich::NeweyWest(model_s_d, lag = 6)))
nw_se_s_n_t <- sqrt(diag(sandwich::NeweyWest(model_s_n, lag = 6)))
nw_se_s_ta_t <- sqrt(diag(sandwich::NeweyWest(model_s_ta, lag = 6)))
nw_se_s_tr_t <- sqrt(diag(sandwich::NeweyWest(model_s_tr, lag = 6)))
nw_se_s_china_t <- sqrt(diag(sandwich::NeweyWest(model_s_ch, lag = 6)))


robust_s_d_t <- 2 * (1-pt(abs(coef(model_s_d) / nw_se_s_d_t), df = df.residual(model_s_d)))
robust_s_n_t <- 2 * (1-pt(abs(coef(model_s_n) / nw_se_s_n_t), df = df.residual(model_s_n)))
robust_s_ta_t <- 2 * (1-pt(abs(coef(model_s_ta) / nw_se_s_ta_t), df = df.residual(model_s_ta)))
robust_s_tr_t <- 2 * (1-pt(abs(coef(model_s_tr) / nw_se_s_tr_t), df = df.residual(model_s_tr)))
robust_s_ch_t <- 2 * (1-pt(abs(coef(model_s_ch) / nw_se_s_china_t), df = df.residual(model_s_ch)))

nw_se_s_d_t       <- nw_se_s_d_t[names(coef(model_s_d))]
robust_s_d_t      <- robust_s_d_t[names(coef(model_s_d))]
nw_se_s_n_t      <- nw_se_s_n_t[names(coef(model_s_n))]
robust_s_n_t     <- robust_s_n_t[names(coef(model_s_n))]
nw_se_s_ta_t      <- nw_se_s_ta_t[names(coef(model_s_ta))]
robust_s_ta_t     <- robust_s_ta_t[names(coef(model_s_ta))]
nw_se_s_tr_t     <- nw_se_s_tr_t[names(coef(model_s_tr))]
robust_s_tr_t     <- robust_s_tr_t[names(coef(model_s_tr))]
nw_se_s_china_t  <- nw_se_s_china_t[names(coef(model_s_ch))]
robust_s_ch_t <- robust_s_ch_t[names(coef(model_s_ch))]

# lists of model, robust SE  and p-values 
models_list_s <- list(model_s_d, model_s_n, model_s_ta, model_s_tr, model_s_ch)
robust_ses_s <- list(nw_se_s_d_t, nw_se_s_n_t, nw_se_s_ta_t, nw_se_s_tr_t, nw_se_s_china_t)
robust_pvals_s <- list(robust_s_d_t, robust_s_n_t, robust_s_ta_t, robust_s_tr_t, robust_s_ch_t)

tmpfile_s <- tempfile(fileext = ".html")  # create temp file


# Generate table
table_texreg_s <- htmlreg(
  l = models_list_s,
  override.se = robust_ses_s,
  override.pvalues = robust_pvals_s,
  custom.model.names = c("TweetDummy", "TweetCount", "Tariff", "Trade", "China"),
  custom.coef.map = custom_names,
  caption = "Second-Term VAR Models of Average Hourly Volatility",
  label = "tab:VAR_Second_Term",
  caption.above = TRUE,
  digits = 6,
  custom.gof.rows = list("Shock (IRF)" = c(0.014659, 0.013315, 0.010752, -0.005665, 0.013937)),
  star.cutoffs = c(0.05, 0.01, 0.001),
  custom.note = "\\parbox[t]{0.9\\textwidth}{\\small\\textit{This table displays VAR regression with only two variables : AHV and X regressor. The column names represent the X variable for the selected model.}}",
  file = tmpfile_s
)

 
# Print
cat(readLines(tmpfile_s), sep = "\n")
```

<!--chapter:end:07-appendix.Rmd-->

# Cross-references {#cross}

Cross-references make it easier for your readers to find and link to elements in your book.

## Chapters and sub-chapters

There are two steps to cross-reference any heading:

1. Label the heading: `# Hello world {#nice-label}`. 
    - Leave the label off if you like the automated heading generated based on your heading title: for example, `# Hello world` = `# Hello world {#hello-world}`.
    - To label an un-numbered heading, use: `# Hello world {-#nice-label}` or `{# Hello world .unnumbered}`.

1. Next, reference the labeled heading anywhere in the text using `\@ref(nice-label)`; for example, please see Chapter \@ref(cross). 
    - If you prefer text as the link instead of a numbered reference use: [any text you want can go here](#cross).

## Captioned figures and tables

Figures and tables *with captions* can also be cross-referenced from elsewhere in your book using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively.

See Figure \@ref(fig:nice-fig).

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Plot with connected points showing that vapor pressure of mercury increases exponentially as temperature increases.'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Don't miss Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(pressure, 10), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

<!--chapter:end:08-test.Rmd-->

