---
title: "Terminally Online: Does Donald Trump impact Financial Markets"
author: "Marcos Constantinou, Ryan Fellarhi & Jonas Bruno"
date: "Last edited: 20.05.2025"
site: bookdown::bookdown_site
documentclass: book
bibliography: [packages.bib, citations.bib] #name bib files
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg -> Do Later
description: |
  This is our website for a Universityproject. We put this together from a basic bookdown example so there might be some stuff in here from that.
link-citations: true           # clickable citations
csl: apa.csl
github-repo: www.github.com/theshufflebee/mmetricsproject
---


```{r r library_setup:index, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE}
#rm(list=ls())
require(here)
require(stringr)
require(dplyr)
require(ggplot2)
require(lubridate)
require(RColorBrewer)
require(wordcloud)
require(tidyr)
require(tidytext)
require(textstem)
require(tidyverse)
require(tm)
require(SnowballC)
require(quanteda.textplots)
require(quantmod)
require(alphavantager)
require(quanteda)
require(rvest)
require(httr)
require(xml2)
require(textdata)
require(sentimentr)
require(syuzhet)
require(text)
library(wordcloud2)


#load datasets
tweets <- read.csv(here("data/political_data/tweets.csv"))
truths <- read.csv(here("data/political_data/truths250510.csv"))

source(here("helperfunctions/truths_cleaning_function.R"))
```



```{r data_title, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
#Only keep original Tweets
tweets_clean <- tweets %>% filter(isRetweet != "t")
tweets_clean <- tweets_clean$text
truths_clean <- truths_processer(truths)

truths_clean2 <- truths_clean$post

full_posts <- c(tweets_clean, truths_clean2)

tokens <- tokens(full_posts)
dfm <- dfm(tokens)
corpus_tweets <- corpus(tweets$text)

additional_words <- c("rt")
all_stopwords <- c(stopwords("en"), additional_words)

tokens_tweets <- tokens(corpus_tweets,  
                        remove_punct = TRUE, 
                        remove_numbers = TRUE)

tokens_tweets <- tokens_remove(tokens_tweets, all_stopwords)  # Remove stopwords

dfm_tweets <- dfm(tokens_tweets)
```



```{r wordcould, cache=FALSE, echo=FALSE}
print(textplot_wordcloud(
  dfm_tweets,
  min_count = 200,
  color = RColorBrewer::brewer.pal(8, "Reds")
))
```


# Readme {.unnumbered}

This is a Website for a class project from the 2025 Class Macroeconometrics at Université de Lausanne. It is intended as an archive and Display of our Project. You can find all data for recreation on www.github.com/theshufflebee/mmetricsproject.

# Abstract {.unnumbered}

This Project assesses to what extent Financial Markets react to information provided by Donald Trump on Twitter and Truth Social. We asses the impact of posts on hourly volatility using ARMA-X. We evaluate multiple time horizons and independent variables, such as if Trump posts anything, specific words such as tariff and sentiments. We then calculate IRFs and show that there are significant impacts on volatility.


<!--chapter:end:index.Rmd-->

---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r, warning=FALSE, echo=FALSE, cache=TRUE}
require("here")
require("stringr")
require("dplyr")
require("ggplot2")
require("lubridate")

truths_raw <- read.csv(here("data/mothership", "social.csv"))

truths <- truths_raw %>%
  mutate(
    # Use POSIX 'timestamp' directly
    date_time_parsed = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S"),
    
    # Extract date only for plot
    day = as.Date(date_time_parsed),
    
    # Extract time only for plot
    time = format(date_time_parsed, "%H:%M"),
    
    # Convert time to numeric hours & minutes as fractions
    time_numeric = hour(date_time_parsed) + minute(date_time_parsed) / 60,
    
    # Shift time such that y = 0 corresponds to 12 PM
    time_shifted = time_numeric - 12
  )

```

# Introduction

## Motivation

Over the past 15 years social media has become an important
communication tool for politicians. One of the pioneers of this novel
approach has been Donald Trump, the 45th and 47th President of the United
States. Since his ban on Twitter after the January 6th riots, his quantity of
social media posts has drastically increased. This is shown in the following
figure. [^1]

[^1]: Includes both Posts and Reposts

The content of his posts can sometimes have announcements or teases of future
political decisions. Note the recent infamous "THIS IS A GREAT TIME TO BUY!!! DJT"
post sent just an hour before lifting his reciprocal tariffs. It is then not 
improbable that agents in financial markets might take this information into 
account in their decision making. This question has been asked before in the 
literature, focusing rather on his first term. 

This brings us to our research question:  Do Donald Trumps Posts impact market Volatility?

    
\@ref(fig:fig1)

```{r fig1, echo=FALSE, fig.cap="Terminally Online: Trump's Twitter & Truth Social Posts (EDT)", warning=FALSE, cache=TRUE}
 #Create the scatter plot
ggplot(truths, aes(x = day, y = time_shifted)) +
  geom_point(alpha = 0.5, color = "blue", size = 0.55) +  # Transparancy to create "heatmap"
  scale_y_continuous(
    breaks = seq(-12, 12, by = 3),  # Custom Y scale
    labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
  ) +
  labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
       x = "",
       y = "Time of Day") +
  theme_minimal() +
  
  
  # Customize X Axis
  scale_x_date(
    date_labels = "%b %Y",  # Format labels to show month and year
    date_breaks = "9 months"
  ) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Add vertical lines at 9:30 AM and 4:00 PM for stock market
  geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") + 
  geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +   
  
  # theme adjustments
  theme(
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```



## Literature Review

Information is one of the most valuable assets in the financial market.
Its importance lies at the core of the "Efficient Market Hypothesis", 
which states that the prices of assets fully reflect all
available information, adjusting immediately to any new data
@famaAdjustmentStockPrices2003 , and thereby creating a strong demand
for information flow. In addition, the “Mixture of Distribution
Hypothesis” states that the release of new information is closely linked
to movements in both realized and implied volatility
@andersenReturnVolatilityTrading1996 (1)(3).

Consequently, a large part of the literature had focused on the relation
between announcements, news and market activity. For example,
@schumakerTextualAnalysisStock2009 use various linguistic and textual
representations derived from financial news to predict stock market
prices. Similarly, @ederingtonHowMarketsProcess1993 analyze the impact
of macroeconomic news announcements on interest rate and foreign
exchange futures markets, particularly in terms of price changes and
volatility. Both studies, among others, find that prices— such as stock
prices—react primarily within minutes after the release of new
information.

Recently, the world has witnessed the rise of the Internet
which revolutionized the dissemination and accessibility of information.
Social media enable investors, analysts or politicians to instantly
share their information, news or opinions. This led some studies to
focus on the communication dynamics of social platform to predict
changes in the returns of financial assets @dechoudhuryCanBlogCommunication2008 & 
@bartovCanTwitterHelp2018. In this context, the
impact of Trump’s tweets on various financial and macroeconomic
variables has been analysed by several studies, especially during his
first mandate.

Using high-frequency financial data,
@gjerstadPresidentTrumpsTweets2021 found an increase in uncertainty and
trading volume, along with a decline in the U.S. stock market—regardless
of the tweet's content. However, the effect was stronger when Trump used
confrontational words such as "tariff" or "trade war." Some of his
announcements also influenced the U.S. dollar exchange rate (l) and
certain market indices within minutes of the tweet being posted
@colonescuEffectsDonaldTrumps2018 & @kinyuaAnalysisImpactPresident2021.

Other scholars have shown that negative Trump tweets about specific
companies tended to reduce demand for their stocks @bransHisThumbEffect2020 &
@mendelsStanfordResearchSeries2019, whereas some
other have shown that they also impact market volatility indices such as
the VIX @fendelPoliticalNewsStock2019 or the Volfele @klausMeasuringTrumpVolfefe2021.
The effects of his tweets also extended beyond the U.S.. For example,
@nishimuraImpactsDonaldTrumps2025 shows a
positive relationship between volatility in European stock markets and
tweeter activity of Trump, and this effect tends to intensify as public
intention for his tweet grows @nishimuraImpactsDonaldTrumps2025.

Our paper is built as follows: Section 2 describes the data, their sources, and 
the several transformations applied. Section 3 focuses on ARMA-X models, describing 
both our methodology and results. Section 4 does the same, though for our VAR models. 
Section 5 concludes.

<!--chapter:end:01-introduction.Rmd-->

---
bibliography: citations.bib
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)

getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

# Data

## Financial Data

For our financial data, we decided to try to find minute-by-minute prices for 
broad market indices. While the actual indices do not update their prices so often,
we had to take proxies under the form of ETF's that track them. Our 3 markets of
analysis are: SPY to track the S&P500, VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage. Our timeframe is from the first 
of January 2014 to the 7th of May 2025.


We then had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is realised market volatility. We did so 
with the following formula:
$$
\begin{aligned}
  v_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close) and $i$ represents
every minute.

We used a custom function in order to get the AHV for each open market hour. Note 
that the first hour is from 9:30 am to 10:00 am since the 
<<<<<<< HEAD
market open on a half-hour but closes at 4:00 pm. We can plot this data.
=======
market open on a half-hour but closes at 4:00 pm. We can plot this data in
the following table:
>>>>>>> 249c30f04f33d9c802ee965feef5b2f53edab0bc


```{r fin plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

price_plotter(SPY, title = "SPY Prices")

hvol_plotter(SPY, breaks = "yearly", title = "SPY Realised Volatility")

```

We can clearly see that the last few months show a new era of never seen before
levels of volatility. Shocks on volatility recently have reached, and even surpassed
(for a few data points) levels seen during the COVID-19 pandemic.

## Political Data

We have two sources for Trump's posts. The Tweets are from Kaggle
@shantanuDonaldTrumpTweets and go until the 8th of January 2021. Since he
switched his primary posting platform to Truth Social we use only that
Data from 2021 onwards. All Truth Social posts were scrapped from
trumpstruth.org, a webpage that aims to conserve all his posts. Note that we have 
had to use web-scrapping methods in order to download all his Truth Social posts
in a dataset.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove a lot of observations. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as an assumption.^[2] 

Since our financial data is hourly, we aggregate the social data by hour. We 
then construct multiple variables from the social media data. These include
a dummy for whether there was a post, the number of posts an hour and counts
for certain words ("tariffs","trade","china"). Further we applied some simple 
sentiment analysis algorithms on the data to see if there are certain sentiments 
in his tweets that move the markets. Details on all our data management procedures
can be found in the GitHub repository.

```{r social plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

#find count
tweetcount_alltime = dplyr::select(data,timestamp,N)
#select time period
tweetcount = filter(tweetcount_alltime,
between(timestamp,
as.Date('2014-01-01'),
as.Date('2025-04-10')))
#plot
ggplot(tweetcount_alltime, aes(x = timestamp, y = N)) +
geom_point(color = "#253494", size = 1) +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
labs(title = "Trump Social Media Count",
x = NULL,
y = "number of tweets/truths") +
theme_minimal(base_size = 14) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(face = "bold", hjust = 0.5))

```


## Final Dataframe

```{r data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

tail(data[1:4])

tail(data[5:9])

tail(data[10:14])

tail(data[15:19])

```





¨
^[2]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 

<!--chapter:end:02-data.Rmd-->

# ARMA-X

## Methodology

Once we have our final dataframe, we could then finally start on some analysis.
We first thought of a simple ARMA-X type specification, taking the AHV as our
"y variable" and taking any of the social media variables as the exogenous
regressors. The assumption here is that, while the market reacts to Trump posts,
Trump's posts are chaotic, nonsensical, and random enough to be considered 
exogenous. 

We of course first start by checking stationarity of our variables (ADF), where we find
p-values of 0.01 suggesting that the processes are not explosive. Then, we use 
a custom function in order to choose the number of lags based on the AIC criterion.
This however, while often choose a very high number of lags, which could be 
explained by our data being hourly. As such we decided to put a limit of 3 lags,
which sees minimal AIC loss and simplifying our models considerably.

## Results


### Full Timeframe
We run models with the following exogenous regressors: $TweetDummy$, $TweetCount$,
and the mentions of words $Tariff$, $Trade$, and $China$. We first note on the table
 in section \@ref(sec:spy-table) that all the x-regressors are significant,
apart from trade. Notice also that all the coefficients (apart from $Tariff_{t-3}$)
are positive, in line with our main hypothesis. The effect of $Tariff_{t-1}$ and 
$Tariff_{t-2}$ are especially large, given the usual size of the volatility as seen
in Section \@ref(sec:means-table). We in fact predict that an 
extra mention of tariffs one hour ago, leads to a whopping extra 0.02 in volatility
 which is just about the average size for the full timeframe. We can see the
<<<<<<< HEAD
impulse response function (IRF) for this shock in Section \@ref(sec:SPY-IRF).
Notice that there is a large response in the first periods, and then a graduate
decline over time. Something to note is that in our analysises of IRF's, when including
=======
impulse response function (IRF) for this shock, in <span style="color:red"> *IRFtarif* </span>.
\@ref(sec:SPY-IRF) Notice that there is a large response in the first periods, and then a graduate
decline over time. Something to note is that in our analyses of IRF's, when including
>>>>>>> 249c30f04f33d9c802ee965feef5b2f53edab0bc
MA terms, the decline shows up gradual while being much sharper when only including
AR terms. 
Note that we ran all these models on the VGK and ASHR ETF's as well, though no
significant results appear apart from a small but statistically significant effect
of the tariff variable for VGK.

### Split Samples
<<<<<<< HEAD
We then split our sample for the first and second term of the Trump presidancy.
We only run models on tariff, trade and china this time. As seen on the table in
Section \@ref(sec:spy-table-terms), the first interesting result
=======
We then split our sample for the first and second term of the Trump presidency.
We only run models on tariff, trade and china this time. As seen on table
<span style="color:red"> *ARMAX Table 2* </span> \@ref(sec:spy-table-terms), the first interesting result
>>>>>>> 249c30f04f33d9c802ee965feef5b2f53edab0bc
is in the coefficients of tariff being significant and very large in the second
term, while being small and not statistically significant in the first. A similar
story goes for the China variable. This may lend some evidence to support the 
claim that investors are much more reactive to Trump's social media presence
now than before. We've found similar IRF's as for the full timeframe. Tables \@ref(sec:SPY-IRF)
show the IRF's for the second term, of the impacts of tariff and china mention
shocks on the AHV.
Finally, we can check the residuals of all these models to test them somewhat. 
In Section \@ref(sec:SPY-res-test), the pvalues being zero
for the full timeframe and first term indicate that there is autocorrelation in 
the residuals, thus suggesting that these estimations have problems. Note however,
that the p-values for the second term are quite high, lending support to our
models on the split sample. These results suggest that perhaps ARMA-X models are
not right in this context as it is not unreasonable to think that Trump does 
in fact react to market movements, which would break the exogeneity assumption
that is critical for this type of model. With this information, we decided to run
a VAR model to deepen our understanding of these variables.



<!--chapter:end:03-armax.Rmd-->

# VAR 


## Methodology


## Results



### Full Timeframe



### Split Samples

<!--chapter:end:04-var.Rmd-->

# Conclusion

We started this project with the intention of understanding whether the impact 
of Trump's social media posts affect financial markets, and to see if there
is perhaps a difference from his first presidential mandate. After various 
headaches with our data, we first ran ARMA-X models where we found significant
and positive results albeit with strong auto-correlation in the errors, with only
the second term analysis offering more convincing results. We then try VAR models
for a possibly more accurate picture, albeit with little to no success. We once
again find strong auto-correlation in the errors, which we here fix by using 
Newey-West standard errors. We found that the only significant coefficients are
actually negative, suggesting Trump's social media presence would reduce volatility.

However, we would suggest strong against trying to interpret these results given
that the models seem to not fit particularly well. This may be due to seasonality
in our data (a common trend seen is our daily AVH is high volatility in the first 
open hours, and a gradual slowdown for the rest of the day), or to our handling 
of non-market hours. Further work could look at these issues in more detail, could
further complicate the models by adding more variables and interaction between them,
and/or additionally <span style="color:red"> *use more sophisticated models, such as models including co-integration.* </span>

<!--chapter:end:05-conclusion.Rmd-->

# Bibliography



`r if (knitr::is_html_output()) '
## References {-}
'`

<!--chapter:end:06-bibliography.Rmd-->

# Appendix


```{r library_setup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(kableExtra)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

## ARMAX

We choose the specification in the armax_models file. In this file, we will
just run said specifications to produce nice tables and graphs to include in 
our final paper. This is also why there are specification differences in the 
separate timeframes. We always use the best fit we found earlier.

```{r datasetup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#backup
backup = data

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#for interpretation
mean1 = mean(data$SPY_vol)

```


```{r fitting models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#All SPY models for the full Dataframe

models <- list()

# ARMA-X(3,3,1) with Tweet Dummy as Exogenous
models[["Model 1"]] <- armax(data$SPY_vol, xreg = data$dummy, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,3,1) with Tweet Count as Exogenous
models[["Model 2"]] <- armax(data$SPY_vol, xreg = data$N, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Model 3"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 3, p = 3, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Model 4"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 1, p = 3, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Model 5"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 2) 

```

### SPY ARMAX Table (Jan 2014 - May 2025) {#sec:spy-table}
```{r armaxfull, results="asis", warning=FALSE, echo=FALSE, message=FALSE}
library(texreg)

# Define variable display names (HTML-friendly using <sub>)
names <- list(
  "ar1" = "AR(1)",
  "ar2" = "AR(2)",
  "ar3" = "AR(3)",
  "ma1" = "MA(1)",
  "ma2" = "MA(2)",
  "ma3" = "MA(3)",
  "(Intercept)" = "Constant",
  "dummy_lag_0" = "TweetDummy<sub>t</sub>",
  "dummy_lag_1" = "TweetDummy<sub>t-1</sub>",
  "N_lag_0" = "TweetCount<sub>t</sub>",
  "N_lag_1" = "TweetCount<sub>t-1</sub>",
  "tariff_lag_0" = "Tariff<sub>t</sub>",
  "tariff_lag_1" = "Tariff<sub>t-1</sub>",
  "tariff_lag_2" = "Tariff<sub>t-2</sub>",
  "tariff_lag_3" = "Tariff<sub>t-3</sub>",
  "trade_lag_0" = "Trade<sub>t</sub>",
  "trade_lag_1" = "Trade<sub>t-1</sub>",
  "china_lag_0" = "China<sub>t</sub>"
)

# Output the HTML table directly
cat(
  htmlreg(
    models,
    custom.model.names = names(models),
    custom.coef.map = names,
    caption = "ARMAX Models of Average Hourly Volatility",
    caption.above = TRUE,
    digits = 4,
    doctype = FALSE
  ))

```


### SPY ARMAX IRFs (Jan 2014 - May 2025){#sec:SPY-IRF}

```{r SPYirf, results="asis", warning=F, message=F, echo=FALSE, cache=TRUE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

#irf.plot(models[["Model 1"]],nb.periods,title="Tweet Dummy Shock")
#irf.plot(models[["Model 2"]],nb.periods,title="Tweet Count Shock")
plot1 = irf.plot(models[["Model 3"]],nb.periods,
                 title="Tariff Mention Shock - Jan 2014 - May 2025")
plot1
#irf.plot(models[["Model 4"]],nb.periods,title="Trade Mention Shock")
#irf.plot(models[["Model 5"]],nb.periods,title="China Mention Shock")

ggsave("armax_plot1.png",plot=plot1,bg="white")

```


```{r, warning=F, message=F, echo=FALSE, results=F}
#Calculate residuals

res1 = checkresiduals(models[["Model 1"]], plot = FALSE)
res2 = checkresiduals(models[["Model 2"]], plot = FALSE)
res3 = checkresiduals(models[["Model 3"]], plot = FALSE)
res4 = checkresiduals(models[["Model 4"]], plot = FALSE)
res5 = checkresiduals(models[["Model 5"]], plot = FALSE)

```

```{r SPYres, warning=F, message=F, echo=FALSE, cache=TRUE}

resnames = c("Twitter Dummy", "Twitter Count", "Tariff", "Trade", "China")

#extract p-values directly from checkresiduals results
pvals <- data.frame("X-Regressor" = resnames,
                    `Full Timeframe` = c(
                      res1$p.value,
                      res2$p.value,
                      res3$p.value,
                      res4$p.value,
                      res5$p.value))


```




```{r datasetup first, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#First term Calculations

#load final dataset
data = backup

#first term
data = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#for interpretation
mean2 = mean(data$SPY_vol)

```


```{r 1st term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#SPY Models first Term

models <- list()

# ARMA-X(3,3,0) with Tariff Mentions as Exogenous
models[["First Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 3, q = 3)

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

```


```{r 1st SPYresiduals, warning=F, message=F, results=F, echo=FALSE, cache=TRUE}
# Run first term residuals
res6 = checkresiduals(models[["First Term (1)"]], plot = FALSE)
res7 = checkresiduals(models[["First Term (2)"]], plot = FALSE)
res8 = checkresiduals(models[["First Term (3)"]], plot = FALSE)

pvals_new1 <- data.frame(
  "First-Term" = c(
    NA,
    NA,
    res6$p.value,
    res7$p.value,
    res8$p.value))

```




```{r datasetup second, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#Second Term Calculations

#load final dataset
data = backup

#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

#for interpretation
mean3 = mean(data$SPY_vol)

```


```{r 2nd term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#Run Second Term Models

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 1, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

```



```{r 2nd SPYresiduals, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
# Calculate SPY residuals for the second term

res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)

pvals_new2 <- data.frame(
  "Second-Term" = c(
    NA,
    NA,
    res9$p.value,
    res10$p.value,
    res11$p.value))

#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)

```


### SPY ARMAX Table (Split Presidential Terms){#sec:spy-table-terms}

\centering

```{r armax1, results="asis", warning=F, echo=FALSE, message=F}

library(texreg)

# Change model names (not sure if needed anymore)
model_names <- c(
 "First Term (1)", "First Term (2)", "First Term (3)",
 "Second Term (1)", "Second Term (2)", "Second Term (3)"
)
names(models) <- model_names

# HTML coefficients names adjusted (is this correct?)
xnames_ordered <- c(
  "AR(1)", "AR(2)", "AR(3)",
  "MA(1)", "MA(2)", "MA(3)",
  "Constant",
  "Tariff<sub>t</sub>", "Tariff<sub>t-1</sub>", "Tariff<sub>t-2</sub>",
  "Trade<sub>t</sub>",
  "China<sub>t</sub>", "China<sub>t-1</sub>", "China<sub>t-2</sub>"
)

# Render as html -> need to render file to have it nice
htmlreg(
  models,
  custom.coef.names = xnames_ordered,
  custom.model.names = model_names,
  caption = "Split-Term ARMAX Models of Average Hourly Volatility",
  caption.above = TRUE,
  digits = 4,
  stars = c(0.001, 0.01, 0.05),
  doctype = FALSE)


```


### SPY ARMAX IRFs (Split Terms){#sec:SPY-SPLIT-IRF}

```{r 2nd SPYirf, warning=F, message=F, echo=FALSE, cache=TRUE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

plot2 = irf.plot(models[["Second Term (1)"]],nb.periods,
                 title="Tariff Mention Shock - Second Term")
plot2

#ggsave("armax_plot2.png",plot=plot2,bg="white")

plot3 = irf.plot(models[["Second Term (3)"]],nb.periods,
                 title="China Mention Shock - Second Term")
plot3

#ggsave("armax_plot3.png",plot=plot3,bg="white")

```


### Residual Test {#sec:SPY-res-test}

```{r res-table, message=F, warning=F, results="asis", echo=FALSE, cache=TRUE}

kable(pvals_combined, digits = 6, format="html", caption = "Ljung-Box Test p-values for Residuals") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```

### Descriptive Stats{#sec:means-table}
 
```{r means, message=F, warning=F, results="asis"}

means <- data.frame(
  Model = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
  `SPY Volatility Mean` = c(
    mean1,
    mean2,
    mean3))

kable(means, digits = 6, format="html", caption = "Summary Statistics of SPY Volatility") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```



<!--chapter:end:07-appendix.Rmd-->

# Cross-references {#cross}

Cross-references make it easier for your readers to find and link to elements in your book.

## Chapters and sub-chapters

There are two steps to cross-reference any heading:

1. Label the heading: `# Hello world {#nice-label}`. 
    - Leave the label off if you like the automated heading generated based on your heading title: for example, `# Hello world` = `# Hello world {#hello-world}`.
    - To label an un-numbered heading, use: `# Hello world {-#nice-label}` or `{# Hello world .unnumbered}`.

1. Next, reference the labeled heading anywhere in the text using `\@ref(nice-label)`; for example, please see Chapter \@ref(cross). 
    - If you prefer text as the link instead of a numbered reference use: [any text you want can go here](#cross).

## Captioned figures and tables

Figures and tables *with captions* can also be cross-referenced from elsewhere in your book using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively.

See Figure \@ref(fig:nice-fig).

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Plot with connected points showing that vapor pressure of mercury increases exponentially as temperature increases.'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Don't miss Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(pressure, 10), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

<!--chapter:end:08-test.Rmd-->

