---
title: "Terminally Online: Does Donald Trump impact Financial Markets"
author: "Marcos Constantinou, Ryan Fellarhi & Jonas Bruno"
date: "Last edited: 12.05.2025"
site: bookdown::bookdown_site
documentclass: book
bibliography: [packages.bib, macroeconometrics_citations.bib] #name bib files
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg -> Do Later
description: |
  This is our website for a Universityproject. We put this together from a basic bookdown example so there might be some stuff in here from that.
link-citations: true           # optional but helpful for clickable citations
csl: apa.csl
github-repo: theshufflebee/mmetricsproject
---

# Readme {.unnumbered}

This is a Website for a class project from the 2025 Class Macroeconometrics at Université de Lausanne.

IMPORTANT:use: bookdown::render_book("final/index.Rmd") to render site

## Usage {.unnumbered}

You can find all sections on the left. There is the Main Report which we hand in. This serves as a complete collection of the whole project to make sure everything is available to readers.

# Abstract {.unnumbered}

This Project assesses to what extent Financial Markets react to information provided by Donald Trump on Twitter and Truth Social. We asses the impact of posts on hourly volatility using ARMA-X. We evaluate multiple time horizons and independent variables, such as if Trump posts anything, specific words such as tariff and sentiments. We then calculate IRFs and show that there are significant impacts on volatility.

<!--chapter:end:index.Rmd-->

---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r, warning=FALSE, echo=FALSE, cache=TRUE}
require("here")
require("stringr")
require("dplyr")
require("ggplot2")
require("lubridate")

truths_raw <- read.csv(here("data/mothership", "social.csv"))

truths <- truths_raw %>%
  mutate(
    # Use POSIX 'timestamp' directly
    date_time_parsed = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S"),
    
    # Extract date only for plot
    day = as.Date(date_time_parsed),
    
    # Extract time only for plot
    time = format(date_time_parsed, "%H:%M"),
    
    # Convert time to numeric hours & minutes as fractions
    time_numeric = hour(date_time_parsed) + minute(date_time_parsed) / 60,
    
    # Shift time such that y = 0 corresponds to 12 PM
    time_shifted = time_numeric - 12
  )

```

# Introduction

Over the past 15 years social media has become an important
communication tool for politicians. One of the pioneers of this novel
approach has been Donald Trump, the 45th and 47th President of the United
States. Since his ban on Twitter after the January 6th riots, his quantity of
social media posts has drastically increased. This is shown in the following
figure. [^1]

[^1]: Includes both Posts and Reposts

The content of his posts can sometimes have announcements or teases of future
political decisions. Note the recent infamous "THIS IS A GREAT TIME TO BUY!!! DJT"
post sent just an hour before lifting his reciprocal tariffs. It is then not 
improbable that agents in financial markets might take this information into 
account in their decision making. This question has been asked before in the 
literature, focusing rather on his first term. 

This brings us to our research question:  Do Donald Trumps Posts impact market Volatility?

    
\@ref(fig:fig1)

```{r fig1, warning=FALSE, echo=FALSE, cache=TRUE, fig.cap="Terminally Online: Trump's Twitter & Truth Social Posts (EDT)"}
 #Create the scatter plot
ggplot(truths, aes(x = day, y = time_shifted)) +
  geom_point(alpha = 0.5, color = "blue", shape = ".") +  # Transparancy to create "heatmap"
  scale_y_continuous(
    breaks = seq(-12, 12, by = 3),  # Custom Y scale
    labels = c("00:00", "03:00", "06:00", "09:00", "12:00", "15:00", "18:00", "21:00", "24:00")  # 24-hour format labels
  ) +
  labs(title = "Terminally Online: Trumps Twitter & Truth Social Posts (EDT)",
       x = "Date",
       y = "Time of Day") +
  theme_minimal() +
  
  
  # Customize X Axis
  scale_x_date(
    date_labels = "%b %Y",  # Format labels to show month and year
    date_breaks = "9 months"
  ) +
  
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Add vertical lines at 9:30 AM and 4:00 PM for stock market
  geom_hline(yintercept = (9 + 30 / 60) - 12, linetype = "longdash", color = "red") + 
  geom_hline(yintercept = 16 - 12, linetype = "dashed", color = "red") +   
  
  # theme adjustments
  theme(
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.grid.major = element_line(linewidth = 0.5),  # Major gridlines
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```



## Literature Review

Information is one of the most valuable assets in the financial market.
Its importance lies at the core of the "Efficient Market Hypothesis", 
which states that the prices of assets fully reflect all
available information, adjusting immediately to any new data
@famaAdjustmentStockPrices2003 , and thereby creating a strong demand
for information flow. In addition, the “Mixture of Distribution
Hypothesis” states that the release of new information is closely linked
to movements in both realized and implied volatility (1)(3)(7).

Consequently, a large part of the literature had focused on the relation
between announcements, news and market activity. For example,
@schumakerTextualAnalysisStock2009 use various linguistic and textual
representations derived from financial news to predict stock market
prices. Similarly, @ederingtonHowMarketsProcess1993 analyze the impact
of macroeconomic news announcements on interest rate and foreign
exchange futures markets, particularly in terms of price changes and
volatility. Both studies, among others, find that prices— such as stock
prices—react primarily within minutes after the release of new
information.

Recently, the world has witnessed the rise of the Internet
which revolutionized the dissemination and accessibility of information.
Social media enable investors, analysts or politicians to instantly
share their information, news or opinions. This led some studies to
focus on the communication dynamics of social platform to predict
changes in the returns of financial assets (6)(8). In this context, the
impact of Trump’s tweets on various financial and macroeconomic
variables has been analysed by several studies, especially during his
first mandate.

Using high-frequency financial data,
@gjerstadPresidentTrumpsTweets2021 found an increase in uncertainty and
trading volume, along with a decline in the U.S. stock market—regardless
of the tweet's content. However, the effect was stronger when Trump used
confrontational words such as "tariff" or "trade war." Some of his
announcements also influenced the U.S. dollar exchange rate (l) and
certain market indices within minutes of the tweet being posted (r)(a).

Other scholars have shown that negative Trump tweets about specific
companies tended to reduce demand for their stocks (b)(g), whereas some
other have shown that they also impact market volatility indices such as
the VIX (w) or the Volfele(v). The effects of his tweets also extended
beyond the U.S.. For example, @nishimuraImpactsDonaldTrumps2025 shows a
positive relationship between volatility in European stock markets and
tweeter activity of Trump, and this effect tends to intensify as public
intention for his tweet grows (z).

Our paper is built as follows: 

<!--chapter:end:01-introduction.Rmd-->

---
bibliography: macroeconometrics_citations.bib  #
csl: apa.csl
link-citations: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r library_setup_data, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(bookdown)

getwd()
#setwd("X:/Onedrive/Desktop/Macroeconometrics/R stuff/Project/mmetricsproject/final") 

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#load initial financial for plots
SPY <- read.csv(here("data/mothership", "SPY.csv"))
SPY$timestamp = as.POSIXct(SPY$timestamp,format = "%Y-%m-%d %H:%M:%S")
SPY = filter(SPY,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```

# Data

## Financial Data

For our financial data, we decided to try to find minute-by-minute prices for 
broad market indices. While the actual indices do not update their prices so often,
we had to take proxies under the form of ETF's that track them. Our 3 markets of
analysis are: SPY to track the S&P500, VGK to track the FTSE Developed Europe 
All Cap Index, and finally ASHR to track the CSI 300 China. We accessed this data
through a free stock API, Alpha Vantage. Our timeframe is from the first 
of January 2014 to the 7th of May 2025.


We then had to transform this data to get our main variable of interest, Average
Hourly Volatility (AHV). Note that this is realised market volatility. We did so 
with the following formula:
$$
\begin{aligned}
  v_t = \frac{1}{N}&\sum_{i=1}^N(\Delta p_{t,i})^2 
\end{aligned}
$$
Where $\Delta p_t$ is the difference in price (open - close) and $i$ represents
every minute.

We used a custom function in order to get the AHV for each open market hour. Note 
that the first hour is from 9:30 am to 10:00 am since the 
market open on a half-hour but closes at 4:00 pm. We can plot this data in
<span style="color:red"> *figX* </span>.


```{r fin plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

price_plotter(SPY, title = "SPY Prices")

hvol_plotter(SPY, breaks = "yearly", title = "SPY Realised Volatility")

```



## Political Data

We have two sources for Trump's posts. The Tweets are from Kaggle
@DonaldTrumpTweets and go until the 8th of January 2021. Since he
switched his primary Posting platform to Truth Social we use only that
Data from 2021 onwards. All Truth Social Posts were scrapped from
trumpstruth.org, a webpage that aims to conserve all his posts. You can
find the dataset, and webscrapper, and Data cleaning process in the
Appendix.

Since we're using financial data that is constrained by trading hours,
we decided to move posts after 16:00 to the next trading day's opening
hour.

A big problem we had in our analysis was what to do with social media posts
which appeared outside market hours. We first decided to simply ignore them, but 
it turned out to remove a lot of observations. We finally decided to push all the 
social media information outside market hours to the next open hour. This comes 
as an assumption.^[2] 

Since our financial data is hourly, we aggregate the social data by hour. We 
then construct multiple variables from the social media data. These include
a dummy for whether there was a post, the number of posts an hour and counts
for certain words ("tariffs","trade","china"). Further we applied some simple 
sentiment analysis algorithms on the data to see if there are certain sentiments 
in his tweets that move the markets. More information and detailed process are in 
the appendix and online.

<span style="color:red"> *insert wordcloud* </span>.

```{r social plots, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

#find count
tweetcount_alltime = dplyr::select(data,timestamp,N)
#select time period
tweetcount = filter(tweetcount_alltime,
between(timestamp,
as.Date('2014-01-01'),
as.Date('2025-04-10')))
#plot
ggplot(tweetcount_alltime, aes(x = timestamp, y = N)) +
geom_point(color = "#253494", size = 1) +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "9 month") +
labs(title = "Trump Social Media Count",
x = NULL,
y = "number of tweets/truths") +
theme_minimal(base_size = 14) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(face = "bold", hjust = 0.5))

```


## Final Dataframe

```{r data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}

tail(data[1:4])

tail(data[5:9])

tail(data[10:14])

tail(data[15:19])

```





¨
^[2]: For instance, if Trump tweets on Good Friday (market holiday), then the 
market will only react to this new information on Monday at 9:30 am. 

<!--chapter:end:02-data.Rmd-->

# ARMA-X

## Methodology

Once we have our final dataframe, we could then finally start on some analysis.
We first thought of a simple ARMA-X type specification, taking the AHV as our
"y variable" and taking any of the social media variables as the exogenous
regressors. The assumption here is that, while the market reacts to Trump posts,
Trump's posts are chaotic, nonsensical, and random enough to be considered 
exogenous. 

We of course first start by checking stationarity of our variables (ADF), where we find
p-values of 0.01 suggesting that the processes are not explosive. Then, we use 
a custom function in order to choose the number of lags based on the AIC criterion.
This however, while often choose a very high number of lags, which could be 
explained by our data being hourly. As such we decided to put a limit of 3 lags,
which sees minimal AIC loss and simplifying our models considerably.

## Results


### Full Timeframe
We run models with the following exogenous regressors: $TweetDummy$, $TweetCount$,
and the mentions of words $Tariff$, $Trade$, and $China$. We first note on 
<span style="color:red"> *ARMAX Table 1* </span> \@ref(sec:spy-table)that all the x-regressors are significant,
apart from trade. Notice also that all the coefficients (apart from $Tariff_{t-3}$)
are positive, in line with our main hypothesis. The effect of $Tariff_{t-1}$ and 
$Tariff_{t-2}$ are especially large, given the usual size of the volatility 
(<span style="color:red"> *ARMAX Table 4* </span>) \@ref(sec:means-table). We in fact predict that an 
extra mention of tariffs one hour ago, leads to a whopping extra 0.02 in volatility
 which is just about the average size for the full timeframe. We can see the
impulse response function (IRF) for this shock, in <span style="color:red"> *IRFtarif* </span>.
\@ref(sec:SPY-IRF) Notice that there is a large response in the first periods, and then a graduate
decline over time. Something to note is that in our analysises of IRF's, when including
MA terms, the decline shows up gradual while being much sharper when only including
AR terms. 
Note that we ran all these models on the VGK and ASHR ETF's as well, though no
signficant results appear apart from a small but statistically significant effect
of the tariff variable for VGK.

### Split Samples
We then split our sample for the first and second term of the Trump presidancy.
We only run models on tariff, trade and china this time. As seen on table
<span style="color:red"> *ARMAX Table 2* </span> \@ref(sec:spy-table-terms), the first interesting result
is in the coefficients of tariff being significant and very large in the second
term, while being small and not statistically significant in the first. A similar
story goes for the China variable. This may lend some evidence to support the 
claim that investors are much more reactive to Trump's social media presence
now than before. 
Finally, we can check the residuals of all these models to test them somewhat. 
On <span style="color:red"> *ARMAX Table 3* </span>, \@ref(sec:SPY-res-test) the pvalues being zero
for the full timeframe and first term indicate that there is autocorrelation in 
the residuals, thus suggesting that these estimations have problems. Note however,
that the pvalues for the second term are quite high, lending support to our
models on the split sample. These results suggest that perhaps ARMA-X models are
not right in this context, as it is not unreasonable to think that Trump does 
in fact react to market movements. With this information, we decided to run
a VAR model to deepen our understanding of these variables.



<!--chapter:end:03-armax.Rmd-->

# VAR 


## Methodology


## Results



### Full Timeframe



### Split Samples

<!--chapter:end:04-var.Rmd-->

# Conclusion


<!--chapter:end:05-conclusion.Rmd-->

# Bibliography



`r if (knitr::is_html_output()) '
## References {-}
'`

<!--chapter:end:06-bibliography.Rmd-->

# Appendix


```{r library_setup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(future.apply) #parallel computation (speed)
require(aTSA) #adf test
require(kableExtra)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))

```

## ARMAX

We choose the specification in the armax_models file. In this file, we will
just run said specifications to produce nice tables and graphs to include in 
our final paper. This is also why there are specification differences in the 
separate timeframes. We always use the best fit we found earlier.

```{r datasetup_appendix, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#backup
backup = data

#select timeframe 
data = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

#for interpretation
mean1 = mean(data$SPY_vol)

```


```{r fitting models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#All SPY models for the full Dataframe

models <- list()

# ARMA-X(3,3,1) with Tweet Dummy as Exogenous
models[["Model 1"]] <- armax(data$SPY_vol, xreg = data$dummy, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,3,1) with Tweet Count as Exogenous
models[["Model 2"]] <- armax(data$SPY_vol, xreg = data$N, latex = F,
                             nb.lags = 1, p = 3, q = 3) 

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Model 3"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 3, p = 3, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Model 4"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 1, p = 3, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Model 5"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 2) 

```

### SPY ARMAX Table (Jan 2014 - May 2025) {#sec:spy-table}
```{r armaxfull, results="asis", warning=FALSE, echo=FALSE, message=FALSE}
library(texreg)

# Define variable display names (HTML-friendly using <sub>)
names <- list(
  "ar1" = "AR(1)",
  "ar2" = "AR(2)",
  "ar3" = "AR(3)",
  "ma1" = "MA(1)",
  "ma2" = "MA(2)",
  "ma3" = "MA(3)",
  "(Intercept)" = "Constant",
  "dummy_lag_0" = "TweetDummy<sub>t</sub>",
  "dummy_lag_1" = "TweetDummy<sub>t-1</sub>",
  "N_lag_0" = "TweetCount<sub>t</sub>",
  "N_lag_1" = "TweetCount<sub>t-1</sub>",
  "tariff_lag_0" = "Tariff<sub>t</sub>",
  "tariff_lag_1" = "Tariff<sub>t-1</sub>",
  "tariff_lag_2" = "Tariff<sub>t-2</sub>",
  "tariff_lag_3" = "Tariff<sub>t-3</sub>",
  "trade_lag_0" = "Trade<sub>t</sub>",
  "trade_lag_1" = "Trade<sub>t-1</sub>",
  "china_lag_0" = "China<sub>t</sub>"
)

# Output the HTML table directly
cat(
  htmlreg(
    models,
    custom.model.names = names(models),
    custom.coef.map = names,
    caption = "ARMAX Models of Average Hourly Volatility",
    caption.above = TRUE,
    digits = 4,
    doctype = FALSE
  )
)
```


### SPY ARMAX IRFs (Jan 2014 - May 2025){#sec:SPY-IRF}

```{r SPYirf, results="asis", warning=F, message=F, echo=FALSE, cache=TRUE}

#we want to plot the IRFs of these models
nb.periods = 7 * 15

#irf.plot(models[["Model 1"]],nb.periods,title="Tweet Dummy Shock")
#irf.plot(models[["Model 2"]],nb.periods,title="Tweet Count Shock")
plot1 = irf.plot(models[["Model 3"]],nb.periods,
                 title="Tariff Mention Shock - Jan 2014 - May 2025")
plot1
#irf.plot(models[["Model 4"]],nb.periods,title="Trade Mention Shock")
#irf.plot(models[["Model 5"]],nb.periods,title="China Mention Shock")

ggsave("armax_plot1.png",plot=plot1,bg="white")

```


```{r, warning=F, message=F, echo=FALSE, results=F}
#Calculate residuals

res1 = checkresiduals(models[["Model 1"]], plot = FALSE)
res2 = checkresiduals(models[["Model 2"]], plot = FALSE)
res3 = checkresiduals(models[["Model 3"]], plot = FALSE)
res4 = checkresiduals(models[["Model 4"]], plot = FALSE)
res5 = checkresiduals(models[["Model 5"]], plot = FALSE)

```

```{r SPYres, warning=F, message=F, echo=FALSE, cache=TRUE}

resnames = c("Twitter Dummy", "Twitter Count", "Tariff", "Trade", "China")

#extract p-values directly from checkresiduals results
pvals <- data.frame("X-Regressor" = resnames,
                    `Full Timeframe` = c(
                      res1$p.value,
                      res2$p.value,
                      res3$p.value,
                      res4$p.value,
                      res5$p.value))


```




```{r datasetup first, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#First term Calculations

#load final dataset
data = backup

#first term
data = filter(data,between(timestamp, as.Date('2017-01-20'), as.Date('2021-01-20')))

#for interpretation
mean2 = mean(data$SPY_vol)

```


```{r 1st term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#SPY Models first Term

models <- list()

# ARMA-X(3,3,0) with Tariff Mentions as Exogenous
models[["First Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 3, q = 3)

# ARMA-X(3,3,0) with Trade Mentions as Exogenous
models[["First Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 0, p = 3, q = 3) 

```


```{r 1st SPYresiduals, warning=F, message=F, results=F, echo=FALSE, cache=TRUE}
# Run first term residuals
res6 = checkresiduals(models[["First Term (1)"]], plot = FALSE)
res7 = checkresiduals(models[["First Term (2)"]], plot = FALSE)
res8 = checkresiduals(models[["First Term (3)"]], plot = FALSE)

pvals_new1 <- data.frame(
  "First-Term" = c(
    NA,
    NA,
    res6$p.value,
    res7$p.value,
    res8$p.value))

```




```{r datasetup second, results=FALSE, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
#Second Term Calculations

#load final dataset
data = backup

#second term
data = filter(data,between(timestamp, as.Date('2025-01-20'), as.Date('2025-05-07')))

#for interpretation
mean3 = mean(data$SPY_vol)

```


```{r 2nd term models, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
#Run Second Term Models

# ARMA-X(3,2,3) with Tariff Mentions as Exogenous
models[["Second Term (1)"]] <- armax(data$SPY_vol, xreg = data$tariff, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

# ARMA-X(3,2,1) with Trade Mentions as Exogenous
models[["Second Term (2)"]] <- armax(data$SPY_vol, xreg = data$trade, latex = F,
                             nb.lags = 0, p = 1, q = 2) 

# ARMA-X(3,2,0) with China Mentions as Exogenous
models[["Second Term (3)"]] <- armax(data$SPY_vol, xreg = data$china, latex = F,
                             nb.lags = 2, p = 1, q = 2) 

```



```{r 2nd SPYresiduals, results=F, warning=F, message=F, echo=FALSE, cache=TRUE}
# Calculate SPY residuals for the second term

res9 = checkresiduals(models[["Second Term (1)"]], plot = FALSE)
res10 = checkresiduals(models[["Second Term (2)"]], plot = FALSE)
res11 = checkresiduals(models[["Second Term (3)"]], plot = FALSE)

pvals_new2 <- data.frame(
  "Second-Term" = c(
    NA,
    NA,
    res9$p.value,
    res10$p.value,
    res11$p.value))

#combine with other term
pvals_combined <- cbind(pvals,pvals_new1)
pvals_combined <- cbind(pvals_combined, pvals_new2)

```


### SPY ARMAX Table (Separate Terms){#sec:spy-table-terms}

\centering

```{r armax1, results="asis", warning=F, echo=FALSE, message=F}

library(texreg)

# Change model names (not sure if needed anymore)
model_names <- c(
 "First Term (1)", "First Term (2)", "First Term (3)",
 "Second Term (1)", "Second Term (2)", "Second Term (3)"
)
names(models) <- model_names

# HTML coefficients names adjusted (is this correct?)
xnames_ordered <- c(
  "AR(1)", "AR(2)", "AR(3)",
  "MA(1)", "MA(2)", "MA(3)",
  "Constant",
  "Tariff<sub>t</sub>", "Tariff<sub>t-1</sub>", "Tariff<sub>t-2</sub>",
  "Trade<sub>t</sub>",
  "China<sub>t</sub>", "China<sub>t-1</sub>", "China<sub>t-2</sub>"
)

# Render as html -> need to render file to have it nice
htmlreg(
  models,
  custom.coef.names = xnames_ordered,
  custom.model.names = model_names,
  caption = "Split-Term ARMAX Models of Average Hourly Volatility",
  stars = c(0.001, 0.01, 0.05),
  doctype = FALSE
)

```


### Residual Test {#sec:SPY-res-test}

```{r res-table, message=F, warning=F, results="asis", echo=FALSE, cache=TRUE}

kable(pvals_combined, digits = 6, format="html", caption = "Ljung-Box Test p-values for Residuals") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```

### Descriptive Stats{#sec:means-table}
 
```{r means, message=F, warning=F, results="asis"}

means <- data.frame(
  Model = c("Full Time Mean", "First Term Mean", "Second Term Mean"),
  `SPY Volatility Mean` = c(
    mean1,
    mean2,
    mean3))

kable(means, digits = 6, format="html", caption = "Summary Statistics of SPY Volatility") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE, position = "center")



```



<!--chapter:end:07-appendix.Rmd-->

# Cross-references {#cross}

Cross-references make it easier for your readers to find and link to elements in your book.

## Chapters and sub-chapters

There are two steps to cross-reference any heading:

1. Label the heading: `# Hello world {#nice-label}`. 
    - Leave the label off if you like the automated heading generated based on your heading title: for example, `# Hello world` = `# Hello world {#hello-world}`.
    - To label an un-numbered heading, use: `# Hello world {-#nice-label}` or `{# Hello world .unnumbered}`.

1. Next, reference the labeled heading anywhere in the text using `\@ref(nice-label)`; for example, please see Chapter \@ref(cross). 
    - If you prefer text as the link instead of a numbered reference use: [any text you want can go here](#cross).

## Captioned figures and tables

Figures and tables *with captions* can also be cross-referenced from elsewhere in your book using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively.

See Figure \@ref(fig:nice-fig).

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Plot with connected points showing that vapor pressure of mercury increases exponentially as temperature increases.'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Don't miss Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(pressure, 10), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

<!--chapter:end:08-test.Rmd-->

