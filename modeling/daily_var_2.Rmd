---
title: "daily_var_2"
output: html_document
---

# Load necessary packages
```{r, message=FALSE, warning=FALSE}
require("fredr")
require(here)
require(stringr)
require(dplyr)
require(ggplot2)
require(lubridate)
require(RColorBrewer)
require(wordcloud)
require(tidyr)
require(randomForest)
require(tidytext)
require(textstem)
require(tidyverse)
require(tm)
require(SnowballC)
require(quanteda.textplots)
require(quantmod)
require(alphavantager)
require(quanteda)
require(rvest)
require(httr)
require(xml2)
require(textdata)
require(sentimentr)
require(syuzhet)
require(text)
require(xts)
require(tseries)
require(vars)
require(BVAR)
require(bvartools)

truths_raw <- read.csv(here("data/political_data", "truths_new.csv"))
snp_raw <- read.csv(here("data/market_data", "SPY_24_25.csv"))

source(here("helperfunctions/truths_cleaning_function.R"))
```

# Get FRED Data
```{r}
fredr_set_key(Sys.getenv("FRED_API_KEY"))
```

```{r}
# time window
start_date <- as.Date("2015-01-01")
end_date <- Sys.Date()

# VIX
vix_data <- fredr(
  series_id = "VIXCLS",
  observation_start = start_date,
  observation_end = end_date
)

# S&P500
sp500_data <- fredr(
  series_id = "SP500",
  observation_start = start_date,
  observation_end = end_date
)

# VIX
vix_df <- vix_data %>%
  mutate(vix = value) %>%
  dplyr::select(date, vix)

# S&P500
sp500_df <- sp500_data %>%
  mutate(sp500 = value) %>%
  dplyr::select(date, sp500)

# Merge into one dataframe
combined_df <- left_join(sp500_df, vix_df, by = "date")
head(combined_df)

```

# Issue 1: How does causality work?
There are two ways. First trump tweets and then the market reacts. Here we use market close so all tweets drom day 1 market close to day 2 market close affect stock prices on day 2. But there is also the very reasonable assumption that the stock market affects trumps tweets. Therefore we use a VAR model to model how the different sentiments of tweets, tweet volume affect VIX, S&P500.


# Load Truths

```{r}
truths <-truths_processer(truths_raw) # can take one or two minutes
```



```{r}
# Assuming you have a 'truths' dataframe with 'time' and 'date' columns
start_date <- as.Date(min(truths$day)) - 3  # First date (earliest tweet) minus 3 for returns as first tweet is on monday
end_date <- max(truths$day)    # Last date (latest tweet)

# Create a vector of all dates from start to end
date_vector <- seq.Date(from = start_date, to = end_date, by = "day")
date_df <- data.frame(day = date_vector)
```


# This groups posts by day
You have multiple posts a day that we need to summarize what tweets do influence the market at what point. Due to the structure of the VAR a tweet during t affects variables at t+1 and is affected by t-1. therefore we move all pre and during market tweets one day back. so a tweet is lagged that is shows up a day earlier in the analysis to it affects VIX and S&P on current day
```{r}
truths$lag_day <- case_when(
  truths$time_numeric <= 16 ~ as.Date(truths$day) - 1,    # before or at market close (16:00)
  truths$time_numeric > 16 ~ as.Date(truths$day), # after market close
  TRUE ~ as.Date(truths$day) # ??
)

```

```{r}
daily_truths <- truths %>%
  group_by(lag_day) %>%
  summarise(
    posts = paste(post, collapse = " "),  # Combine tweets on each day
    tweet_count = n(),                         # add number of tweets
    .groups = "drop"
  )
```


```{r}
combined_data <- left_join(date_df, combined_df, by = c("day" = "date"))

# Left join tweet data (with lag_day adjustments) to the combined_data
final_data <- left_join(combined_data, daily_truths, by = c("day" = "lag_day")) #%>%
  #drop_na(sp500)  # Optional: remove rows where market data is missing

# View the final dataframe
head(final_data)

```

```{r}
# Step 1: Create a new column with the *next available trading day*
collapsed_data <- final_data %>%
  arrange(day) %>%
  mutate(
    next_trading_day = zoo::na.locf(ifelse(!is.na(sp500), day, NA), fromLast = TRUE, na.rm = FALSE)
  )

# Step 2: Group by the next trading day and summarise
collapsed_data <- collapsed_data %>%
  group_by(next_trading_day) %>%
  summarise(
    sp500 = first(na.omit(sp500)),
    vix = first(na.omit(vix)),
    posts = paste(na.omit(posts), collapse = " "),
    tweet_count = sum(tweet_count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  rename(day = next_trading_day)

collapsed_data <- collapsed_data %>%
  mutate(day = as.Date(day, origin = "1970-01-01"))  # Adjust if necessary

# Check the resulting 'day' column
head(collapsed_data)


```









```{r}
# Backfill the 'sp500' column: fill missing (NA) values with the previous available value
complete_data <- collapsed_data %>%
  mutate(
    sp_ret = log(sp500 / lag(sp500)),
    vix_change = log(vix / lag(vix))
    )
    
complete_data <- complete_data %>%
  slice(-1) 

head(complete_data)
```

```{r}


```

```{r}
nrc_scores <- get_nrc_sentiment(complete_data$posts)

analysis_final_df <- bind_cols(complete_data, nrc_scores)
```


```{r}
analysis_norm_df <- analysis_final_df %>%
  mutate(
    # Calculate total sentiment count (sum of all sentiment word counts)
    total_sentiment = anger + anticipation + disgust + fear + joy + sadness + surprise + trust + negative + positive,
    
    # Normalize each sentiment by dividing by total sentiment to get the proportion of each sentiment
    prop_anger = anger / total_sentiment,
    prop_anticipation = anticipation / total_sentiment,
    prop_disgust = disgust / total_sentiment,
    prop_fear = fear / total_sentiment,
    prop_joy = joy / total_sentiment,
    prop_sadness = sadness / total_sentiment,
    prop_surprise = surprise / total_sentiment,
    prop_trust = trust / total_sentiment,
    prop_negative = negative / total_sentiment,
    prop_positive = positive / total_sentiment,
    
    # Calculate total sentiment per tweet by dividing total sentiment by tweet count
    total_sentiment_per_tweet = total_sentiment / tweet_count,
    
    # Create binary variables for positive and negative returns using case_when
    return_bin = case_when(
      sp_ret > 0 ~ 1,
      TRUE ~ 0
    )
  )




analysis_norm_df[is.na(analysis_norm_df)] <- 0


```

or below full vector: c("vix_change", "sp_ret", "norm_anger", "norm_anticipation", "norm_disgust", "norm_fear",  "norm_joy", "norm_sadness", "norm_surprise", "norm_trust",  "norm_negative", "norm_positive", "tweet_count")

"prop_anger", "prop_anticipation", "prop_disgust", "prop_fear", "prop_joy", "prop_sadness", "prop_surprise", "prop_trust",                "prop_negative", "prop_positive", "total_sentiment_per_tweet"


```{r}

variables <- c("return_bin", "prop_anticipation", "prop_disgust", "prop_fear", "prop_joy", "prop_sadness", "prop_surprise", "prop_trust", "prop_negative", "prop_positive", "total_sentiment_per_tweet")
ts_data <- xts(analysis_norm_df[, variables], 
               order.by = analysis_norm_df$day)
```

```{r}
# ADF stationary test for each column
adf_test_results <- apply(ts_data, 2, function(x) adf.test(x)$p.value) #2 means columns

# print p values (greater than 0.05 non stationry?)
adf_test_results
```

```{r}
# Determine optimal lag length using AIC
lag_selection <- VARselect(ts_data, lag.max = 15, type = "const")

# View selection
lag_selection$selection
```

lag_selection$selection["AIC(n)"]

```{r}
# Fit the VAR model with the chosen lag length
var_model <- VAR(ts_data, p = 2, type = "const")

# Check the summary of the model
summary(var_model)
```
```{r}
bvar_model <- bvar(
  data = ts_data,
  lags = 1,
  hyper = set_hyper(lambda = 0.5, alpha = 3) # lambda is shrinkage, alpha is lag punishment -> lag 1 doesnt matter
)
```

```{r}
coef(bvar_model)
```




