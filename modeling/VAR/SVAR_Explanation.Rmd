# Setup

## Load packages & functions

```{r library_setup, results=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization
require(texreg) #arima tables
require(vars) #VAR models
require(xts) #time series objects
require(tseries) #includes adf test
require(quantmod)
require(TSA)
require(aTSA)
require(tibble)
require(FinTS)
require(kableExtra)
require(writexl)
require(purrr)

getwd()
#setwd("...") -> set wd at base repo folder

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/armax_functions.R"))
source(here("helperfunctions/var_irf.R"))

```


## Load Data

We connect R to our GitHub folder where the data are stored. 
We then load the dataset and select the relevant time window 
for our analysis.

```{r datasetup, results=FALSE, warning=FALSE, message=FALSE}

#load final dataset
source(here("helperfunctions/full_data.R"))

#select timeframe 
Vdata = filter(data,between(timestamp, as.Date('2014-01-01'), as.Date('2025-05-07')))

```



# Stationarity 

We begin by testing whether our variables are stationary over time. 
We use the Augmented Dickey-Fuller test on the volatility of 
different markets and on variables derived from Trump post.

```{r adf tests, warning=FALSE, message=FALSE}

adf.test(data$SPY_vol)
adf.test(data$VGK_vol)
adf.test(data$ASHR_vol)

adf.test(data$dummy)
adf.test(data$N)
adf.test(data$tariff)
adf.test(data$china)

```



# Information Criteria

we determine the optimal number of lags using  several Information 
Criteria (AIC, SC, HQ, FPE). 

Here is an example using the AIC

```{r AIC, warning=FALSE, message=FALSE}

##with dummy
y = cbind(Vdata$dummy, Vdata$SPY_vol)

y_lag = VARselect(y, lag.max = 80)
y_list = list(y_lag)


##AIC
resultats1 <- lapply(y_list, function(x) x$criteria["AIC(n)", ])

resultats1 = as.data.frame(resultats1)

resultats1 = resultats1 %>%
  rename(name = 1) %>%
  mutate(
    n = c(1:length(name))
  )

ggplot (resultats1, aes(x=n, y= name)) +
  geom_line(color = "steelblue") +
  labs(title = "hourly Returns AIC",x="dummy" , y = "AIC") +
  theme_minimal()

```


# VAR Model

## Volatility & TweetDummy 

We first extract and bind the relevant variables, estimate a VAR model using the 
VAR function, and then display the results in a table for the volatility equation. 
Our main interest is in the effect of a post (dummy) on volatility (and not the inverse).


```{r dummy, warning=FALSE, message=FALSE}

y = cbind(Vdata$dummy, Vdata$SPY_vol)
colnames(y)[1:2] <- c("dummy", "vol")
est.VAR <- VAR(y,p=6)
mod_vol <- est.VAR$varresult$vol
screenreg(mod_vol, digits = 6)

```


## Serial test
We run a Portmanteau-Godfrey test in order to detect some serial correlation on the
residuals. We found strong evidence of the presence of serial correlation in all 
estimations.  

```{r dummy, warning=FALSE, message=FALSE}

serial2 = serial.test(est.VAR, lags.pt =6, type = "PT.asymptotic")
serial2

```


## Newey-west estimator


Newey-West correction of Standard Error is implemented in order to correct for 
bias in the standard errors. We run an OLS with both regressions of the VAR framework 
in order to use the Newey-West function. Then we calculate the p-value with the robust
estimation of the variance of the residuals in order to display correct statistical
significance on our tables.

```{r Newey West, warning=FALSE, message=FALSE}
#extract results
mod_vol = est.VAR$varresult$vol
f = formula(mod_vol)
d = model.frame(mod_vol)
lm_clean = lm(f, data= d)

#apply Newey-West
nw_vcov = NeweyWest(lm_clean, lag=6)
nw_se = sqrt(diag(nw_vcov))

#t-stats
coef = coef(lm_clean)
t_stat = coef/nw_se

#recalculate p-values
robust = 2*(1-pt(abs(t_stat), df = df.residual(lm_clean)))

nw_se       <- nw_se[names(coef(lm_clean))]
robust      <- robust[names(coef(lm_clean))]

#table
screenreg(lm_clean, override.se = nw_se, override.pvalues = robust, digits = 6)
```


We also look for the effect of the AHV on Trump's post, here TweetDummy, with robust standard error

```{r effect of Vol on dummy}
#extract results
mod_post = est.VAR$varresult$dummy
ff = formula(mod_post)
dd = model.frame(mod_post)
lm_clean_post = lm(ff, data= dd)

#apply Newey-West
nw_vcov_post = NeweyWest(lm_clean_post, lag=6)
nw_se_post = sqrt(diag(nw_vcov_post))

#t-stats
coef_post = coef(lm_clean_post)
t_stat_post = coef_post/nw_se_post

#recalculate p-values
robust_post = 2*(1-pt(abs(t_stat_post), df = df.residual(lm_clean_post)))

nw_se_post       <- nw_se_post[names(coef(lm_clean_post))]
robust_post      <- robust_post[names(coef(lm_clean_post))]

#table
screenreg(lm_clean_post, override.se = nw_se_post, override.pvalues = robust_post, digits = 6)


```


## Breusch-Pagan test

We also run Breusch-Pagan test for checking the heteroscedasticity.
We found strong evidence of the presence of heteroscedasticity in all estimations, 
except for the second term.

```{r bp test}
#H0 test whether there is NOT heteroscedasticity. 
bptest(lm_clean)

```


## B matrix

Then we use the Newey-West formula in order to construct our Robust covariance 
matrix using the residuals of our estimation

```{r Omega Robust Matrix, warning=FALSE, message=FALSE}
#Recreate a Robust Omega Matrix
U = residuals(est.VAR)
T = nrow(U)
L = 6 #number of lag
Omega = matrix(0, ncol(U), ncol(U))
for(l in 0:L) {
  weight = 1 - l/(L+1)
  Gamma_l_ = t(U[(l+1):T, , drop=FALSE]) %*% U[1:(T-l), , drop=FALSE] /T
  if (l == 0){
    Omega = Omega + Gamma_l_
  } else {
    Omega = Omega + weight*(Gamma_l_ + t(Gamma_l_))
  }
}

```



# SVAR Model

## B matrix 

In order to implement our SVAR framework with a short run restriction, 
we need to reconstruct the Variance-Covariance Omega Matrix with said restriction. 
Here, as we have 2 outcome variables, and as the Omega 
Matrix is 2x2, we only need n(n-1)/2 restrictions, which is one 
restriction. Our assumption is that while the market reacts instantly to Trump
posts, Trump does not react contemporaneously with changes in market volatility.
For constructing our BB' matrix we define a matrix in a function and 
find the square distance between the True Omega matrix and the 
constructed BB matrix. We then use a optimization function in order to find 
the elements of B matrix (B.hat) that minimize the distance with the 
true Variance-Covariance matrix.


```{r B mat, warning=FALSE, message=FALSE}


#create the B matrix
loss <- function(param){
  #define the restriction
  B <- matrix(c(param[1], param[2], 0, param[3]), ncol = 2)
  
  #make BB' approximatively equal to omega
  X <- Omega - B %*% t(B)
  
  #loss function
  loss <- sum(X^2)
  return(loss)
}

res.opt <- optim(c(1, 0, 1), loss, method = "BFGS")
B.hat <- matrix(c(res.opt$par[1], res.opt$par[2], 0, res.opt$par[3]), ncol = 2)

print(cbind(Omega,B.hat %*% t(B.hat)))

#shock by dummy
B.hat

```


## IRF 

We then use the irf() function of the vars packages in order 
to graph the effect of a choc of Trump post (here TweetDummy) 
on market volatility using the coefficient of our 
est.VAR function and using the choc coefficient of our 
estimated B matrix with our restriction.


```{r IRF 1dummy}
nb.sim = 7*7
#get back the coefficient of est.VAR
phi <- Acoef(est.VAR)
PHI = make.PHI(phi)

#take the constant
constant <- sapply(est.VAR$varresult, function(eq) coef(eq)["const"])
c=as.matrix(constant)

#Simulate the IRF
p <- length(phi)
n <- dim(phi[[1]])[1]


Y <- simul.VAR(c=c, Phi = phi, B = B.hat, nb.sim ,y0.star=rep(0, n*p),
                  indic.IRF = 1, u.shock = c(1,0))



Yd = data.frame(
  period = 1:nrow(Y),
  response = Y[,2])

ggplot(Yd,aes(x=period, y=response)) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P IRF of Dummy on Volatility")+
  ylab("")+
  xlab("") +
  theme_minimal()

```

We also graph cumulative IRF as well.

```{r IRF 2Dummy}
ggplot(Yd,aes(x=period, y=cumsum(response))) +
  geom_hline(yintercept = 0, color="red") +
  geom_line() +
  theme_light() +
  ggtitle("S&P Cumulalitve IRF of Dummy on Volatility") +
  ylab("")+
  xlab("") +
  theme_minimal()
```

## Granger test

Finally, we use a Granger causality test to evalutate 
the robustness of the correlation we've found. We look 
for Granger causalities in both directions, i.e. whether 
volatility Granger-causes dummy mentions and vice versa.

```{r granger, warning=FALSE, message=FALSE}

#does volatility Granger cause dummy mentions
grangertest(y[,c("vol","dummy")], order = 6)

#does dummy mentions Granger cause volatility
grangertest(y[,c("dummy", "vol")], order = 6)
```


