---
title: "Garch presentation"
author: "pte"
date: "2025-04-28"
output:
  html_document: default
---

```{r 1}


library("quantmod")
library("TSA")
library("aTSA")
library("rugarch")
library("rmgarch")
library("ggplot2")
library("tibble")
library("dplyr")
library("lubridate")
library(tibble)
library(tidyverse)
library(knitr)
library("fGarch")
library(FinTS)
library(kableExtra)
library(writexl)
library(texreg)
library("purrr")
library("forecast")
require(tinytex) #LaTeX
require(ggplot2) #plots
require(AEC) #JP-Renne functions
require(AER) #NW formula
require(forecast) #time series stuff
require(expm) #matrix exponents
require(here) #directory finder
require(stringr) # analysis of strings, important for the detection in tweets
require(dplyr) #data management
require(lubridate) #data dates management
require(zoo) #for lagging
require(jtools) #tables
require(huxtable) #tables
require(lmtest) #reg tests
require(vroom) #for loading data
require(data.table) #for data filtering
require(sysid) #for ARMA-X modeling
require(sandwhich) #regression errors
require(stargazer) #nice reg tables
require(tidytext) #text mining
require(textstem) #lemmatization
require(quanteda) #tokenization

```

```{r 2}
rm(list=ls())

data <- read.csv(here("data/market_data/SPY_24_25.csv"))

#load helper functions
source(here("helperfunctions/data_loaders.R"))
source(here("helperfunctions/date_selector.R"))
source(here("helperfunctions/plotters.R"))
source(here("helperfunctions/quick_arma.R"))
source(here("helperfunctions/r.vol_calculators.R"))
source(here("helperfunctions/truths_cleaning_function.R"))
source(here("helperfunctions/select_armax.R"))


```

```{r 3}
#minute return
ret3 <- data %>%
  mutate(
    timestamp = dmy_hm(timestamp),
    date = as.Date(timestamp),
    hour = hour(timestamp),
    minute = minute(timestamp),
  ) %>%
  arrange(timestamp) %>%
  mutate(return = 100*log(Close/lag(Close))) %>%  #better way diff(log(prices), lag=1)
  group_by(date, hour, minute) %>%
  summarise(mean_minute_return = mean(return, na.rm = TRUE), .groups = "drop")

ret4 <- ret3 %>%
  mutate(datetime = as.POSIXct(paste(date, hour, minute), format = "%Y-%m-%d %H%M", tz = "UTC")) %>%
  select(datetime, mean_minute_return) %>%
  mutate(mean_minute_return = if_else(is.na(mean_minute_return), 0, mean_minute_return))

ggplot (ret4, aes(x=datetime, y= mean_minute_return)) +
  geom_line(color = "steelblue") +
  labs(title = "hourly Returns", x = "Date", y = "Returns") +
  theme_minimal()

```

```{r 4}
#Trump data

#truthsocial
raw_truths <- read.csv(here("data/political_data", "truths_new.csv"))

#twitter
raw_tweets <- read.csv(here("data/political_data", "tweets.csv"))



## Tweet Cleanup \& Count

tweets = raw_tweets

#only keep original Tweets
tweets <- tweets %>% filter(isRetweet != "t")
tokens <- tokens(tweets$text)
dfm <- dfm(tokens)

#cleanup
tweets = as.data.table(tweets)
names(tweets)[names(tweets) == 'date'] <- 'timestamp'
tweets <- tweets[order(tweets$timestamp, decreasing=T), ]
tweets$timestamp = as.POSIXct(tweets$timestamp,format = "%Y-%m-%d %H:%M:%S", tz = "UTC")

#count by hour
tweet_count = tweets[, .N, by=.(year(timestamp), month(timestamp), 
                                day(timestamp), hour(timestamp), minute(timestamp))] 

#fix timestamp
tweet_count$timestamp = as.POSIXct(sprintf("%04d-%02d-%02d %02d:%02d:00", 
                                           tweet_count$year, tweet_count$month, tweet_count$day, 
                                           tweet_count$hour, tweet_count$minute), format = "%Y-%m-%d %H:%M:00")

tweet_count$timestamp = format(tweet_count$timestamp, tz = "UTC")

#remove useless columns and reorder by oldest first
tweet_count = select(tweet_count, timestamp, N)
tweet_count = tweet_count[ order(tweet_count$timestamp , decreasing = F ),]


```

```{r 5}
## Truths Cleanup \& Count
truthsbackup <- truths_processer(raw_truths) 

#cleanup
truths = as.data.table(truthsbackup)
names(truths)[names(truths) == 'date_time_parsed'] <- 'timestamp'
truths <- truths[order(truths$timestamp, decreasing=T), ]

#count by hour
truth_count = truths[, .N, by=.(year(timestamp), month(timestamp), 
                                day(timestamp), hour(timestamp), minute(timestamp))] 

#fix timestamp
truth_count$timestamp = as.POSIXct(sprintf("%04d-%02d-%02d %02d:%02d:00", 
                                           truth_count$year, truth_count$month, truth_count$day, 
                                           truth_count$hour, truth_count$minute), format = "%Y-%m-%d %H:%M:00", tz = "UTC")

truth_count$timestamp = format(truth_count$timestamp, tz = "UTC")

#verify the fz 
attr(truth_count$timestamp, "tzone")

                                                                                              
#remove useless columns and reorder by oldest first
truth_count = select(truth_count, timestamp, N)
truth_count = truth_count[ order(truth_count$timestamp , decreasing = F ),]



#reorder and graph
tt_count = rbind(tweet_count,truth_count) #tweets & truths count
tt_count = tt_count %>% mutate(dummy = if_else(N > 0, 1, 0))




```

```{r 6}
#filter date from when financial data begin 
tt <- tt_count %>%
  filter(timestamp >= as.POSIXct("2024-01-02", tz = "UTC")) %>%
  rename(datetime = timestamp) 

tt$datetime <- as.POSIXct(tt$datetime, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")

tt$datetime <- lubridate::floor_date(tt$datetime, unit = "minute")
ret4$datetime <- lubridate::floor_date(ret4$datetime, unit = "minute")



```

```{r 7}





#link hourly return and Trump tweets
d <- ret4 %>%
  left_join(tt, by = "datetime") %>%      # jointure sur datetime
  mutate(dummy = if_else(is.na(dummy), 0, dummy)) 

```

```{r 8}
#modeling and testing

acf(ret4$mean_minute_return)

```

```{r 9}

pacf(ret4$mean_minute_return)

```

```{r 10}
#LM test
do_arch_test <- function(x, max_lag = 5) {
  require(FinTS)
  require(tidyverse)
  
  do_single_arch <- function(x, used_lag)  {
    test_out <- FinTS::ArchTest(x, lags = used_lag)
    
    res_out <- tibble(Lag = used_lag,
                      `LMStatistic` = test_out$statistic, 
                      `pvalue` = test_out$p.value)
  }
  
  tab_out <- bind_rows(map(1:max_lag,.f = do_single_arch, x = x))
  
  return(tab_out)
}


table <- do_arch_test(x = ret4$mean_minute_return, max_lag = 5)
table

```

```{r 11}
#adf test (for stationarity of the outcome data)
adf.test(ret4$mean_minute_return)

```

```{r 12}
#KPSS test (stationarity)
require("tseries")
kpss.test(d$mean_minute_return, null = c("Level", "Trend"), lshort = FALSE)

```

```{r 13}
#3) finding the best model for our GARCH
#set x = data
x = ret4$mean_minute_return
max_lag_AR <- 1 
max_lag_MA <- 1 
max_lag_ARCH <- 1
max_lag_GARCH <- 1 
dist_to_use <- c("norm")
  #c('norm','std') #or c('norm')
models_to_estimate <- c('sGARCH', 'gjrGARCH')
                      #c('sGARCH', 'eGARCH', 'gjrGARCH')




do_single_garch <- function(x, 
                            type_model, 
                            type_dist, 
                            lag_ar, 
                            lag_ma,
                            lag_arch, 
                            lag_garch) {
  require(rugarch)
  
  
  spec = ugarchspec(variance.model = list(model =  type_model, 
                                          garchOrder = c(lag_arch, lag_garch)),
                    mean.model = list(armaOrder = c(lag_ar, lag_ma), external.regressors=matrix(c(d$dummy), ncol=1)),
                    distribution = type_dist)
  
  message('Estimating ARMA(',lag_ar, ',', lag_ma,')-',
          type_model, '(', lag_arch, ',', lag_garch, ')', 
          ' dist = ', type_dist,
          appendLF = FALSE)
  
  try({
    my_rugarch <- list()
    my_rugarch <- ugarchfit(spec = spec, data = x)
  })
  
  if (!is.null(coef(my_rugarch))) {
    message('\tDone')
    
    AIC <- rugarch::infocriteria(my_rugarch)[1]
    BIC <- rugarch::infocriteria(my_rugarch)[2]
  } else {
    message('\tEstimation failed..')
    
    AIC <- NA
    BIC <- NA
  }
  
  est_tab <- tibble(lag_ar, 
                    lag_ma,
                    lag_arch,
                    lag_garch,
                    AIC =  AIC,
                    BIC = BIC,
                    type_model = type_model,
                    type_dist,
                    model_name = paste0('ARMA(', lag_ar, ',', lag_ma, ')+',
                                        type_model, '(', lag_arch, ',', lag_garch, ') ',
                                        type_dist) ) 
  
  return(est_tab)
}


find_best_arch_model <- function(x, 
                                 type_models, 
                                 dist_to_use,
                                 max_lag_AR,
                                 max_lag_MA,
                                 max_lag_ARCH,
                                 max_lag_GARCH) {
  
  require(tidyr)
  
  df_grid <- expand_grid(type_models = type_models,
                         dist_to_use = dist_to_use,
                         arma_lag = 0:max_lag_AR,
                         ma_lag = 0:max_lag_MA,
                         arch_lag = 1:max_lag_ARCH,
                         garch_lag = 1:max_lag_GARCH)
  
  
  l_out <- pmap(.l = list(x = rep(list(x), nrow(df_grid)), 
                          type_model = df_grid$type_models,
                          type_dist = df_grid$dist_to_use,
                          lag_ar = df_grid$arma_lag,
                          lag_ma = df_grid$ma_lag,
                          lag_arch = df_grid$arch_lag,
                          lag_garch  = df_grid$garch_lag),
                do_single_garch)
  
  tab_out <- bind_rows(l_out)
  
  # find by AIC
  idx <- which.min(tab_out$AIC)
  best_aic <- tab_out[idx, ]
  
  # find by BIC
  idx <- which.min(tab_out$BIC)
  best_bic <- tab_out[idx, ]
  
  l_out <- list(best_aic = best_aic,
                best_bic = best_bic,
                tab_out = tab_out)
  
  return(l_out)
}

out <- find_best_arch_model(x, 
                            type_models = models_to_estimate,
                            dist_to_use = dist_to_use,
                            max_lag_AR = max_lag_AR,
                            max_lag_MA = max_lag_MA,
                            max_lag_ARCH = max_lag_ARCH,
                            max_lag_GARCH = max_lag_GARCH)
tab_out <- out$tab_out

# pivot table to long format (better for plotting)
df_long <- tidyr::pivot_longer(data = tab_out %>%
                                 select(model_name,
                                        type_model,
                                        type_dist,
                                        AIC, BIC),  cols = c('AIC', 'BIC'))

models_names <- unique(df_long$model_name)
best_models <- c(tab_out$model_name[which.min(tab_out$AIC)],
                 tab_out$model_name[which.min(tab_out$BIC)])

# figure out where is the best model
df_long <- df_long %>%
  mutate(order_model = if_else(model_name %in% best_models, 'Best Model', 'Not Best Model') ) %>%
  na.omit()

# make table with best models
df_best_models <- df_long %>%
  group_by(name) %>%
  summarise(model_name = model_name[which.min(value)],
            value = value[which.min(value)],
            type_model = type_model[which.min(value)])

# plot results
p1 <- ggplot(df_long %>%
               arrange(type_model), 
             aes(x = reorder(model_name, 
                             order(type_model)),
                 y = value, 
                 shape = type_dist,
                 color = type_model)) + 
  geom_point(size = 3.5, alpha = 0.65) + 
  coord_flip() + 
  facet_wrap(~name, scales = 'free_x') + 
  geom_point(data = df_best_models, mapping = aes(x = reorder(model_name, 
                                                              order(type_model)),
                                                  y = value), 
             color = 'blue', size = 5, shape = 8) +
  labs(title = 'Selecting Garch Models by Fitness Criteria', 
       subtitle = 'The best model is the one with lowest AIC or BIC (with star)',
       x = '',
       y = 'Value of Fitness Criteria',
       shape = 'Type of Dist.',
       color = 'Type of Model') + 
  theme(legend.position = "right")
#normal size is width = 5, height = 7. Here it is the maximum
#ggsave("garch_model2_selection.jpg", plot = p1, path = "C:/Users/th9fe/OneDrive/Bureau/Econometric", width = 20, height = 25, limitsize = TRUE)

p1




```

```{r 14}
# estimate best garch model by BIC 
best_spec = ugarchspec(variance.model = list(model =  out$best_bic$type_model, 
                                             garchOrder = c(out$best_bic$lag_arch,
                                                            out$best_bic$lag_garch)),
                       mean.model = list(armaOrder = c(out$best_bic$lag_ar, 
                                                       out$best_bic$lag_ma)),
                       distribution = 'std')

my_best_garch <- ugarchfit(spec = best_spec, 
                           data = x)

my_best_garch

```


